{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_files/user_jason/metric_df.csv dataset...\n",
      "Processing data_files/user_lauren/metric_df.csv dataset...\n",
      "Processing data_files/user_lizzie1/metric_df.csv dataset...\n",
      "Processing data_files/user_lizzie2/metric_df.csv dataset...\n",
      "Processing data_files/user_sarah1/metric_df.csv dataset...\n",
      "Processing data_files/user_shreya/metric_df.csv dataset...\n",
      "Processing data_files/user_sujaan/metric_df.csv dataset...\n",
      "Processing data_files/user_xiao/metric_df.csv dataset...\n",
      "Processing data_files/user_yutong/metric_df.csv dataset...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from utils import stratified_sample, annotate, even_train_split\n",
    "import glob  # Importing the glob module to find all the files matching a pattern\n",
    "\n",
    "# Pattern to match the data files\n",
    "file_pattern = \"data_files/user_*/metric_df.csv\"\n",
    "\n",
    "# Initialize a dictionary to store one_user_one_user_dataframes for each dataset\n",
    "all_datasets = {}\n",
    "\n",
    "# Loop through each file that matches the file pattern\n",
    "for filepath in glob.glob(file_pattern):\n",
    "    # print(filepath)\n",
    "    # print(filepath.split('/'))\n",
    "    user_name = filepath.split('/')[1]\n",
    "    # user_name = filepath.split('\\\\')[1]\n",
    "    print(f\"Processing {filepath} dataset...\")\n",
    "\n",
    "    # Read in data file as a pandas dataframe\n",
    "    data = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    # add weighted performance metric\n",
    "    w = 1\n",
    "    data[\"total_error\"] = data['avg_osd'] + data['avg_target_error']\n",
    "    data[\"weighted_performance\"] = 10*data['throughput'] - w*data[\"total_error\"]\n",
    "\n",
    "    all_datasets[user_name] = data\n",
    "\n",
    "# Combine datasets for Lizzie\n",
    "lizzie1 = all_datasets[\"user_lizzie1\"]\n",
    "lizzie2 = all_datasets[\"user_lizzie2\"]\n",
    "combined_df = pd.concat([lizzie1, lizzie2])\n",
    "all_datasets[\"user_lizzie\"] = combined_df.groupby(['latency', 'scale']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import PolyRegression, GPRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, RationalQuadratic, WhiteKernel\n",
    "import utils\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename='warnings_log.txt', level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "# Function to redirect warnings to logging\n",
    "def warn_to_logging(message, category, filename, lineno, file=None, line=None):\n",
    "    logging.warning(f'{filename}:{lineno}: {category.__name__}: {message}')\n",
    "\n",
    "# Redirect all warnings to the warn_to_logging function\n",
    "warnings.showwarning = warn_to_logging\n",
    "\n",
    "all_results = {}\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"total_error\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t\n",
    "\tprint(output_metric)\n",
    "\tuser_results = {}\n",
    "\tfor user, data in list(all_datasets.items()): \n",
    "\t\t# if user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t# \tcontinue\n",
    "\t\tprint(f\"\\t{user}\")\n",
    "\n",
    "\t\t# Prepare data \n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tY = data[output_metric]\n",
    "\n",
    "\t\t# Initialize evaluation metrics\n",
    "\t\toptimal_match_rate = []\n",
    "\t\toptimal_scale_error = []\n",
    "\t\tmse_scores = []\n",
    "\t\tfull_mse_scores = []\n",
    "\t\tn_train_mse = []\n",
    "\t\tn_train_full_mse = []\n",
    "\t\tn_train_p = []\n",
    "\n",
    "\t\tn = len(data)\n",
    "\t\tn_train_values = range(2, n-1)\n",
    "\t\tfor n_train in n_train_values:\n",
    "\n",
    "\t\t\tn_train_p.append(n_train / n)\n",
    "\t\t\t# Split into training/test sets\n",
    "\t\t\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\t\t\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\t\t\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\t\t\tY_train, Y_test = train_set[output_metric], test_set[output_metric]\n",
    "\t\t\t\n",
    "\t\t\t# Create dense test input\n",
    "\t\t\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t\t\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\t\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\t\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\t\t\n",
    "\t\t\t# # Polynomial Regression\n",
    "\t\t\tif model_type.startswith(\"Poly\"):\n",
    "\t\t\t\tdegree = int(model_type.strip(\"Poly\"))\n",
    "\t\t\t\tY_pred, model_params = PolyRegression(X_train.values, Y_train.values, X.values, degree)\n",
    "\t\t\t\tY_pred_dense, _ = PolyRegression(X_train.values, Y_train.values, X_dense, degree)\n",
    "\n",
    "\t\t\t# Gaussian Process Regression\n",
    "\t\t\telif model_type.startswith(\"GPR\"):\n",
    "\t\t\t\t# Choose kernel\n",
    "\t\t\t\tkernel_type = model_type.removeprefix(\"GPR_\")\n",
    "\t\t\t\t# print(kernel_type)\n",
    "\t\t\t\tif kernel_type == \"RBF_Noise_default\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF() + WhiteKernel() # Default RBF with likelihood noise\n",
    "\t\t\t\telif kernel_type == \"RBF_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0])\n",
    "\t\t\t\telif kernel_type == \"RBF_Noise_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0]) + WhiteKernel() # RBF with anistropic length scale\n",
    "\t\t\t\telif kernel_type == \"RQ_Noise_default\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RationalQuadratic() + WhiteKernel() # Default Rational Quadratic with likelihood noise\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(\"Invalid kernel specification!\")\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\tY_pred, Y_pred_std, model_params = GPRegression(X_train.values, Y_train.values, X.values, kernel)\n",
    "\t\t\t\tY_pred_dense, Y_pred_std, _ = GPRegression(X_train.values, Y_train.values, X_dense, kernel)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Invalid model type specification!\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t## Evaluate metrics\n",
    "\t\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "\t\t\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t\t# Mean Square Error on whole dataset\n",
    "\t\t\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\t\t\tif True: #full_mse < 5000:\n",
    "\t\t\t\tn_train_full_mse.append(n_train)\n",
    "\t\t\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t\t\t# Mean Square Error on test set\n",
    "\t\t\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\t\t\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\t\t\tif True: #mse < 5000:\n",
    "\t\t\t\tn_train_mse.append(n_train)\n",
    "\t\t\t\tmse_scores.append(mse)\n",
    "\t\t\t\n",
    "\t\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\t\t\telse: # optimal scale at minimum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t\t# Merge the results on 'latency'\n",
    "\t\t\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\t\t\n",
    "\t\t\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t\t\t# print(optimal_scale_dense)\n",
    "\t\t\t# print(merged_ref_dense)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# Count the number of matches\n",
    "\t\t\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\t\t\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\t\t\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\t\t\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t\t\t# Visualize model prediction\n",
    "\t\t\tif n_train == n-2:\n",
    "\t\t\t\tutils.model_heatmaps(data, dense_df, X_train, user, output_metric, model_type, model_params)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Store results from this dataset\n",
    "\t\t\tuser_results[user] = {\n",
    "\t\t\t\t'n_train_mse': list(n_train_mse),\n",
    "\t\t\t\t'n_train_full_mse': list(n_train_full_mse),\n",
    "\t\t\t\t'full_mse_scores': full_mse_scores,\n",
    "\t\t\t\t'mse_scores': mse_scores,\n",
    "\t\t\t\t'n_train_all': list(n_train_values),\n",
    "\t\t\t\t'match_rate': optimal_match_rate,\n",
    "\t\t\t\t'scale_error': optimal_scale_error,\n",
    "\t\t\t\t'n_train_p': n_train_p\n",
    "\t\t\t}\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\t\n",
    "\telse:\n",
    "\t\tall_results[output_metric] = user_results\n",
    "\t\tcontinue\n",
    "\tbreak\n",
    "\n",
    "# print(all_results.keys())\n",
    "with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "\tjson.dump(all_results, file)\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "# \tog_results = json.load(file)\n",
    "\n",
    "# og_results[\"total_error\"] = all_results[\"total_error\"]\n",
    "\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(og_results, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the results for all datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "\tall_results = json.load(file)\n",
    "\n",
    "for output_metric, user_results in all_results.items():\n",
    "\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\tfig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}\")\n",
    "\tfor user, results in user_results.items():\n",
    "\t\taxes[0, 0].plot(results['n_train_p'], results['match_rate'], marker='o', label=user)\n",
    "\t\taxes[0, 1].plot(results['n_train_p'], results['scale_error'], marker='o', label=user)\n",
    "\t\taxes[1, 0].plot(results['n_train_p'], results['full_mse_scores'], marker='o', label=user)\n",
    "\t\taxes[1, 1].plot(results['n_train_p'], results['mse_scores'], marker='o', label=user)\n",
    "\n",
    "\taxes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\taxes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\taxes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\taxes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\taxes[1, 0].set_title('MSE on whole dataset')\n",
    "\taxes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\taxes[1, 1].set_title('MSE on test set')\n",
    "\taxes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\taxes[1, 0].legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/model_results/{output_metric}/{model_type}_model_eval_metrics_{output_metric}.png\", facecolor='w')\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot averaged results\n",
    "\n",
    "# Load data\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "\tall_results = json.load(file)\n",
    "\n",
    "print(all_results)\n",
    "for output_metric, user_results in all_results.items():\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\tfig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}, mean over users\")\n",
    "\n",
    "\t# Prepare lists of lists to store data\n",
    "\tmatch_rate_lists = []\n",
    "\tscale_error_lists = []\n",
    "\tfull_mse_score_lists = []\n",
    "\tmse_score_lists = []\n",
    "\tn_train_lists = []\n",
    "\n",
    "\t# Collect data for each metric\n",
    "\tfor user, results in user_results.items():\n",
    "\t\tmatch_rate_lists.append(results['match_rate'])\n",
    "\t\tscale_error_lists.append(results['scale_error'])\n",
    "\t\tfull_mse_score_lists.append(results['full_mse_scores'])\n",
    "\t\tmse_score_lists.append(results['mse_scores'])\n",
    "\t\tn_train_lists.append(results['n_train_all'])\n",
    "\n",
    "\t# Function to calculate average and standard deviation safely\n",
    "\tdef safe_mean_std(data_lists, index):\n",
    "\t\tvalid_data = [data[index] for data in data_lists if index < len(data)]\n",
    "\t\treturn np.mean(valid_data), np.std(valid_data)\n",
    "\n",
    "\t# Calculate averages and standard deviations safely\n",
    "\tavg_match_rate = []\n",
    "\tstd_match_rate = []\n",
    "\tavg_scale_error = []\n",
    "\tstd_scale_error = []\n",
    "\tavg_full_mse_scores = []\n",
    "\tstd_full_mse_scores = []\n",
    "\tavg_mse_scores = []\n",
    "\tstd_mse_scores = []\n",
    "\n",
    "\tmin_n_train_length = min([len(data_list) for data_list in n_train_lists])\n",
    "\tmin_n_train_list = n_train_lists[0][:min_n_train_length]\n",
    "\tfor n in range(min_n_train_length):\n",
    "\t\tmean, std = safe_mean_std(match_rate_lists, n)\n",
    "\t\tavg_match_rate.append(mean)\n",
    "\t\tstd_match_rate.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(scale_error_lists, n)\n",
    "\t\tavg_scale_error.append(mean)\n",
    "\t\tstd_scale_error.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(full_mse_score_lists, n)\n",
    "\t\tavg_full_mse_scores.append(mean)\n",
    "\t\tstd_full_mse_scores.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(mse_score_lists, n)\n",
    "\t\tavg_mse_scores.append(mean)\n",
    "\t\tstd_mse_scores.append(std)\n",
    "\n",
    "\t# Plotting the average values and standard deviation\n",
    "\t# Plotting the average values and standard deviation\n",
    "\taxes[0, 0].plot(min_n_train_list, avg_match_rate, marker='o', label='Mean over users')\n",
    "\taxes[0, 0].fill_between(min_n_train_list, np.subtract(avg_match_rate, std_match_rate), \n",
    "\t\t\t\t\t\t\tnp.add(avg_match_rate, std_match_rate), alpha=0.2)\n",
    "\n",
    "\taxes[0, 1].plot(min_n_train_list, avg_scale_error, marker='o', label='Mean over users')\n",
    "\taxes[0, 1].fill_between(min_n_train_list, np.subtract(avg_scale_error, std_scale_error), \n",
    "\t\t\t\t\t\t\tnp.add(avg_scale_error, std_scale_error), alpha=0.2)\n",
    "\n",
    "\taxes[1, 0].plot(min_n_train_list, avg_full_mse_scores, marker='o', label='Mean over users')\n",
    "\taxes[1, 0].fill_between(min_n_train_list, np.subtract(avg_full_mse_scores, std_full_mse_scores), \n",
    "\t\t\t\t\t\t\tnp.add(avg_full_mse_scores, std_full_mse_scores), alpha=0.2)\n",
    "\n",
    "\taxes[1, 1].plot(min_n_train_list, avg_mse_scores, marker='o', label='Mean Over Users')\n",
    "\taxes[1, 1].fill_between(min_n_train_list, np.subtract(avg_mse_scores, std_mse_scores), \n",
    "\t\t\t\t\t\t\tnp.add(avg_mse_scores, std_mse_scores), alpha=0.2)\n",
    "\t\n",
    "\taxes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\taxes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\taxes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\taxes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\taxes[1, 0].set_title('MSE on whole dataset')\n",
    "\taxes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\taxes[1, 1].set_title('MSE on test set')\n",
    "\taxes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\t# axes[1, 0].legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/model_results/{output_metric}/{model_type}_model_eval_metrics_{output_metric}_average.png\", facecolor='w')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale\n",
    "\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tfor user, data in list(all_datasets.items()):\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\t# model_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\t# kernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\t# Y_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\t# dense_df = pd.DataFrame({\n",
    "\t\t# \t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t# \t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t# \t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t# \t\t})\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t# optimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\t# optimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\t# plt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\")\n",
    "\t\tplt.ylabel(\"scaling factor\")\n",
    "\t\tplt.plot(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='x', label=user)\n",
    "\tplt.legend()\n",
    "\tplt.savefig(f\"figures/optimal_scale_per_latency/{output_metric}.png\", facecolor='w')\n",
    "\tplt.show()\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale per latency with model prediction\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tfor user, data in list(all_datasets.items())[:1]:\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\tmodel_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\tkernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\tY_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\tplt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\")\n",
    "\t\tplt.ylabel(\"scaling factor\")\n",
    "\t\tplt.scatter(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='x', label=\"measured\")\n",
    "\t\tplt.plot(optimal_scale_dense['latency'], optimal_scale_dense['scale'], label=\"predicted\")\n",
    "\tplt.legend()\n",
    "\t# plt.savefig(f\"figures/optimal_scale_per_latency/{output_metric}.png\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot key metric heatmaps for average over users\n",
    "from utils import annotate_extrema\n",
    "\n",
    "delete_keys = [\"user_lizzie\", \"user_lizzie1\", \"user_lauren\", \"user_sarah1\"]\n",
    "sub_datasets = [all_datasets[key] for key in all_datasets.keys() if key not in delete_keys]\n",
    "\n",
    "combined_df = pd.concat(sub_datasets)\n",
    "averaged_df = combined_df.groupby([\"latency\", \"scale\"]).mean().reset_index()\n",
    "\n",
    "# averaged_df = all_datasets[\"user_lizzie\"]\n",
    "# Create a 2x5 subplot for the heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "# Plot the heatmap for throughput\n",
    "heatmap_throughput = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='throughput')\n",
    "ax = sns.heatmap(heatmap_throughput, ax=axes[0], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[0].set_title('Throughput vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_throughput.values, ax)\n",
    "\n",
    "# Plot heatmap for total error (target deviation + osd)\n",
    "averaged_df['total_error'] = averaged_df['avg_osd'] + averaged_df['avg_target_error']\n",
    "heatmap_error = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='total_error')\n",
    "ax = sns.heatmap(heatmap_error, ax=axes[1], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[1].set_title('Total Error vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_error.values, ax, extrema_type='min')\n",
    "\n",
    "# Plot heatmap for combined performance (movement speed - total error)\n",
    "heatmap_combo = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='weighted_performance')\n",
    "ax = sns.heatmap(heatmap_combo, ax=axes[2], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[2].set_title('Combined Performance vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_combo.values, ax, extrema_type='max')\n",
    "\n",
    "# plt.title(\"User A\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{data_folder}/heatmap_key_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function, e.g., a sine wave function\n",
    "def smooth_2d_function(x, y):\n",
    "    return np.sin(np.sqrt(x**2 + y**2))\n",
    "\n",
    "# Generate sample points\n",
    "x = np.linspace(-5, 5, 10)\n",
    "y = np.linspace(-5, 5, 10)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Apply the function to the sample points\n",
    "z = smooth_2d_function(x, y)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'x': x.ravel(), 'y': y.ravel(), 'z': z.ravel()})\n",
    "\n",
    "X = df[['x', 'y']]\n",
    "Y = df['z']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
    "\n",
    "z_pred, _ = GPRegression(X_train, Y_train, X)\n",
    "z_pred = z_pred.reshape(x.shape)\n",
    "\n",
    "# df[\"y_pred\"] = Y_pred\n",
    "\n",
    "# Plotting the function for visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "ax = axes[0].contourf(x, y, z, cmap='viridis')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Smooth 2D Function')\n",
    "\n",
    "ax = axes[1].contourf(x, y, z_pred, cmap='viridis')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Predictions')\n",
    "\n",
    "fig.colorbar(ax, label='Function Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   latency  scale  col1  col2\n",
      "0        1      3     5     7\n",
      "1        2      4     6     8\n",
      "0        1      3     9    11\n",
      "1        2      4    10    12\n",
      "latency  scale\n",
      "1        3         [5, 9]\n",
      "2        4        [6, 10]\n",
      "Name: col1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataframes\n",
    "df1 = pd.DataFrame({'latency': [1, 2], 'scale': [3, 4], 'col1': [5, 6], 'col2': [7, 8]})\n",
    "df2 = pd.DataFrame({'latency': [1, 2], 'scale': [3, 4], 'col1': [9, 10], 'col2': [11, 12]})\n",
    "\n",
    "# Concatenating the dataframes\n",
    "combined_df = pd.concat([df1, df2])\n",
    "print(combined_df)\n",
    "# Grouping by 'latency' and 'scale' and calculating the average of other columns\n",
    "grouped_df = combined_df.groupby(['latency', 'scale'])['col1'].apply(list)\n",
    "\n",
    "print(grouped_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_samples_with_noise(x_min, x_max, num_x, num_samples, noise_std):\n",
    "    \"\"\"\n",
    "    Generates samples from a sine function with added Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    x_min (float): Minimum x value.\n",
    "    x_max (float): Maximum x value.\n",
    "    num_x (int): Number of distinct x values in the range.\n",
    "    num_samples (int): Number of samples to generate for each x value.\n",
    "    noise_std (float): Standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Generate x values\n",
    "    x_values = np.linspace(x_min, x_max, num_x)\n",
    "\n",
    "    # Plot the underlying sine function\n",
    "    plt.plot(x_values, np.sin(x_values), label='Underlying sine function', color='blue')\n",
    "\n",
    "    # Generate and plot samples with noise for each x value\n",
    "    all_samples = []\n",
    "    for x in x_values:\n",
    "        noisy_samples = np.sin(x) + np.random.normal(0, noise_std, num_samples)\n",
    "        plt.scatter([x]*num_samples, noisy_samples, color='red', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Samples from a Sine Function with Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "generate_samples_with_noise(x_min=0, x_max=2*np.pi, num_x=30, num_samples=10, noise_std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from utils import annotate_extrema\n",
    "from models import BayesRegression\n",
    "\n",
    "### Modeling approach 2. Using Bayesian Regression, and all users as prior\n",
    "\n",
    "# Separate datasets into one vs rest\n",
    "user_to_remove = \"user_jason\"\n",
    "one_user_data = all_datasets[user_to_remove]\n",
    "sub_datasets = [all_datasets[key] for key in all_datasets.keys() if key != user_to_remove]\n",
    "combined_df = pd.concat(sub_datasets)\n",
    "averaged_df = combined_df.groupby([\"latency\", \"scale\"]).mean().reset_index()\n",
    "\n",
    "# Perform Bayesian Regression on combined dataset\n",
    "metric = 'throughput'\n",
    "X = combined_df[['latency', 'scale']]\n",
    "y = combined_df[metric]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X.values)\n",
    "# print(X_poly.shape)\n",
    "# print(y.shape)\n",
    "\n",
    "noise_std = 1\n",
    "model = BayesRegression(X_poly.T, y, noise=noise_std)\n",
    "weight_prior_mean, weight_prior_covar = model.fit()\n",
    "# print(post_mean.shape, post_covar.shape)\n",
    "\n",
    "latency_range = np.arange(0.0, combined_df['latency'].max()+0.01, 0.01)\n",
    "scale_range = np.arange(combined_df['scale'].min(), combined_df['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "latency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "X_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "X_dense = np.round(X_dense, 3)\n",
    "X_dense_poly = poly.transform(X_dense)\n",
    "\n",
    "# Y_pred, _ = model.predict(X_poly.T)\n",
    "Y_pred_dense, _ = model.predict(X_dense_poly.T)\n",
    "dense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "# one_user_data[\"Y_pred\"] = Y_pred\n",
    "\n",
    "# utils.model_heatmaps(one_user_data, dense_df, X, \n",
    "# \t\t\t\t\t user_to_remove, metric, \n",
    "# \t\t\t\t\t \"BayesRegression\", post_mean.flatten())\n",
    "\n",
    "if metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: \n",
    "        extrema_type = \"max\" # optimal scale at maximum\n",
    "else:\n",
    "\textrema_type = \"min\" # optimal scale at minimum\n",
    "\t\t\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "title = (f\"Modeling all but one user\")\n",
    "fig.suptitle(title)\n",
    "\n",
    "# Average over all but one user\n",
    "averaged_data = averaged_df.pivot(\n",
    "\tindex='latency', columns='scale', values=metric\n",
    ")\n",
    "sns.heatmap(averaged_data, cmap='YlGnBu', ax=ax[0], annot=True)\n",
    "# annotate(ax[1], averaged_data, X_train, color='green')\n",
    "ax[0].set_title('Average over all but one user')\n",
    "ax[0].set_xlabel('Scale')\n",
    "ax[0].set_ylabel('Latency')\n",
    "annotate_extrema(averaged_data.values, ax[0], extrema_type)\n",
    "\n",
    "dense_pred_data = dense_df.pivot(\n",
    "\tindex='latency', columns='scale', values='Y_pred_dense'\n",
    ")\n",
    "sns.heatmap(dense_pred_data, cmap='YlGnBu', ax=ax[1])\n",
    "# annotate(ax[1], dense_pred_data, X_train, color='green')\n",
    "ax[1].set_title(f'Predicted Data over Dense Input\\n{weight_prior_mean.flatten()}')\n",
    "ax[1].set_xlabel('Scale')\n",
    "ax[1].set_ylabel('Latency')\n",
    "annotate_extrema(dense_pred_data.values, ax[1], extrema_type)\n",
    "\n",
    "# # Plot residuals\n",
    "# data[\"residual\"] = np.abs(data[\"performance\"] - data[\"Y_pred\"])\n",
    "# residual = data.pivot(\n",
    "# \tindex='latency', columns='scale', values='residual'\n",
    "# )\n",
    "# sns.heatmap(residual, cmap='YlGnBu', ax=ax[2], annot=True)\n",
    "# annotate(ax[2], residual, X_train, color='green')\n",
    "# ax[2].set_title('Residuals')\n",
    "# ax[2].set_xlabel('Scale')\n",
    "# ax[2].set_ylabel('Latency')\n",
    "# annotate_extrema(residual.values, ax[2], 'min')\n",
    "\n",
    "plt.tight_layout()\n",
    "filepath = f\"figures/allbutone_noise_{noise_std}.png\"\n",
    "plt.savefig(filepath, facecolor='w')\n",
    "plt.show()\n",
    "# plt.close()\n",
    "\n",
    "## Use as prior for one user model\n",
    "data = one_user_data\n",
    "X = data[['latency', 'scale']]\n",
    "Y = data[metric]\n",
    "\n",
    "# Initialize evaluation metrics\n",
    "optimal_match_rate = []\n",
    "optimal_scale_error = []\n",
    "mse_scores = []\n",
    "full_mse_scores = []\n",
    "n_train_mse = []\n",
    "n_train_full_mse = []\n",
    "n_train_p = []\n",
    "\n",
    "n = len(data)\n",
    "n_train_values = range(2, n-1)\n",
    "for n_train in n_train_values:\n",
    "\n",
    "\tn_train_p.append(n_train / n)\n",
    "\t# Split into training/test sets\n",
    "\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\tY_train, Y_test = train_set[metric], test_set[metric]\n",
    "\t\n",
    "\t# Create dense test input\n",
    "\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\tpoly = PolynomialFeatures(degree=2)\n",
    "\tX_poly = poly.fit_transform(X.values)\n",
    "\tX_train_poly = poly.transform(X_train.values)\n",
    "\tX_test_poly = poly.transform(X_test.values)\n",
    "\tX_dense_poly = poly.transform(X_dense)\n",
    "\t\n",
    "\t# Train model\n",
    "\tmodel = BayesRegression(X_train_poly.T, Y_train, noise_std)\n",
    "\tmodel.set_prior(weight_prior_mean, weight_prior_covar)\n",
    "\tpost_mean, post_covar = model.fit()\n",
    "\n",
    "\t# Predict\n",
    "\tY_pred, _ = model.predict(X_poly.T)\n",
    "\tY_pred_dense, _ = model.predict(X_dense_poly.T)\n",
    "\n",
    "\t## Evaluate metrics\n",
    "\tdense_df = pd.DataFrame({\n",
    "\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t})\n",
    "\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t# Mean Square Error on whole dataset\n",
    "\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\tif True: #full_mse < 5000:\n",
    "\t\tn_train_full_mse.append(n_train)\n",
    "\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t# Mean Square Error on test set\n",
    "\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\tif True: #mse < 5000:\n",
    "\t\tn_train_mse.append(n_train)\n",
    "\t\tmse_scores.append(mse)\n",
    "\t\n",
    "\tif metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmax()][['latency', 'scale']]\n",
    "\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\telse: # optimal scale at minimum\n",
    "\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmin()][['latency', 'scale']]\n",
    "\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t# Merge the results on 'latency'\n",
    "\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\n",
    "\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t# print(optimal_scale_dense)\n",
    "\t# print(merged_ref_dense)\n",
    "\t\n",
    "\n",
    "\t# Count the number of matches\n",
    "\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t# Visualize model prediction\n",
    "\tif n_train == n-2:\n",
    "\t\tutils.model_heatmaps(data, dense_df, X_train, user_to_remove, metric, \"BayesRegressionwithPrior\", post_mean.flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from models import BayesRegression\n",
    "\n",
    "\n",
    "### Generate example linear dataset\n",
    "\n",
    "# Parameters for the linear relationship\n",
    "a = 2  # slope\n",
    "b = 3  # intercept\n",
    "c = 1\n",
    "noise_std = 30 # standard deviation of the noise\n",
    "\n",
    "np.random.seed(0)  # for reproducibility\n",
    "X = np.linspace(-5, 5, 100)\n",
    "y = []\n",
    "\n",
    "# Generating Y values with noise\n",
    "X_full = []\n",
    "y_full = []\n",
    "for x_val in X:\n",
    "\ty_val = a * x_val**2 + b * x_val + c\n",
    "\ty.append(y_val)\n",
    "\tX_full += [x_val for i in range(10)]\n",
    "\ty_noisy = y_val + np.random.normal(0, noise_std, 10)\n",
    "\ty_full += list(y_noisy)\n",
    "\n",
    "X_full = np.array(X_full)\n",
    "y_full = np.array(y_full)\n",
    "# Creating a DataFrame\n",
    "dataset = pd.DataFrame({'X': X, 'y': y})\n",
    "\n",
    "X_homo = X.reshape((1, -1))\n",
    "X_homo = np.vstack((X_homo, np.ones(X_homo.shape)))\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "# print(X.shape)\n",
    "X_poly = poly.fit_transform(X_full.reshape(-1, 1))\n",
    "# print(X_poly.shape)\n",
    "# print(X_poly[1,:])\n",
    "# print(y.shape)\n",
    "model = BayesRegression(X_poly.T, y_full, noise=1000)\n",
    "model.set_prior(np.array([1, 3, 2]), 0.0001)\n",
    "post_mean, post_covar = model.fit()\n",
    "# print(post_mean)\n",
    "# print(post_covar)\n",
    "\n",
    "# Test inputs\n",
    "test_input = np.arange(-5, 6, 1).reshape(-1, 1)\n",
    "test_input_poly = poly.transform(test_input)\n",
    "# print(test_input_poly.shape)\n",
    "# test_input_homo = np.vstack((test_input, np.ones(test_input.shape)))\n",
    "pred_mean, pred_covar = model.predict(test_input_poly.T)\n",
    "# print(pred_mean)\n",
    "# print(pred_covar.shape)\n",
    "\n",
    "# print(X.shape, y.shape, y_noisy.shape)\n",
    "plt.scatter(X_full, y_full, marker='x')\n",
    "plt.plot(X, y, linestyle='--', color='black')\n",
    "plt.scatter(test_input, pred_mean, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.0\n",
      "[28.52964860618778, -6.912388498020192, 17.67670776019209, 15.640582448711346, 19.10796841633197]\n",
      "comparing scale 0.1 against baseline\n",
      "  Scale: 0.1, Values: [5.859171189176864, -24.422816341892812, 0.03888165943062383, -7.634886184650977, -6.883074070694837]\n",
      "0.9998945258819952\n",
      "comparing scale 0.15 against baseline\n",
      "  Scale: 0.15, Values: [13.42968595226474, -5.088029221047723, 2.82110636594539, 15.632127564402888, -10.263440963940212]\n",
      "0.9427751455177883\n",
      "comparing scale 0.2 against baseline\n",
      "  Scale: 0.2, Values: [23.4745634503754, 15.436422125731134, 21.84819335551348, 14.596988980835876, 12.881259526255871]\n",
      "0.3072454897603878\n",
      "comparing scale 0.4 against baseline\n",
      "  Scale: 0.4, Values: [38.89315110295446, 27.175667347409316, 40.59204130100416, 27.315549640396817, 16.252965026243373]\n",
      "0.035491443472708745\n",
      "comparing scale 0.7 against baseline\n",
      "  Scale: 0.7, Values: [31.110113689819954, 25.10277305829859, 36.70409060313884, 20.319422156434694, 24.810341932481244]\n",
      "0.04225291217894224\n",
      "Latency: 0.25\n",
      "[-25.532145356598967, -41.2136954972071, -6.690605248033769, -20.586473850048726, -20.33792317189798]\n",
      "comparing scale 0.1 against baseline\n",
      "  Scale: 0.1, Values: [3.6978989534736604, -9.805236701035728, -9.155750094371815, -1.9795631948318864, -17.95685691805898]\n",
      "0.04135156800843631\n",
      "comparing scale 0.15 against baseline\n",
      "  Scale: 0.15, Values: [3.3684377498978497, -3.392425851094486, -8.970034582563514, 5.031429169937022, -3.7000608203204743]\n",
      "0.017519552801321735\n",
      "comparing scale 0.2 against baseline\n",
      "  Scale: 0.2, Values: [8.962591524020217, 6.051034860362588, 0.8997442617694134, 5.567332527432725, 0.7339049499185197]\n",
      "0.007303744140921186\n",
      "comparing scale 0.4 against baseline\n",
      "  Scale: 0.4, Values: [9.876864594624598, -7.403100630885461, 3.4829540577793274, 5.027988872882911, -11.935589905028896]\n",
      "0.008310252325202473\n",
      "comparing scale 0.7 against baseline\n",
      "  Scale: 0.7, Values: [-2.390681397014191, -5.994301685345263, -13.14389115179073, 10.439191238348899, -30.829851499313733]\n",
      "0.10275211563513116\n",
      "Latency: 0.5\n",
      "[-69.6934406983263, -53.78333271977958, -98.92599704056433, -66.25203387529952, -15.112592891637068]\n",
      "comparing scale 0.1 against baseline\n",
      "  Scale: 0.1, Values: [-3.7378040939433514, -18.96513783435083, -8.35513498616941, -11.033077473565491, -16.031498232719578]\n",
      "0.01662663712845248\n",
      "comparing scale 0.15 against baseline\n",
      "  Scale: 0.15, Values: [-3.6647623177050566, -15.27573480949829, -22.171041255779983, -5.3800820530646, -28.089321295081238]\n",
      "0.02273994751182357\n",
      "comparing scale 0.2 against baseline\n",
      "  Scale: 0.2, Values: [0.680950401206637, -13.100734038517258, -6.049669862653456, -2.9069243767891564, -31.78057269245651]\n",
      "0.027435388649220578\n",
      "comparing scale 0.4 against baseline\n",
      "  Scale: 0.4, Values: [-7.471022103682953, -31.32135351930721, -48.68783692134828, -6.346615758704587, -21.263864634706145]\n",
      "0.022252483836374164\n",
      "comparing scale 0.7 against baseline\n",
      "  Scale: 0.7, Values: [-54.53512628759939, -69.50540845251545, -18.79228710526752, -21.867488851273368, -5.7493919002382885]\n",
      "0.09000876685426246\n",
      "Latency: 0.75\n",
      "[-109.86960987928315, -95.84357098682798, -74.22969396403113, -59.863592835840535, -87.85359703347811]\n",
      "comparing scale 0.1 against baseline\n",
      "  Scale: 0.1, Values: [-8.950130921160358, -11.746240295687459, -17.908364338772476, -10.466784199007764, -26.121808535529276]\n",
      "0.0009066015244750882\n",
      "comparing scale 0.15 against baseline\n",
      "  Scale: 0.15, Values: [-18.686828242473233, -31.742998410963732, -23.209258865452988, -11.66868491537208, -43.79401393998577]\n",
      "0.0011067505670558813\n",
      "comparing scale 0.2 against baseline\n",
      "  Scale: 0.2, Values: [-21.13307050787938, -33.11109140334031, -61.958805225058086, -12.82557810879841, -28.110292533509657]\n",
      "0.00611763707111123\n",
      "comparing scale 0.4 against baseline\n",
      "  Scale: 0.4, Values: [-60.49254771723502, -68.1211956045921, -50.69289027917783, -19.47547277380947, -16.791524208171836]\n",
      "0.003773382979589239\n",
      "comparing scale 0.7 against baseline\n",
      "  Scale: 0.7, Values: [-70.5922600277856, -67.32515266080735, -84.64972853061282, -36.348310571917615, -23.139223993281963]\n",
      "0.037519062282243325\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA680lEQVR4nO3deXiU5bn48e89SzIJJBOSsGaCrFFZElRwqR5F0OJSkVYrWG3deqRWj4qtiq31x9Haaj11q8vRLselR9GjrYWCtYoLilVBVgFBZDEJQSArSWaSzMzz+2MmyWQyCZOQyUwm9+e65pqZd73fTJJ7nvd93ucWYwxKKaVUorHEOwCllFIqEk1QSimlEpImKKWUUglJE5RSSqmEpAlKKaVUQrLFO4Cuys3NNaNGjYp3GEoppXrIp59+etAYMzh8ep9LUKNGjWLNmjXxDkMppVQPEZE9kabrKT6llFIJSROUUkqphKQJSimlVELqc9eglFKqr2lqaqKkpASPxxPvUOLK4XDgcrmw2+1RLa8JSimlYqykpISMjAxGjRqFiMQ7nLgwxlBeXk5JSQmjR4+Oah09xaeUUjHm8XjIycnpt8kJQETIycnpUisyZglKRP4kIvtF5LMO5ouIPCoiO0Rko4gcH6tYmr22rpRT73ub0QuXcep9b/PautJY71IppQD6dXJq1tWfQSxbUM8A53Qy/1xgfPBxLfBkDGPhtXWl3PGXTZRWuTFAaZWbO/6ySZOUUkolqJglKGPMSqCik0UuBJ4zAR8BWSIyPFbxPPDGNtxNvjbT3E0+HnhjW6x2qZRSCeMf//gHRx99NOPGjeO+++6LuExDQwNz585l3LhxnHTSSezevbt3gwwTz2tQeUBxyPuS4LR2RORaEVkjImsOHDjQrZ3trXJ3OF2LNiqlEklPX47w+Xxcf/31vP7662zZsoUXX3yRLVu2tFvuj3/8I4MGDWLHjh0sWLCA22+//Yj2e6T6RCcJY8zTxpipxpipgwe3G64pKiOy0iJvG5j92Cr+sraEBq8v4jJKKdVbYnE54pNPPmHcuHGMGTOGlJQU5s2bx9/+9rd2y/3tb3/jiiuuAODiiy9mxYoV7b7Al5WVcfrppzNlyhQmTZrE+++/3+24Diee3cxLgfyQ967gtJi4ddbR3PGXTW1O8znsFr5VOIJ1X1Vyy8sb+NXyz7nspJFcdvJIhmQ4YhWKUqof+8+lm9myt6bD+eu+qqLR528zzd3k47ZXNvLiJ19FXGfCiEz+3wUTO9xmaWkp+fmt/25dLhcff/xxp8vZbDacTifl5eXk5ua2LPPCCy8wa9Ysfv7zn+Pz+aivr+9wv0cqnglqCXCDiCwGTgKqjTFlsdrZnOMCZw8feGMbe6vcjMhK49ZZRzPnuDz8fsMHOw7yP6t28ciKL3ji3R18q3AEV506ikJXVqxCUkqpdsKT0+Gm97Zp06Zx9dVX09TUxJw5c5gyZUrM9hWzBCUiLwLTgVwRKQH+H2AHMMb8N7AcOA/YAdQDV8UqlmZzjstrSVShLBbh9ILBnF4wmJ0HannuX3v4vzXF/HVdKSccNYirTh3FrInDsFv7xBlRpVQC66ylA3DqfW9TGuGaeV5WGi/NP6Vb+8zLy6O4uPWSf0lJCXl57f8XNi/ncrnwer1UV1eTk5PTZpnTTz+dlStXsmzZMq688kpuueUWfvCDH3QrrsOJWYIyxlx6mPkGuD5W+++uMYMHsmj2RG75ZgH/t6aEZz/czQ0vrGO408HlJx/FpSeOJHtASrzDVEolqUiXI9LsVm6ddXS3tzlt2jS++OILdu3aRV5eHosXL+aFF15ot9zs2bN59tlnOeWUU3jllVeYMWNGu3uX9uzZg8vl4t///d9paGhg7dq1fS9B9XWZDjvXnDaaK78xinc+38//fLiLB97YxqMrvmDOlDyuOm0UxwzLjHeYSqkk09nliO6y2Ww89thjzJo1C5/Px9VXX83EiYGW3F133cXUqVOZPXs211xzDd///vcZN24c2dnZLF68uN223n33XR544AHsdjsDBw7kueee63ZchyN9rYv11KlTTbwKFm7bd4hnPtzNX9eV4Gnyc8qYHK48dRRnHTsUq0XvEldKRbZ161aOPfbYeIeRECL9LETkU2PM1PBl9aJKFxw9LINff2cyH90xk4XnHsOe8jrmP/8p0//rHf7w/k6q3U3xDlEppZKGnuLrhqz0FH50xlh+eNpo/rnla55ZtZtfLtvKg29u56LjXVx56ijGDh7Ia+tKe7SZrpRS/YkmqCNgs1o4b/Jwzps8nM9Kq/mfVbt5aXUxz3+0h2OGZbDzQF1L19Dmm+0ATVJKKRUFPcXXQyblOfntJUWsWjiDW84uYPvXhyLebKdj/ymlVHQ0QfWwwRmp3DhzPB31PeloTECllFJtaYKKkY7G/utoulJKqbY0QcXIrbOOJs1ubTPtSG+2U0qp7oqm3MaDDz7IhAkTKCwsZObMmezZs6dlntVqZcqUKUyZMoXZs2f3SszaSSJGQm+2K61yY7cKv/7OZO0goZQ6vI0vw4q7oboEnC6YeRcUXtLtzTWX23jzzTdxuVxMmzaN2bNnM2HChDbLHXfccaxZs4b09HSefPJJbrvtNl566SUA0tLSWL9+/ZEcVZdpCyqG5hyXx6qFM7jmtNFYRDi/MGb1GJVSyWLjy7D0RqguBkzgeemNgendFG25jTPPPJP09HQATj75ZEpKSrq0n0cffbSlBTZv3rxux9tMW1C9oNDlpMHrZ9u+Q0zKc8Y7HKVUPL2+EPZt6nh+yWrwNbSd1uSGv90Anz4beZ1hk+HcyKftIPpyG6H++Mc/cu6557a893g8TJ06FZvNxsKFC5kzZ067de677z527dpFamoqVVVVnW4/GpqgesGU/CwANpZUa4JSSnUuPDkdbnoM/PnPf2bNmjW89957LdP27NlDXl4eO3fuZMaMGUyePJmxY8e2Wa+wsJDLLruMOXPmRExgXaUJqheMzE4nK93OxpIqvnfSyHiHo5SKp05aOgA8NCl4ei+MMx+uWtatXUZbbgPgrbfe4t577+W9994jNTW1zTYAxowZw/Tp01m3bl27BLVs2TJWrlzJ0qVLuffee9m0aRM2W/fTjF6D6gUiwuQ8JxtKquMdilIq0c28C+xht6PY0wLTuym03EZjYyOLFy+O2BNv3bp1zJ8/nyVLljBkyJCW6ZWVlTQ0BFpwBw8eZNWqVe06WPj9foqLiznzzDO5//77qa6upra2ttsxg7agek2RK4sn3/sSd6OPtBTr4VdQSvVPzb31erAXX7TlNm699VZqa2v57ne/C8DIkSNZsmQJW7duZf78+VgsFvx+PwsXLmyXoHw+H5dffjnV1dUYY7jxxhvJysrqdsyg5TZ6zT837+Pa5z/llR+dwtRR2fEORynVi7TcRistt5GAmjtK6Gk+pZSKjiaoXjIk08GwTAcbS6riHYpSSvUJmqB6UaHLyUZtQSmlVFQ0QfWiovwsdh2so7peK+8qpdThaILqRUWuLAA2llbFNQ6llOoLNEH1osmuwCgSeppPKaUOTxNUL3Km2RmdO4ANxVXxDkUp1c9EU27jmWeeYfDgwS1lNf7whz/0cpRt6Y26vazQ5eTjnRXxDkMplcCW7VzGI2sfYV/dPoYNGMZNx9/E+WPO7/b2oi23ATB37lwee+yxIwm/x2gLqpcVurLYV+Ph6xpPvENRSiWgZTuXsejDRZTVlWEwlNWVsejDRSzb2b1x+CD6chvRqKur4/zzz6eoqIhJkya11IuKBW1B9bIp+YHrUBuKq/jmxGFxjkYp1dvu/+R+Pq/4vMP5Gw9spNHf2Gaax+fhrlV38cr2VyKuc0z2Mdx+4u0dbrMr5TZeffVVVq5cSUFBAQ899FCb9SBwqnDEiBEsWxZImNXVsbumri2oXjZhuBOrRbSjhFIqovDkdLjpPemCCy5g9+7dbNy4kbPPPpsrrrii3TKTJ0/mzTff5Pbbb+f999/H6YxdCSFtQfWytBQrBUMz2KAjSijVL3XW0gH45ivfpKyurN304QOG8z/n/E+39hltuY2cnJyW1z/84Q+57bbb2i1TUFDA2rVrWb58OXfeeSczZ87krru6P9J6Z7QFFQdFwREl+tpAvUqp2Lvp+JtwWB1tpjmsDm46/qZubzPachtlZa2JccmSJREHuN27dy/p6elcfvnl3Hrrraxdu7bbcR2OtqDioNCVxeLVxewpr2dU7oB4h6OUSiDNvfV6shdftOU2Hn30UZYsWYLNZiM7O5tnnnmm3bY2bdrErbfeisViwW638+STT3Y7rsPRchtxsHlvNec/+gGPzJvChVMiV7VUSiUPLbfRSsttJLiCoRmk2izaUUIppTqhCSoO7FYLE0dkaukNpZTqREwTlIicIyLbRGSHiCyMMH+kiLwjIutEZKOInBfLeBJJoSuLTaXVeH3+eIeilFIJKWYJSkSswOPAucAE4FIRCR9X407gZWPMccA84IlYxZNopuRn4Wny88X+2niHopRSCSmWLagTgR3GmJ3GmEZgMXBh2DIGyAy+dgJ7YxhPQilsGdm8Kr6BKKVUgoplgsoDikPelwSnhVoEXC4iJcBy4D8ibUhErhWRNSKy5sCBA7GItdeNyhlAhsPGBu0ooZRSEcW7k8SlwDPGGBdwHvC8iLSLyRjztDFmqjFm6uDBg3s9yFiwWCRYAr4q3qEopfqBaMptLFiwoKXURkFBAVlZWS3zrFZry7xIN/nGQixv1C0FQkcZdAWnhboGOAfAGPMvEXEAucD+GMaVMApdWfx+5U48TT4cdmu8w1FKJYjqpUvZ/9DDeMvKsA0fzpAFN+O84IJuby/achsPPfRQy+vf/e53rFu3ruV9Wloa69ev73YM3RHLFtRqYLyIjBaRFAKdIJaELfMVMBNARI4FHEBynMOLQpErC6/fsKWsJt6hKKUSRPXSpZT94i68e/eCMXj37qXsF3dRvXRpt7fZnXIbL774IpdeemmX9vPoo48yYcIECgsLmTdvXrfjbRazFpQxxisiNwBvAFbgT8aYzSJyN7DGGLME+AnwexFZQKDDxJWmrw1tcQSKgqU3NhZXcfzIQXGORinVG/b96lc0bO243IZ7wwZMY9uRy43HQ9nP76Tq5f+LuE7qsccw7Gc/63CbXSm3AbBnzx527drFjBkzWqZ5PB6mTp2KzWZj4cKFzJkzp9169913H7t27SI1NZWqqqoOtx+tmI7FZ4xZTqDzQ+i0u0JebwFOjWUMiWxYpoPBGak6ooRSqkV4cjrc9FhYvHgxF198MVZr66WHPXv2kJeXx86dO5kxYwaTJ09m7NixbdYrLCzksssuY86cORETWFfpYLFxJCIUuZxaekOpfqSzlg7AFzNmBk7vhbGNGMFRzz/XrX1GW26j2eLFi3n88cfbbQNgzJgxTJ8+nXXr1rVLUMuWLWPlypUsXbqUe++9l02bNmGzdT/NxLsXX79X6MriywN11Hia4h2KUioBDFlwM+JoW25DHA6GLLi529uMttwGwOeff05lZSWnnHJKy7TKykoaGhoAOHjwIKtWrWrXwcLv91NcXMyZZ57J/fffT3V1NbW1RzYQgbag4qwoPwuAz0qq+ca43PgGo5SKu+beej3Ziy/achsQaD3NmzcPEWlZf+vWrcyfPx+LxYLf72fhwoXtEpTP5+Pyyy+nujpQ6+7GG29s0029O7TcRpxV1jVy3D1vcvs5x3Dd9LGHX0Ep1edouY1WWm6jDxk0IIWR2el6w65SSoXRBJUACoMl4JVSSrXSBJUAilxZlFa5OXCoId6hKKVipK9dTomFrv4MNEElgOaOEnqaT6nk5HA4KC8v79dJyhhDeXk5jrAeip3RXnwJYFJeJhaBDSXVzDx2aLzDUUr1MJfLRUlJCclSjaG7HA4HLpcr6uU1QSWA9BQb44dkaAtKqSRlt9sZPXp0vMPoc/QUX4Jo7ijRn08BKKVUKE1QCaIoP4uKukZKKt3xDkUppRKCJqgEUeTKAtBx+ZRSKkgTVII4elgGKVaL3g+llFJBmqASRIrNwrEjMtlQXBXvUJRSKiFogkogRS4nm0qr8fm1o4RSSmmCSiBFrizqG318eeDIhqhXSqlkoAkqgTSXgNfTfEoppQkqoYzJHcjAVJt2lFBKKTRBJRSLRZiUl6kjSiilFJqgEk6RK4stZTU0eH3xDkUppeJKE1SCKcrPosln+LzsULxDUUqpuNIElWAKXYGOEnqaTynV32mCSjB5WWnkDEhhg3aUUEr1c5qgEoyIBEc2r4p3KEopFVeaoBJQUX4WX+yvpbbBG+9QlFIqbjRBJaAiVxbGwGeleppPKdV/aYJKQNpRQimlNEElpJyBqeRlpWlHCaVUv6YJKkEV5WtHCaVU/6YJKkEVubIornBTXtsQ71CUUiouNEElqMJgCfiN2lFCKdVPaYJKUJNdTkRgY7EmKKVU/6QJKkENTLUxdvBAvQ6llOq3YpqgROQcEdkmIjtEZGEHy1wiIltEZLOIvBDLePqaQpeTDSXVGKMl4JVS/U/MEpSIWIHHgXOBCcClIjIhbJnxwB3AqcaYicDNsYqnL5qSn8XB2gb2VnviHYpSSvW6WLagTgR2GGN2GmMagcXAhWHL/DvwuDGmEsAYsz+G8fQ5LR0ltAS8UqofimWCygOKQ96XBKeFKgAKRGSViHwkIudE2pCIXCsia0RkzYEDB2IUbuI5dngGdqvoDbtKqX4p3p0kbMB4YDpwKfB7EckKX8gY87QxZqoxZurgwYN7N8I4SrVZOWaYloBXSvVPsUxQpUB+yHtXcFqoEmCJMabJGLML2E4gYamgonwnm0qq8fu1o4RSqn+JZYJaDYwXkdEikgLMA5aELfMagdYTIpJL4JTfzhjG1OcUurI41OBl58G6eIeilFK9KqoEJSJpInJ0VzZsjPECNwBvAFuBl40xm0XkbhGZHVzsDaBcRLYA7wC3GmPKu7KfZFfU3FFCT/MppfqZwyYoEbkAWA/8I/h+ioiEt4QiMsYsN8YUGGPGGmPuDU67yxizJPjaGGNuMcZMMMZMNsYs7vaRJKlxQwaSnmJlo3aUUEr1M9G0oBYR6DJeBWCMWQ+MjllEqg2rRZg0wskGbUEppfqZaBJUkzEm/Ou7XrHvRUX5TjbvraHR6493KEop1WuiSVCbReR7gFVExovI74APYxyXClHoyqLR62f714fiHYpSSvWaaBLUfwATgQbgRaAGHZKoVzV3lNDTfEqp/uSwCcoYU2+M+bkxZlrwZtmfG2N0cLhelJ+dxqB0u5beUEr1K7bDLSAi7xDhmpMxZkZMIlLtiAiTXVnaglJK9SuHTVDAT0NeO4CLAG9swlEdmeJy8tg7B6hv9JKeEs3HppRSfdth/9MZYz4Nm7RKRD6JUTyqA4WuLPwGNu+tYdqo7HiHo5RSMRfNjbrZIY9cEZkFOHshNhWiMD/wI9+gpTeUUv1ENOeKPiVwDUoInNrbBVwTy6BUe0MyHAx3OnRECaVUvxHNKT4dNSJBFLqcOiafUqrf6DBBich3OlvRGPOXng9HdaYoP4s3Nn9NVX0jWekp8Q5HKaViqrMW1AWdzDOAJqhe1jqyeTWnF/Sfwo1Kqf6pwwRljLmqNwNRhzcpL9BRYmNJlSYopVTSi+qGGhE5n8BwR47macaYu2MVlIrMmWZnTO4ANmhHCaVUPxBNN/P/BuYSGJNPgO8CR8U4LtWBovws7SihlOoXohks9hvGmB8AlcaY/wROIVCaXcVBocvJ1zUN7KvW4RCVUsktmgTlDj7Xi8gIoAkYHruQVGcKdWRzpVQ/EU2C+ruIZAEPAGuB3cALMYxJdWLiiExsFtHTfEqppBfNjbr3BF++KiJ/BxwRKuyqXuKwWykYmqEjSiilkl40nSQ2isjPRGSsMaZBk1P8FeVnsaG4CmPaVUFRSqmkEc0pvgsIjMH3soisFpGfisjIGMelOlHkclLj8bK7vD7eoSilVMxEU1F3jzHmN8aYE4DvAYUEBoxVcVLYMqJEVVzjUEqpWIqmBYWIHCUitwGLgWOA22IalepUwdCBOOwWNmgJeKVUEoum5PvHgB14GfiuMWZnzKNSnbJZLUwcoSObK6WSWzRDHf3AGLMt5pGoLilyZfHCJ3vw+vzYrFE1hJVSqk+J5hqUJqcEVJTvxNPkZ/vXtfEORSmlYkK/evdR2lFCKZXsNEH1UaNy0sl02HRkc6VU0oqmk4QD+DFwGoFChR8ATxpjdLTSOBIRHdlcKZXUomlBPUegFtTvgMeACcDzsQxKRafQ5eTzfYfwNPniHYpSSvW4aHrxTTLGTAh5/46IbIlVQCp6ha4sfH7D5r01nHDUoHiHo5RSPSqaFtRaETm5+Y2InASsiV1IKlpF2lFCKZXEoklQJwAfishuEdkN/AuYJiKbRGRjZyuKyDkisk1EdojIwk6Wu0hEjIhM7VL0/dwwp4MhGak6srlSKilFc4rvnO5sWESswOPA2UAJsFpElhhjtoQtlwHcBHzcnf30d0X5WVq8UCmVlKIdLLbDRyerngjsMMbsNMY0EhjH78IIy90D3A9or8BuKHI52Xmgjmp3U7xDUUqpHhXL+6DygOKQ9yXBaS1E5Hgg3xizLIZxJLXmG3Y/K9XTfEqp5BK3G3VFxAI8CPwkimWvFZE1IrLmwIEDsQ+uDyl0OQH0NJ9SKunEMkGVAvkh713Bac0ygEnAu8HOFycDSyJ1lDDGPG2MmWqMmTp48OAYhtz3ZKWncFROOhu19IZSKsnEMkGtBsaLyGgRSQHmAUuaZxpjqo0xucaYUcaYUcBHwGxjjHZh76Iil44ooZRKPjFLUMYYL3AD8AawFXjZGLNZRO4Wkdmx2m9/VOhysrfaw/5D2s9EKZU8oulm3m3GmOXA8rBpd3Ww7PRYxpLMivKzANhYXM1ZExzxDUYppXqIjmaeBCaOyMQiOqKEUiq5aIJKAukpNgqGZmjpDaVUUtEElSSaO0oYY+IdilJK9QhNUEmiMN9JZX0TxRXueIeilFI9QhNUkmge2Vxv2FVKJQtNUEni6GEZpNgs2lFCKZU0NEElCbvVwsQRmdpRQimVNDRBJZEiVxaflVbj82tHCaVU36cJKokUupzUN/rYsb823qEopdQR0wSVRAq1o4RSKologkoiY3IHkJFq044SSqmkENOx+FTvsliEYc5UXlpdzP9+9BUjstK4ddbRzDku7/ArK6VUgtEElUReW1fKroP1eIOdJEqr3Nzxl00AmqSUUn2OnuJLIg+8sa0lOTVzN/l44I1tcYpIKaW6TxNUEtlbFXmYo46mK6VUItMElURGZKVFnO5Ms9Pk8/dyNEopdWQ0QSWRW2cdTZrd2maaCFS5mzjrwff467oSvYlXKdVnaIJKInOOy+PX35lMXlYaAuRlpfHgd4v4ww+mkp5iY8FLG5j18EqWbyrDr4lKKZXgpK/VD5o6dapZs2ZNvMPoc/x+w+uf7eOht7azY38txw7P5CdnFzDz2CGISLzDU0r1YyLyqTFmavh0bUH1ExaLcH7hcN64+XQemltEfaOXHz63hjlPfMjK7Qe00KFSKuFoC6qfavL5+cvaEh5dsYPSKjcnjsrmJ98s4KQxOfEOTSnVz3TUgtIE1c81eH28tLqYx97ewf5DDZw2LpdbvlnA8SMHxTs0pVQ/oQlKdcrT5OPPH+3hyXe/pLyukRnHDOGWswuYlOeMd2hKqSSnCUpFpa7ByzMf7uap976kxuPl3EnDWHB2AQVDM+IdmlIqSWmCUl1S7W7ijx/s4k8f7KKu0cvsohHcfFYBo3MHxDs0pVSS0QSluqWyrpGnVu7k2Q930+jzc9HxefzHjPHkZ6fHOzSlVJLQBKWOyIFDDTz57pf8+eM9GGOYOy2fG84czzCnI96hKaX6OE1QqkeUVbt5/J0dLP6kGItFuPyko7hu+lgGZ6TGOzSlVB+lCUr1qOKKeh5d8QWvri0h1WblylNHce2/jWHQgJR4h6aU6mM0QamY2HmglkdWfMGSDXsZkGLjmtNGc82/jSbTYY93aEqpPkITlIqpbfsO8fBb23n9s3040+xce/oYrvzGKAakatFmpVTnNEGpXvFZaTUPvrmdtz/fT86AFK6bPpbLTz4KR1gZEKWUaqYJSvWqT/dU8tCb2/lgx0GGZqZyw5njuGRaPqk2TVRKqbY0Qam4+NeX5Tz45jZW764kLyuNG2eO4zvHu7BbdSB9pVRAXMptiMg5IrJNRHaIyMII828RkS0islFEVojIUbGMR/W+U8bm8PL8U3ju6hPJHZjC7a9u4myt7quUikLMEpSIWIHHgXOBCcClIjIhbLF1wFRjTCHwCvCbWMWj4kdEOL1gMK9dfyp/+MFU0rS6r1IqCrFsQZ0I7DDG7DTGNAKLgQtDFzDGvGOMqQ++/QhwxTAeFWciwlkThrLsP07j8e8dD8CP/3ct3/rdB7y15WstmqiUaiOWCSoPKA55XxKc1pFrgNcjzRCRa0VkjYisOXDgQA+GqOIhvLpvnVb3VUpFkBBXqkXkcmAq8ECk+caYp40xU40xUwcPHty7wamYsVqEbx/n4q1bzuD+iyZz8FADP/jTJ8x96iM+3lke7/CUUnEWywRVCuSHvHcFp7UhImcBPwdmG2MaYhiPSlB2q4W500by9k/P4O4LJ7K7vI65T3/E9//4Meu+qox3eEqpOIlZN3MRsQHbgZkEEtNq4HvGmM0hyxxHoHPEOcaYL6LZrnYzT37N1X2fePdLKuoamXnMEBZodV+lklZc7oMSkfOAhwEr8CdjzL0icjewxhizRETeAiYDZcFVvjLGzO5sm5qg+g+t7qtU/6A36qo+K7y674VFI7hJq/sqlTQ0Qak+T6v7KpWcNEGppKHVfZVKLpqgVNIpq3bz2Ns7eGm1VvdVqi/TBKWSVqTqvvNPH0NWulb3Vaov0ASlkl5odd+BKTau1uq+SvUJmqBUv7Ft3yEeenM7/9gcqO47/4wxXHGKVvdVKlFpglL9jlb3Vapv0ASl+q1I1X3nThtJii0hhqJUqt/TBKX6vfDqvjfNHM93js/DptV9lYqruFTUVSqRhFf3ve3VjZz14Hu8tq5Uq/sqlYA0Qal+JbS67++D1X1vfmk95zy8kte1uq9SCUUTlOqXRISzQ6r7+o3humB13xVbtbqvUolAE5Tq15qr+/5zwRkt1X2veXYN337iQ97/Qqv7KtWhjS/DQ5NgUVbgeePLPb4L7SShVIgmn5+/rC3h0RU7KK1yc+LobH5ydgEnjcmJd2hKJY6NL8PSG6HJ3TrNngYXPAqFl3R5c9qLT6kuaPD6eGl1MY+9vYP9hxr4t/G53HJ2AceNHBTv0JSKH78favfBU2dA3f728535sOCzLm9WE5RS3aDVfVW/0uSG6hKoLoaq4tbX1SVQ9RXU7AV/EwDLBqTzyKAs9tmsDPP6uKmyivPr3LCoqsu71QSl1BEIr+573uRhLDirgPFa3Vf1FcZAfUUw4TQnneLW91XFUH+w7TpigYwR4HRBVn7g2ZnPsn/dz6IMOx5LazcGh9/Ponrh/Ot7rgWlg5MpFYUBqTauP3Mcl598VEt139c/26fVfVXi8DUFWjgtrZ4IraCm+rbr2NMDp+WcLhheFExAI8HpwjhduNOyKG+qocJTQYW7gsqGSio8FfxhUAYef2ObTXksFh4Z5OT8HjwkbUEp1Q3N1X2f+XAXTT6j1X1V7DUcCkk4X4W0gIIJ6FAZGH/bdQYMbk1AWSNpyhxORfogKlIHUJHioMLfSEVDJeWe8tYE5K4IJCRPBR6fp0shCsLGKzZ2+dD0FJ9SMbD/kIcn3/2S//34K4wxzJs2kuvPHKfVfVXX+P2BTgehp9zaJKCvwFPddh2LHZ9zBNXOPCoGDqZywCDKUwdQYU+lwiJU4KWisaYl2VR4KjjUeCji7u0WOzlpOQxKHUR2WjY5jhyyHdltH8HpgxyDuOCvF1BWV9ZuO8MHDOefF/+zy4evCUqpGAqv7vv9kwPVfXMHanVfBTR5oKY00NEgvONBdUlgnq8RA9SJUGG1UuHIpDxzMBXpWVSmDqDCnkKFxUIFPsp9bioaa6hqqMIf3moCLGIhKzWLbEdrshnkGNSSaEKnZzuyGWAfgIhEfTjLdi5j0YeL2rSwHFYHi76xiPPHdP0knyYopXqBVvfth4wBd2WEaz6B9w3VJVR6yim3WgKJp/nhyKTC0Zx4hArjpcLnodF4I+4mw57RklwitW6yU1tfO1OcWC2xLSuzbOcyHln7CPvq9jFswDBuOv6mbiUn0ASlVK8Kr+57zb+N5urTtLpvn+TzwqG9LafcvFV7qKraRUVNMZW1ZVS4y6mgiXKrlQqLlQqrhQqbnQq7nQqLUEfk/7Gp1tSWU2bhp9HCE9AgxyBSrMn7JUcTlFJxEKm675XfGEV6inagTRTGc4hD5duoKN9GReWXVNSUUFFXRrm7nMqGGir87sCptWALqMpiwUQ4HWbFwqCUDLLTcshOH9JxSyd4ei3Nltal02rJTBOUUnEUWt03d2AKPzpDq/vGktvrptJTSUX9QSqqdlFetZOKmmIq6vYFWjyNNVT66ik3XiosgreDRJGJjWxbGtkpTnLScsgeMIzszHyyBwxtadk0t3gyUzOxiA5v2h2aoCAwftSKuwNNdacLZt7VrXGjlOqudtV9Z4xn7tR8re57GE3+Jqo8VVR4Kij3lAeSj6eCivoDrYnHU0554yEqvPW4ad9xACDN7yfbb8gRO4Ns6WSnOAMtngHDyM4cSXb2GHKyxpKdPpgsRxZ2i56S7chr60p54I1t7K1yMyIrjVtnHc2c4/K6ta2kSVAZGRnmhBNOaDPtkksu4cc//jH19fWcd9557da58sorufL4dA4uvoGLX6honSEWyB3PdT+9i7lz51JcXMz3v//9duv/5Cc/4YILLmDbtm3Mnz+/3fw777yTs846i/Xr13PzzTe3m/+rX/2Kb3zjG3z44Yf87Gc/azf/4YcfZsqUKbz11lv88pe/bDf/qaee4uijj2bp0qX89re/bTf/+eefJz8/n5deeoknn3yy3fxXXnmF3NxcnnnmGZ555pl285cvX056ejpPPPEEL7/cfkTid999F4D/+q//4u9//3ubeWlpabz++usA3HPPPaxYsaLN/JycHF599VUA7rjjDv71r3+1me9yufjzn/8MwM0338z69evbzC8oKODpp58G4Nprr2X79u1t5k+ZMoWHH34YgMsvv5ySkpI280855RR+/etfA3DRRRdRXl7eZv7MmTP5xS9+AcC5556L2+1uM/9b3/oWP/3pTwGYPn064aL63bvySg4ePMjFF1/cMr3G3URxpRuOPZuCU2Zx2aQBvPTA7YR/kU/m3z2v38tTLz2FW9w8+/tnWbF0BV6/lyZ/U8vziYtOoMJTwZdLdlOzvqbN+pYUC+NuOYpsn499Sw5QubUOmwG7WLBZrGRnDuC+X3yL7MyRPPXcajZuKcViTwNL4PRqf/3da3bdddd1+//ewdoGjhp0DLce/Jzyyn3cc7Cc/QNzyHYNa+m52pXfvbPPPrufjySx4m7wtv0FwPihcndcwlH9W2aanYlpds44cxzrbSn8avlWakuqcA1KAwPFlW4avD5ue2UDPtfxHJvg9/8aY/Abf5vk4vV7eX7L8zQ6Gnlv23tsq9yG1+9tmW+M4ZKll2BJtVD+RTnVNdUIYEWwAzbjZ3zZVrL9fj51u9nW5MWGYLPasFtSybAN4J+jvo9k5XPP5rdZUf4ZWFNpzvI5OTmcccHvAcj4xx1Yvqzp+AB6+ucB+PyGBq8Pvx+8foPPb1q6SxhjqGvwsr/Gg99AQ5OPBq8fDDQvdbC2gS++PoTPGOobfS2lX5q38VVFHat3V1BfV0eNu6llngm+2FJWzVtbvqay4iAVdY2t6wb38cmucqyflrC/bC/7DzW0xNW83Ftbvmb/oF2U7Slmb5W7zb7r9u3nhgNfMzQtlQogxe9lxKEDfF0q5B49ssd+jn2uBdXtU3yLsqCD3jScdgsMnQhDJ0HOOLD23bzdk10/Ve8wxvDW1v08+OZ2tpbVILT9TXXYLfzsvGM5Z+IwfMbgN+D3G4wBvzEhj+B7P+2mmeBrn98E3xO2TvtlG7yN1HqrONRURW1T4LnOW0Wtt4o6bzV13sD7el819b4qvKYx4vGlkkIGKTj9FrJ8hlxfI0Oa6hneeIgcn5dsn49sn59snw+7ZFKbMoRK+zCq7EMptw2l0j6ECttQym1DqLFktcYcjLPjY2v/s+h02bD1WpYN+ZmE/6wibbevs/m9OLyNOHyNpAafHb6m1tfeRuZveo3MJne7db9Oy2L6un9F2GrnkuYUX7cT1EOTAvclhGs+xxwcoRdrKgw+GoZNDiatYOIakNvlXfZ2sujpm+dU7/L7DSf88k0q65titQfE6kastYitFrHWBZ9rEVtdy7OlZX7kYW6M34rxDcTqTcPhs5HhEwb5/AzxNzHC5+Yofy1jfVWM9tcyyO/HEfwf02Ss7CObMpPLXnLZRy5lDGaf5LJPhrBfcmi0BHq2WQQsIlgtggRfN0+zWFpfSwfTLRKommwVwWKJsGyb9UKWjXa7XYmhp+PFj6WpEWtDA9YGD5bQR2Pw2eNBGhsQjwdpcGPxeKDBg3jc4PGAxx183YDxuMHjBrcb4/YEutV3kwEmfL61y+vpYLEz7+q4wNaEOXBwO3y9Gb7+LPC84y1Y/7+tyw4cGkhUzQlr6ETILQBb5HsTwpNFWV0Ziz5cBHBEycLn99Hga8Dj89DgDT77GvB4Pfxm9W/ajZ3l8Xm4/5P7yUjJwCrW4B+BFYtYsIilzbSWZ9ou09mj3XoWKxZa52s32uhZLEJVfRO2zHWkDn4DsVdhmrJoODALb81x/Orbk0P+0YEATdTj9tXg9lVT5w20ZOqCLZzaYAvnUFOg1VPbVI2J0HlAsJBhzyQzZRDOlGycqWNx2p3kYCXbb8j1NpDbWEdufRWD6w/irCnFdmg74mtosx2TMhC/Mx8yCzDB8d8kKx+/Mx8ZlI89Yzj5Fiv5vfTzjCfT2Ijf7Q486t0YT+trf309JmSe39383hNcpx5T725dP+y98XRtfDxEsKSlIenpWNLSWh4yMB3L4NzA+/Q0JC0NiyPw2pIWfJ+W3vZ9cBufz7sMe8XBdrvy5g7poZ9gMPR+04IClr37Cx7Z+Vf2WWCYH24a823On35PxyvUHoD9m4OJazPs2wQHPgdf8FSGxQa5R8OwSXiHHIs7dxz12aOpT0nn6n9ew0F3+w8wMyWTKyZegccbSCzNySU86XQ03+vv/rebeBDk8AkOiZgoO1wPCxaLpU0ijGq98H1a2u/7sEkZS8T1uhJzxC8FwefrXn2JxgHvIJbWz9n4Ldgaj+H8iePajKtW4a6g0R/5tFroqAPN46s134uT48gh25JCdlMD2Z5anLXlWGtK247/Vvs17U6JDxwWUnYh+AgpwYDD2XL9J9EZvx/j8YQkkbZJo01C6SxpBLdh6utD5rnB27W/U0lJaZtEHA4kPZggmhNK6PsuJBRJSenxL4rVS5dS8vNfYGls/ZLiT0nFde89OC+4oMvb6/en+CKd/kq1pjK/cD7Thk2jvqket9dNvbee+qb6wHPo6+b5TXW43ZXUN1RR31RHva+BeuOlsYu/ABaxkGpNxWF1kGoLPltT27x22FqfO50f3MadH9xJuae83b5y03J55MxH8Bt/2wd+/P7gc3Caz/jaLxf28JnABVuf8WEw+PzB5yjWbVk/bL3m7R12fcL238Fz+DF2N9aWbUQY76y3DR8wvMPBO9tMT3GS0lL3p6T9+G/VJdAQ1mHAmhpW82dk8Dk4LTMPbL07rmCbVojb3UGrI+S9x3P4hOIOJBXjbn/9pFMdtUKak0Z4QukoaYQnlPTgura+dzKreulS9j/0MN6yMmzDhzNkwc3dSk6gCYpvvvLNiKPvHk6aLY10Wzrp9vR2z2m2tMB8ezrpRkj31JBee5C0Q/u4/9BmKq3tb8Ic6vWyvArs6dlIejak57R9pGVD+PSU6LpwLdu5jDf++w4ufreJnBooz4RXptuZ9aNf6zWoI2SMaZdMD5tUQ74ARJVMjZ+r3riKUzf7+N67puUzfGG68OFEW2sZg8b6tuO9hReeq9kL4S3ttEGRWz3N7wcM7nLrp9NWSLuE0kHScLeu3xOtEAlJHu1aHWEJpaUVEt7qaPc+DUlN1dPVMRSXa1Aicg7wCGAF/mCMuS9sfirwHHACUA7MNcbsjkUs++r2RfzDXzXRylNnPUWaPZiIbOktrx02R/fvDH9gBG987eTilbQmi9Nh1tAqUgq+E6hsWV8O+z4LPLsr6bCXoc0RTFbZYYksp8300/6ylFHLG7F4A39Ig2tg/vJGXPkfwI9mtm6/5UtJyP7Cp0X9/kjX7xsxCYFrPpYub4MuLf+trTYuWe7FEfzfHPgMDYP8fnjqjEACqg9rJYsVMkcEkk3+yW1aQSZ9KP6UXPx+S/trH6X1mB0l+N1fBN57gkkl2oTSA60QSXNgSUvH7nRGPI0VmiQ6vS7SR1shqnMxa0GJiBXYDpwNlACrgUuNMVtClvkxUGiM+ZGIzAO+bYyZ29l2u9uCuuPOU7nktYqWP3wAjw1enpPNr3+5qsvbO5zqH0+g5D0/Fl/rty6/1eA6w4LziS1tljV+P6a+Dn/VfkzVPvxV+/FX78fUlOOvKcccqsRfV4OprcZfV4tx1wZPaTTg9wl+n2C8wqFSB8bXPqGKxZCWG3KtIniBHUzzi5bnNl8SxUSYFvJSwrbR0bJiWheJuD/Tdlqk5cL201m8baaHPEc65nZxBee3iaFdbJ3E27zdkFjazQuLNTSOXZ8MwuZp/xn6UvwMO3UYfstAjKTjJxW/347xWfE3mZ67FmK3d3waKy0NS5qjNUmEXguJ0OpoXT9dWyGqU/FoQZ0I7DDG7AwGsBi4EAj973whsCj4+hXgMRERE4OseelKP/awv1WHFy5feoiSugWtE1vvpAuZ1sm3elpvbgudXPeBFYuv7WFYfMLe94SDs84J/EPxeDAeD6ahbW+o6KQgKQMRRyqW1BQsKTaMb1/EJY0fyDoK42/9Vm+Cx9J84x4A/sBtfib0ZxDy2piQlUNem5B1WxsGps0y7dYP2X5g2dAWR8j0sG23TAvffpLo6A/S2mjhwDv7QQ60b1UEE0jEVkhoQjnMaSxLWpq2QlRCieVvYx4QeuNRCXBSR8sYY7wiUg3kAG26v4nItcC1ACNHdu8uZfuB6ojTLQ1NNHy+re1XcQn9atwSQ2hEbZeL8No0dvDN1efHMWlSILE4gv9AUh2B53bvHYFTFw5Ha8+ekGliaftN+4upx+Ktbb9L20DhqCXvRI4nSbQkubBHpOmtidIQvCMzLPGFLNuSHCNsw/gj7KvttttNC102wraLr7oUX4QzZ7YBMHbVOm2FqH6lT3xdMsY8DTwNgVN83dmGbfhwvHv3tp8+YgRjX19+ZAFG8MWMmR3uL++3/9Xj+wMYctV3KHviVUzIaUWxGoZcdVFM9pdIRCTiRf6+9q986A87+AyvvgiLQ8vIq/4llkMol0Kbe/JcwWkRlxERG+Ak0Fmixw1ZcDMS9gcuDgdDFtwci931+v4AnNffy/AfX4RtIIDBNhCG//ginNffG7N9qp6ln6FSrWLZScJGoJPETAKJaDXwPWPM5pBlrgcmh3SS+I4xptP6F0dyo25P9ttPxP0ppVRfFJf7oETkPOBhAt3M/2SMuVdE7gbWGGOWiIgDeB44DqgA5jV3quiIFixUSqnkEpf7oIwxy4HlYdPuCnntAb4byxiUUkr1TVrGUymlVELSBKWUUiohaYJSSimVkDRBKaWUSkiaoJRSSiWkPlduQ0QOAHuOcDO5hA2nlISS/RiT/fgg+Y8x2Y8P9BijdZQxZnD4xD6XoHqCiKyJ1Oc+mST7MSb78UHyH2OyHx/oMR4pPcWnlFIqIWmCUkoplZD6a4J6Ot4B9IJkP8ZkPz5I/mNM9uMDPcYj0i+vQSmllEp8/bUFpZRSKsFpglJKKZWQkjpBicg5IrJNRHaIyMII808XkbUi4hWRi+MR45GI4vhuEZEtIrJRRFaIyFHxiPNIRHGMPxKRTSKyXkQ+EJEJ8YjzSBzuGEOWu0hEjIj0qW7LUXyGV4rIgeBnuF5EfhiPOI9ENJ+hiFwS/HvcLCIv9HaMRyKKz/ChkM9vu4hU9ciOjTFJ+SBQg+pLYAyQAmwAJoQtMwooBJ4DLo53zDE4vjOB9ODr64CX4h13DI4xM+T1bOAf8Y67p48xuFwGsBL4CJga77h7+DO8Engs3rHG+BjHA+uAQcH3Q+Idd08eX9jy/0Gg/t8R7zuZW1AnAjuMMTuNMY3AYuDC0AWMMbuNMRsBfzwCPELRHN87xpj64NuPAFcvx3ikojnGmpC3A4C+1uvnsMcYdA9wP+DpzeB6QLTH15dFc4z/DjxujKkEMMbs7+UYj0RXP8NLgRd7YsfJnKDygOKQ9yXBacmiq8d3DfB6TCPqeVEdo4hcLyJfAr8Bbuyl2HrKYY9RRI4H8o0xy3ozsB4S7e/pRcFT0a+ISH7vhNZjojnGAqBARFaJyEcick6vRXfkov5fE7yMMBp4uyd2nMwJSgWJyOXAVOCBeMcSC8aYx40xY4HbgTvjHU9PEhEL8CDwk3jHEkNLgVHGmELgTeDZOMcTCzYCp/mmE2hh/F5EsuIZUIzMA14xxvh6YmPJnKBKgdBvYq7gtGQR1fGJyFnAz4HZxpiGXoqtp3T1M1wMzIllQDFwuGPMACYB74rIbuBkYEkf6ihx2M/QGFMe8rv5B+CEXoqtp0Tze1oCLDHGNBljdgHbCSSsvqArf4fz6KHTe0BSd5KwATsJNDebL+xN7GDZZ+h7nSQOe3zAcQQubo6Pd7wxPMbxIa8vANbEO+6ePsaw5d+lb3WSiOYzHB7y+tvAR/GOOwbHeA7wbPB1LoFTZjnxjr2nji+43DHAboIDQPTEI2lbUMYYL3AD8AawFXjZGLNZRO4WkdkAIjJNREqA7wJPicjm+EXcNdEcH4FTegOB/wt2/1wSp3C7JcpjvCHYbXc9cAtwRXyi7Z4oj7HPivL4bgx+hhsIXEO8Mj7Rdk+Ux/gGUC4iW4B3gFuNMeXxibhruvA7Og9YbILZqifoUEdKKaUSUtK2oJRSSvVtmqCUUkolJE1QSimlEpImKKWUUglJE5RSSqmEpAlKqRgQkVEi8lnw9VQRebSHtjtYRD4WkXUi8m9dXHeKiJzXE3Eo1Rts8Q5AqWRnjFkDrOmhzc0ENhljulOSYgqBIa+WR7uCiNiC98Eo1eu0BaVUGBEZICLLRGSDiHwmInOD06eJyIfB6Z+ISEawpfR+sK7YWhH5RoTtTReRvwdfLxKRP4nIuyKyU0RuDFnuF8GaOx+IyIsi8tOw7UwhMCDuhcEbr9NE5EkRWRO80fU/Q5YNj9UJ3A3MDa47V0SyReS14CCtH4lIYUiMz4vIKuD5nv8JKxUdbUEp1d45wF5jzPkAIuIUkRTgJWCuMWa1iGQCbmA/cLYxxiMi4wmMQ3a4cfKOIVCrKwPYJiJPEmjdXAQUAXZgLfBp6ErGmPUicheBoY5uCMb2c2NMhYhYgRXBJPN5hFjrgfB1fwesM8bMEZEZBOqiTQnubgJwmjHG3eWfnlI9RBOUUu1tAn4rIvcDfzfGvC8ik4EyY8xqaK1DJSIDgMeCrRsfgbIKh7PMBAZHbRCR/cBQ4FTgb8YYD+ARkaVRxnqJiFxL4G95OIHEYjqINXzd0wgkRYwxb4tITjCZQWBgU01OKq40QSkVxhizPViD6TzglyKyAvhrB4svAL4m0PKxEF1BwdBR5X108+9QREYDPwWmGWMqReQZwNGdbUVQ10PbUarb9BqUUmFEZARQb4z5M4EBd48HtgHDRWRacJkMEbEBTgKtFT/wfQLlsbtjFXCBiDhEZCDwrSjWySSQSKpFZChwbnB6R7EeInBasdn7wGXBZaYDB03bCsVKxZW2oJRqbzLwgIj4gSbgOmNMY7CzxO9EJI3A9aezgCeAV0XkB8A/6GbLI3itaAmwkUCLbBNQfZh1NojIOgLXnIoJJDk6ifUdYGFw5PdfA4uAP4nIRgLXqPrUSPAq+elo5kolCBEZaIypFZF0YCVwrTFmbbzjUipetAWlVOJ4WkQmELiO9KwmJ9XfaQtKKaVUQtJOEkoppRKSJiillFIJSROUUkqphKQJSimlVELSBKWUUioh/X9l+G6EM57Z5gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Stats test\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "model_type = \"BR_CustomPrior_Noise_5_1\"\n",
    "# Prepare data\n",
    "users_to_remove = [\"user_lauren\", \"user_lizzie\", \"user_lizzie1\", \"user_sarah1\", \"user_lizzie2\"]\n",
    "all_data_list = [all_datasets[user] for user in all_datasets.keys() if user not in users_to_remove]\n",
    "# print(len(all_data_list))\n",
    "combined_df = pd.concat(all_data_list)\n",
    "grouped_df = combined_df.groupby([\"latency\", \"scale\"])\n",
    "TP_group = grouped_df[\"throughput\"].apply(list)\n",
    "\n",
    "# Group by 'latency'\n",
    "grouped_by_latency = combined_df.groupby('latency')\n",
    "pvals = {}\n",
    "scales = {}\n",
    "# Iterate over each latency group\n",
    "metric = \"weighted_performance\"\n",
    "alternative_hyp = \"greater\"\n",
    "for l, (latency, latency_group) in enumerate(grouped_by_latency):\n",
    "\tprint(f\"Latency: {latency}\")\n",
    "\n",
    "\t# Within each latency group, further group by 'scale'\n",
    "\tgrouped_by_scale = latency_group.groupby('scale')\n",
    "\tbaseline = list(grouped_by_scale.get_group(1.0)[metric])\n",
    "\tprint(baseline)\n",
    "\tpvals[latency] = []\n",
    "\tscales[latency] = []\n",
    "\t# Iterate over each scale group within the current latency group\n",
    "\tfor s, (scale, scale_group) in enumerate(grouped_by_scale):\n",
    "\t\tvalues = list(scale_group[metric])\n",
    "\t\tif len(values) == 5 and scale != 1.0:\n",
    "\t\t\tprint(f\"comparing scale {scale} against baseline\")\n",
    "\t\t\tprint(f\"  Scale: {scale}, Values: {list(scale_group[metric])}\")\n",
    "\t\t\tt_statistic, p_value = ttest_rel(values, baseline, alternative=alternative_hyp)\n",
    "\t\t\tpvals[latency].append(p_value)\n",
    "\t\t\tscales[latency].append(scale)\n",
    "\t\t\tprint(p_value)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for latency, p_values in pvals.items():\n",
    "\tplt.plot(scales[latency], p_values, marker='o', label=f\"{latency} s\")\n",
    "\n",
    "# plt.title(f\"Paired-Sample T Test Comparing {metric}, Alternative Hypothesis: {alternative_hyp}\")\n",
    "plt.axhline(0.05, color=\"black\", linestyle='--')\n",
    "plt.xlabel(\"scaling factor\")\n",
    "plt.ylabel(\"p value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"figures/stat_tests/{metric}_{alternative_hyp}.png\", facecolor='w')\n",
    "plt.show()\n",
    "\n",
    "# print(TP_group)\n",
    "# p_vals = {}\n",
    "# baseline = TP_group[(0.0, 1.0)]\n",
    "# test = TP_group[(0.0, 0.15)]\n",
    "# print(baseline)\n",
    "# print(test)\n",
    "# t_stat, p_val = ttest_rel(baseline, test)\n",
    "# print(p_val)\n",
    "# for params, values in TP_group.items():\n",
    "# \t# print(f\"Latency: {latency}, Scale: {scale}, Values: {len(list(throughput))}\")\n",
    "# \tif len(values) == 6\n",
    "\n",
    "# # Example data: before and after treatment\n",
    "# data_before = np.array([20, 21, 19, 22, 20, 23, 21])\n",
    "# data_after = np.array([22, 22, 20, 23, 21, 24, 22])\n",
    "\n",
    "# # Perform the paired t-test\n",
    "# t_statistic, p_value = ttest_rel(data_before, data_after)\n",
    "\n",
    "\t\tprior_noise_std = 5\n",
    "\t\tmodel = BayesRegression(X_poly.T, y, noise=prior_noise_std)\n",
    "\t\tweight_prior_mean, weight_prior_covar = model.fit()\n",
    "\t\t# print(post_mean.shape, post_covar.shape)\n",
    "\n",
    "\t\tlatency_range = np.arange(0.0, combined_df['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.arange(combined_df['scale'].min(), combined_df['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\tX_dense_poly = poly.transform(X_dense)\n",
    "\n",
    "\t\t# Y_pred, _ = model.predict(X_poly.T)\n",
    "\t\tY_pred_dense, _ = model.predict(X_dense_poly.T)\n",
    "\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t\t\t})\n",
    "\t\t# one_user_data[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t# utils.model_heatmaps(one_user_data, dense_df, X, \n",
    "\t\t# \t\t\t\t\t user_to_remove, metric, \n",
    "\t\t# \t\t\t\t\t \"BayesRegression\", post_mean.flatten())\n",
    "\n",
    "\t\tif metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: \n",
    "\t\t\t\textrema_type = \"max\" # optimal scale at maximum\n",
    "\t\telse:\n",
    "\t\t\textrema_type = \"min\" # optimal scale at minimum\n",
    "\t\t\t\t\n",
    "\t\tfig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\t\ttitle = (f\"Modeling all but {user}\")\n",
    "\t\tfig.suptitle(title)\n",
    "\n",
    "\t\t# Average over all but one user\n",
    "\t\taveraged_data = averaged_df.pivot(\n",
    "\t\t\tindex='latency', columns='scale', values=metric\n",
    "\t\t)\n",
    "\t\tsns.heatmap(averaged_data, cmap='YlGnBu', ax=ax[0], annot=True)\n",
    "\t\t# annotate(ax[1], averaged_data, X_train, color='green')\n",
    "\t\tax[0].set_title('Average')\n",
    "\t\tax[0].set_xlabel('Scale')\n",
    "\t\tax[0].set_ylabel('Latency')\n",
    "\t\tannotate_extrema(averaged_data.values, ax[0], extrema_type)\n",
    "\n",
    "\t\tdense_pred_data = dense_df.pivot(\n",
    "\t\t\tindex='latency', columns='scale', values='Y_pred_dense'\n",
    "\t\t)\n",
    "\t\tsns.heatmap(dense_pred_data, cmap='YlGnBu', ax=ax[1])\n",
    "\t\t# annotate(ax[1], dense_pred_data, X_train, color='green')\n",
    "\t\tax[1].set_title((\"Predicted Data over Dense Input.\" \n",
    "\t\t\t\t   \t\t f\"Likelihood Noise {prior_noise_std}\\n\"\n",
    "\t\t\t\t\t\t f\"Coef: {weight_prior_mean.flatten()}\"))\n",
    "\t\tax[1].set_xlabel('Scale')\n",
    "\t\tax[1].set_ylabel('Latency')\n",
    "\t\tannotate_extrema(dense_pred_data.values, ax[1], extrema_type)\n",
    "\n",
    "\t\t# # Plot residuals\n",
    "\t\t# data[\"residual\"] = np.abs(data[\"performance\"] - data[\"Y_pred\"])\n",
    "\t\t# residual = data.pivot(\n",
    "\t\t# \tindex='latency', columns='scale', values='residual'\n",
    "\t\t# )\n",
    "\t\t# sns.heatmap(residual, cmap='YlGnBu', ax=ax[2], annot=True)\n",
    "\t\t# annotate(ax[2], residual, X_train, color='green')\n",
    "\t\t# ax[2].set_title('Residuals')\n",
    "\t\t# ax[2].set_xlabel('Scale')\n",
    "\t\t# ax[2].set_ylabel('Latency')\n",
    "\t\t# annotate_extrema(residual.values, ax[2], 'min')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tfilepath = f\"figures/allbutone/noise_{prior_noise_std}/{metric}_{user}.png\"\n",
    "\t\tplt.savefig(filepath, facecolor='w')\n",
    "\t\t# plt.show()\n",
    "\n",
    "\t\t# Now perform Bayesian Regression for one user, using prior from other users\n",
    "\t\t# Prepare data \n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tY = data[metric]\n",
    "\n",
    "\t\t# Initialize evaluation metrics\n",
    "\t\toptimal_match_rate = []\n",
    "\t\toptimal_scale_error = []\n",
    "\t\tmse_scores = []\n",
    "\t\tfull_mse_scores = []\n",
    "\t\tn_train_mse = []\n",
    "\t\tn_train_full_mse = []\n",
    "\t\tn_train_p = []\n",
    "\n",
    "\t\tn = len(data)\n",
    "\t\tn_train_values = range(2, n-1)\n",
    "\t\tfor n_train in n_train_values:\n",
    "\n",
    "\t\t\tn_train_p.append(n_train / n)\n",
    "\t\t\t# Split into training/test sets\n",
    "\t\t\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\t\t\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\t\t\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\t\t\tY_train, Y_test = train_set[metric], test_set[metric]\n",
    "\t\t\t\n",
    "\t\t\t# Create dense test input\n",
    "\t\t\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t\t\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\t\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\t\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\t\t\t\n",
    "\t\t\tif model_type.startswith(\"BR\"):\n",
    "\t\t\t\tpoly = PolynomialFeatures(degree=2)\n",
    "\t\t\t\tX_poly = poly.fit_transform(X.values)\n",
    "\t\t\t\tX_train_poly = poly.transform(X_train.values)\n",
    "\t\t\t\tX_test_poly = poly.transform(X_test.values)\n",
    "\t\t\t\tX_dense_poly = poly.transform(X_dense)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Train model\n",
    "\t\t\t\tnoise_std = 1\n",
    "\t\t\t\tmodel = BayesRegression(X_train_poly.T, Y_train, noise=noise_std)\n",
    "\t\t\t\tmodel.set_prior(weight_prior_mean, weight_prior_covar)\n",
    "\t\t\t\tpost_mean, post_covar = model.fit()\n",
    "\t\t\t\tmodel_params = f\"likelihood noise: {noise_std}, coef: {post_mean.flatten()}\"\n",
    "\t\t\t\t# Predict\n",
    "\t\t\t\tY_pred, _ = model.predict(X_poly.T)\n",
    "\t\t\t\tY_pred_dense, _ = model.predict(X_dense_poly.T)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Invalid kernel specification!\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t## Evaluate metrics\n",
    "\t\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "\t\t\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t\t# Mean Square Error on whole dataset\n",
    "\t\t\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\t\t\tif True: #full_mse < 5000:\n",
    "\t\t\t\tn_train_full_mse.append(n_train)\n",
    "\t\t\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t\t\t# Mean Square Error on test set\n",
    "\t\t\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\t\t\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\t\t\tif True: #mse < 5000:\n",
    "\t\t\t\tn_train_mse.append(n_train)\n",
    "\t\t\t\tmse_scores.append(mse)\n",
    "\t\t\t\n",
    "\t\t\tif metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\t\t\telse: # optimal scale at minimum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t\t# Merge the results on 'latency'\n",
    "\t\t\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\t\t\n",
    "\t\t\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t\t\t# print(optimal_scale_dense)\n",
    "\t\t\t# print(merged_ref_dense)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# Count the number of matches\n",
    "\t\t\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\t\t\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\t\t\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\t\t\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t\t\t# Visualize model prediction\n",
    "\t\t\tif n_train == n-2:\n",
    "\t\t\t\tmodel_heatmaps(data, dense_df, X_train, user, metric, model_type, model_params)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Store results from this dataset\n",
    "\t\t\tuser_results[user] = {\n",
    "\t\t\t\t'n_train_mse': list(n_train_mse),\n",
    "\t\t\t\t'n_train_full_mse': list(n_train_full_mse),\n",
    "\t\t\t\t'full_mse_scores': full_mse_scores,\n",
    "\t\t\t\t'mse_scores': mse_scores,\n",
    "\t\t\t\t'n_train_all': list(n_train_values),\n",
    "\t\t\t\t'match_rate': optimal_match_rate,\n",
    "\t\t\t\t'scale_error': optimal_scale_error,\n",
    "\t\t\t\t'n_train_p': n_train_p\n",
    "\t\t\t}\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\t\n",
    "\telse:\n",
    "\t\t# print(f\"saving {metric} to all_results\")\n",
    "\t\tall_results[metric] = user_results\n",
    "\t\tcontinue\n",
    "\tbreak\n",
    "\n",
    "# print(all_results.keys())\n",
    "with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "\tjson.dump(all_results, file)\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "# \tog_results = json.load(file)\n",
    "\n",
    "# og_results[\"total_error\"] = all_results[\"total_error\"]\n",
    "\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(og_results, file)"
    "# print(\"T-statistic:\", t_statistic)\n",
    "# print(\"P-value:\", p_value)\n",
    "\n",
    "# # Interpret the p-value\n",
    "# alpha = 0.05  # commonly used threshold for significance\n",
    "# if p_value < alpha:\n",
    "#     print(\"There is a significant difference.\")\n",
    "# else:\n",
    "#     print(\"There is no significant difference.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
