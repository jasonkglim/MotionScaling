{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from utils import stratified_sample, annotate, even_train_split\n",
    "import glob  # Importing the glob module to find all the files matching a pattern\n",
    "\n",
    "# Pattern to match the data files\n",
    "file_pattern = \"data_files/user_*/metric_df.csv\"\n",
    "\n",
    "# Initialize a dictionary to store Dataframes for each dataset\n",
    "all_datasets = {}\n",
    "\n",
    "# Loop through each file that matches the file pattern\n",
    "for filepath in glob.glob(file_pattern):\n",
    "    # print(filepath)\n",
    "    # print(filepath.split('/'))\n",
    "    # user_name = filepath.split('/')[1]\n",
    "    user_name = filepath.split('\\\\')[1]\n",
    "    print(f\"Processing {filepath} dataset...\")\n",
    "\n",
    "    # Read in data file as a pandas dataframe\n",
    "    data = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    # add weighted performance metric\n",
    "    w = 1\n",
    "    data[\"total_error\"] = data['avg_osd'] + data['avg_target_error']\n",
    "    data[\"weighted_performance\"] = 10*data['throughput'] - w*data[\"total_error\"]\n",
    "\n",
    "    all_datasets[user_name] = data\n",
    "\n",
    "# Combine datasets for Lizzie\n",
    "lizzie1 = all_datasets[\"user_lizzie1\"]\n",
    "lizzie2 = all_datasets[\"user_lizzie2\"]\n",
    "combined_df = pd.concat([lizzie1, lizzie2])\n",
    "all_datasets[\"user_lizzie\"] = combined_df.groupby(['latency', 'scale']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import PolyRegression, GPRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, RationalQuadratic, WhiteKernel\n",
    "import utils\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename='warnings_log.txt', level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "# Function to redirect warnings to logging\n",
    "def warn_to_logging(message, category, filename, lineno, file=None, line=None):\n",
    "    logging.warning(f'{filename}:{lineno}: {category.__name__}: {message}')\n",
    "\n",
    "# Redirect all warnings to the warn_to_logging function\n",
    "warnings.showwarning = warn_to_logging\n",
    "\n",
    "all_results = {}\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"total_error\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t\n",
    "\tprint(output_metric)\n",
    "\tuser_results = {}\n",
    "\tfor user, data in list(all_datasets.items()): \n",
    "\t\t# if user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t# \tcontinue\n",
    "\t\tprint(f\"\\t{user}\")\n",
    "\n",
    "\t\t# Prepare data \n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tY = data[output_metric]\n",
    "\n",
    "\t\t# Initialize evaluation metrics\n",
    "\t\toptimal_match_rate = []\n",
    "\t\toptimal_scale_error = []\n",
    "\t\tmse_scores = []\n",
    "\t\tfull_mse_scores = []\n",
    "\t\tn_train_mse = []\n",
    "\t\tn_train_full_mse = []\n",
    "\t\tn_train_p = []\n",
    "\n",
    "\t\tn = len(data)\n",
    "\t\tn_train_values = range(2, n-1)\n",
    "\t\tfor n_train in n_train_values:\n",
    "\n",
    "\t\t\tn_train_p.append(n_train / n)\n",
    "\t\t\t# Split into training/test sets\n",
    "\t\t\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\t\t\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\t\t\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\t\t\tY_train, Y_test = train_set[output_metric], test_set[output_metric]\n",
    "\t\t\t\n",
    "\t\t\t# Create dense test input\n",
    "\t\t\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t\t\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\t\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\t\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\t\t\n",
    "\t\t\t# # Polynomial Regression\n",
    "\t\t\tif model_type.startswith(\"Poly\"):\n",
    "\t\t\t\tdegree = int(model_type.strip(\"Poly\"))\n",
    "\t\t\t\tY_pred, model_params = PolyRegression(X_train.values, Y_train.values, X.values, degree)\n",
    "\t\t\t\tY_pred_dense, _ = PolyRegression(X_train.values, Y_train.values, X_dense, degree)\n",
    "\n",
    "\t\t\t# Gaussian Process Regression\n",
    "\t\t\telif model_type.startswith(\"GPR\"):\n",
    "\t\t\t\t# Choose kernel\n",
    "\t\t\t\tkernel_type = model_type.removeprefix(\"GPR_\")\n",
    "\t\t\t\t# print(kernel_type)\n",
    "\t\t\t\tif kernel_type == \"RBF_Noise_default\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF() + WhiteKernel() # Default RBF with likelihood noise\n",
    "\t\t\t\telif kernel_type == \"RBF_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0])\n",
    "\t\t\t\telif kernel_type == \"RBF_Noise_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0]) + WhiteKernel() # RBF with anistropic length scale\n",
    "\t\t\t\telif kernel_type == \"RQ_Noise_default\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RationalQuadratic() + WhiteKernel() # Default Rational Quadratic with likelihood noise\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(\"Invalid kernel specification!\")\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\tY_pred, Y_pred_std, model_params = GPRegression(X_train.values, Y_train.values, X.values, kernel)\n",
    "\t\t\t\tY_pred_dense, Y_pred_std, _ = GPRegression(X_train.values, Y_train.values, X_dense, kernel)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Invalid model type specification!\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t## Evaluate metrics\n",
    "\t\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "\t\t\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t\t# Mean Square Error on whole dataset\n",
    "\t\t\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\t\t\tif True: #full_mse < 5000:\n",
    "\t\t\t\tn_train_full_mse.append(n_train)\n",
    "\t\t\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t\t\t# Mean Square Error on test set\n",
    "\t\t\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\t\t\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\t\t\tif True: #mse < 5000:\n",
    "\t\t\t\tn_train_mse.append(n_train)\n",
    "\t\t\t\tmse_scores.append(mse)\n",
    "\t\t\t\n",
    "\t\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\t\t\telse: # optimal scale at minimum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t\t# Merge the results on 'latency'\n",
    "\t\t\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\t\t\n",
    "\t\t\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t\t\t# print(optimal_scale_dense)\n",
    "\t\t\t# print(merged_ref_dense)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# Count the number of matches\n",
    "\t\t\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\t\t\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\t\t\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\t\t\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t\t\t# Visualize model prediction\n",
    "\t\t\tif n_train == n-2:\n",
    "\t\t\t\tutils.model_heatmaps(data, dense_df, X_train, user, output_metric, model_type, model_params)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Store results from this dataset\n",
    "\t\t\tuser_results[user] = {\n",
    "\t\t\t\t'n_train_mse': list(n_train_mse),\n",
    "\t\t\t\t'n_train_full_mse': list(n_train_full_mse),\n",
    "\t\t\t\t'full_mse_scores': full_mse_scores,\n",
    "\t\t\t\t'mse_scores': mse_scores,\n",
    "\t\t\t\t'n_train_all': list(n_train_values),\n",
    "\t\t\t\t'match_rate': optimal_match_rate,\n",
    "\t\t\t\t'scale_error': optimal_scale_error,\n",
    "\t\t\t\t'n_train_p': n_train_p\n",
    "\t\t\t}\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\t\n",
    "\telse:\n",
    "\t\tall_results[output_metric] = user_results\n",
    "\t\tcontinue\n",
    "\tbreak\n",
    "\n",
    "# print(all_results.keys())\n",
    "with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "\tjson.dump(all_results, file)\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "# \tog_results = json.load(file)\n",
    "\n",
    "# og_results[\"total_error\"] = all_results[\"total_error\"]\n",
    "\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(og_results, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the results for all datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "\tall_results = json.load(file)\n",
    "\n",
    "for output_metric, user_results in all_results.items():\n",
    "\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\tfig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}\")\n",
    "\tfor user, results in user_results.items():\n",
    "\t\taxes[0, 0].plot(results['n_train_p'], results['match_rate'], marker='o', label=user)\n",
    "\t\taxes[0, 1].plot(results['n_train_p'], results['scale_error'], marker='o', label=user)\n",
    "\t\taxes[1, 0].plot(results['n_train_p'], results['full_mse_scores'], marker='o', label=user)\n",
    "\t\taxes[1, 1].plot(results['n_train_p'], results['mse_scores'], marker='o', label=user)\n",
    "\n",
    "\taxes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\taxes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\taxes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\taxes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\taxes[1, 0].set_title('MSE on whole dataset')\n",
    "\taxes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\taxes[1, 1].set_title('MSE on test set')\n",
    "\taxes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\taxes[1, 0].legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/model_results/{output_metric}/{model_type}_model_eval_metrics_{output_metric}.png\", facecolor='w')\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot averaged results\n",
    "\n",
    "# Load data\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "\tall_results = json.load(file)\n",
    "\n",
    "print(all_results)\n",
    "for output_metric, user_results in all_results.items():\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\tfig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}, mean over users\")\n",
    "\n",
    "\t# Prepare lists of lists to store data\n",
    "\tmatch_rate_lists = []\n",
    "\tscale_error_lists = []\n",
    "\tfull_mse_score_lists = []\n",
    "\tmse_score_lists = []\n",
    "\tn_train_lists = []\n",
    "\n",
    "\t# Collect data for each metric\n",
    "\tfor user, results in user_results.items():\n",
    "\t\tmatch_rate_lists.append(results['match_rate'])\n",
    "\t\tscale_error_lists.append(results['scale_error'])\n",
    "\t\tfull_mse_score_lists.append(results['full_mse_scores'])\n",
    "\t\tmse_score_lists.append(results['mse_scores'])\n",
    "\t\tn_train_lists.append(results['n_train_all'])\n",
    "\n",
    "\t# Function to calculate average and standard deviation safely\n",
    "\tdef safe_mean_std(data_lists, index):\n",
    "\t\tvalid_data = [data[index] for data in data_lists if index < len(data)]\n",
    "\t\treturn np.mean(valid_data), np.std(valid_data)\n",
    "\n",
    "\t# Calculate averages and standard deviations safely\n",
    "\tavg_match_rate = []\n",
    "\tstd_match_rate = []\n",
    "\tavg_scale_error = []\n",
    "\tstd_scale_error = []\n",
    "\tavg_full_mse_scores = []\n",
    "\tstd_full_mse_scores = []\n",
    "\tavg_mse_scores = []\n",
    "\tstd_mse_scores = []\n",
    "\n",
    "\tmin_n_train_length = min([len(data_list) for data_list in n_train_lists])\n",
    "\tmin_n_train_list = n_train_lists[0][:min_n_train_length]\n",
    "\tfor n in range(min_n_train_length):\n",
    "\t\tmean, std = safe_mean_std(match_rate_lists, n)\n",
    "\t\tavg_match_rate.append(mean)\n",
    "\t\tstd_match_rate.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(scale_error_lists, n)\n",
    "\t\tavg_scale_error.append(mean)\n",
    "\t\tstd_scale_error.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(full_mse_score_lists, n)\n",
    "\t\tavg_full_mse_scores.append(mean)\n",
    "\t\tstd_full_mse_scores.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(mse_score_lists, n)\n",
    "\t\tavg_mse_scores.append(mean)\n",
    "\t\tstd_mse_scores.append(std)\n",
    "\n",
    "\t# Plotting the average values and standard deviation\n",
    "\t# Plotting the average values and standard deviation\n",
    "\taxes[0, 0].plot(min_n_train_list, avg_match_rate, marker='o', label='Mean over users')\n",
    "\taxes[0, 0].fill_between(min_n_train_list, np.subtract(avg_match_rate, std_match_rate), \n",
    "\t\t\t\t\t\t\tnp.add(avg_match_rate, std_match_rate), alpha=0.2)\n",
    "\n",
    "\taxes[0, 1].plot(min_n_train_list, avg_scale_error, marker='o', label='Mean over users')\n",
    "\taxes[0, 1].fill_between(min_n_train_list, np.subtract(avg_scale_error, std_scale_error), \n",
    "\t\t\t\t\t\t\tnp.add(avg_scale_error, std_scale_error), alpha=0.2)\n",
    "\n",
    "\taxes[1, 0].plot(min_n_train_list, avg_full_mse_scores, marker='o', label='Mean over users')\n",
    "\taxes[1, 0].fill_between(min_n_train_list, np.subtract(avg_full_mse_scores, std_full_mse_scores), \n",
    "\t\t\t\t\t\t\tnp.add(avg_full_mse_scores, std_full_mse_scores), alpha=0.2)\n",
    "\n",
    "\taxes[1, 1].plot(min_n_train_list, avg_mse_scores, marker='o', label='Mean Over Users')\n",
    "\taxes[1, 1].fill_between(min_n_train_list, np.subtract(avg_mse_scores, std_mse_scores), \n",
    "\t\t\t\t\t\t\tnp.add(avg_mse_scores, std_mse_scores), alpha=0.2)\n",
    "\t\n",
    "\taxes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\taxes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\taxes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\taxes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\taxes[1, 0].set_title('MSE on whole dataset')\n",
    "\taxes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\taxes[1, 1].set_title('MSE on test set')\n",
    "\taxes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\t# axes[1, 0].legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/model_results/{output_metric}/{model_type}_model_eval_metrics_{output_metric}_average.png\", facecolor='w')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale\n",
    "\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tfor user, data in list(all_datasets.items()):\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\t# model_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\t# kernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\t# Y_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\t# dense_df = pd.DataFrame({\n",
    "\t\t# \t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t# \t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t# \t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t# \t\t})\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t# optimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\t# optimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\t# plt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\")\n",
    "\t\tplt.ylabel(\"scaling factor\")\n",
    "\t\tplt.plot(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='x', label=user)\n",
    "\tplt.legend()\n",
    "\tplt.savefig(f\"figures/optimal_scale_per_latency/{output_metric}.png\", facecolor='w')\n",
    "\tplt.show()\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale per latency with model prediction\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tfor user, data in list(all_datasets.items())[:1]:\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\tmodel_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\tkernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\tY_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\tplt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\")\n",
    "\t\tplt.ylabel(\"scaling factor\")\n",
    "\t\tplt.scatter(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='x', label=\"measured\")\n",
    "\t\tplt.plot(optimal_scale_dense['latency'], optimal_scale_dense['scale'], label=\"predicted\")\n",
    "\tplt.legend()\n",
    "\t# plt.savefig(f\"figures/optimal_scale_per_latency/{output_metric}.png\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot key metric heatmaps for average over users\n",
    "from utils import annotate_extrema\n",
    "\n",
    "delete_keys = [\"user_lizzie\", \"user_lizzie1\", \"user_lauren\", \"user_sarah1\"]\n",
    "sub_datasets = [all_datasets[key] for key in all_datasets.keys() if key not in delete_keys]\n",
    "\n",
    "combined_df = pd.concat(sub_datasets)\n",
    "averaged_df = combined_df.groupby([\"latency\", \"scale\"]).mean().reset_index()\n",
    "\n",
    "# averaged_df = all_datasets[\"user_lizzie\"]\n",
    "# Create a 2x5 subplot for the heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "# Plot the heatmap for throughput\n",
    "heatmap_throughput = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='throughput')\n",
    "ax = sns.heatmap(heatmap_throughput, ax=axes[0], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[0].set_title('Throughput vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_throughput.values, ax)\n",
    "\n",
    "# Plot heatmap for total error (target deviation + osd)\n",
    "averaged_df['total_error'] = averaged_df['avg_osd'] + averaged_df['avg_target_error']\n",
    "heatmap_error = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='total_error')\n",
    "ax = sns.heatmap(heatmap_error, ax=axes[1], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[1].set_title('Total Error vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_error.values, ax, extrema_type='min')\n",
    "\n",
    "# Plot heatmap for combined performance (movement speed - total error)\n",
    "heatmap_combo = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='weighted_performance')\n",
    "ax = sns.heatmap(heatmap_combo, ax=axes[2], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[2].set_title('Combined Performance vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_combo.values, ax, extrema_type='max')\n",
    "\n",
    "# plt.title(\"User A\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{data_folder}/heatmap_key_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function, e.g., a sine wave function\n",
    "def smooth_2d_function(x, y):\n",
    "    return np.sin(np.sqrt(x**2 + y**2))\n",
    "\n",
    "# Generate sample points\n",
    "x = np.linspace(-5, 5, 10)\n",
    "y = np.linspace(-5, 5, 10)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Apply the function to the sample points\n",
    "z = smooth_2d_function(x, y)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'x': x.ravel(), 'y': y.ravel(), 'z': z.ravel()})\n",
    "\n",
    "X = df[['x', 'y']]\n",
    "Y = df['z']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
    "\n",
    "z_pred, _ = GPRegression(X_train, Y_train, X)\n",
    "z_pred = z_pred.reshape(x.shape)\n",
    "\n",
    "# df[\"y_pred\"] = Y_pred\n",
    "\n",
    "# Plotting the function for visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "ax = axes[0].contourf(x, y, z, cmap='viridis')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Smooth 2D Function')\n",
    "\n",
    "ax = axes[1].contourf(x, y, z_pred, cmap='viridis')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Predictions')\n",
    "\n",
    "fig.colorbar(ax, label='Function Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataframes\n",
    "df1 = pd.DataFrame({'latency': [1, 2], 'scale': [3, 4], 'col1': [5, 6], 'col2': [7, 8]})\n",
    "df2 = pd.DataFrame({'latency': [1, 2], 'scale': [3, 4], 'col1': [9, 10], 'col2': [11, 12]})\n",
    "\n",
    "# Concatenating the dataframes\n",
    "combined_df = pd.concat([df1, df2])\n",
    "\n",
    "# Grouping by 'latency' and 'scale' and calculating the average of other columns\n",
    "grouped_df = combined_df.groupby(['latency', 'scale']).mean().reset_index()\n",
    "\n",
    "print(grouped_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_samples_with_noise(x_min, x_max, num_x, num_samples, noise_std):\n",
    "    \"\"\"\n",
    "    Generates samples from a sine function with added Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    x_min (float): Minimum x value.\n",
    "    x_max (float): Maximum x value.\n",
    "    num_x (int): Number of distinct x values in the range.\n",
    "    num_samples (int): Number of samples to generate for each x value.\n",
    "    noise_std (float): Standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Generate x values\n",
    "    x_values = np.linspace(x_min, x_max, num_x)\n",
    "\n",
    "    # Plot the underlying sine function\n",
    "    plt.plot(x_values, np.sin(x_values), label='Underlying sine function', color='blue')\n",
    "\n",
    "    # Generate and plot samples with noise for each x value\n",
    "    all_samples = []\n",
    "    for x in x_values:\n",
    "        noisy_samples = np.sin(x) + np.random.normal(0, noise_std, num_samples)\n",
    "        plt.scatter([x]*num_samples, noisy_samples, color='red', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Samples from a Sine Function with Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "generate_samples_with_noise(x_min=0, x_max=2*np.pi, num_x=30, num_samples=10, noise_std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100, 3)\n",
      "[ 1.         -4.8989899  24.00010203]\n",
      "(100,)\n",
      "[[0.92823481]\n",
      " [2.98595125]\n",
      " [2.00407399]]\n",
      "[[ 8.25523276e-02  2.61783744e-18 -5.39356949e-03]\n",
      " [ 2.61783744e-18  4.68291760e-03 -3.39805827e-19]\n",
      " [-5.39356949e-03 -3.39805827e-19  6.59788413e-04]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD7CAYAAABzGc+QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2+klEQVR4nO3dfXzNdf/A8dfn7MbI5j7mZhuFTKHyI5tLyXSFSVJSrmLq0tVVCt2iqU0IFaG6qIyiK6mE0c0qrkJdIXRlE2IbGkax1Wx28/n9cW6cc3bO7s/Ozd7Px8NjOzffcz7Hznmfz/f9+XzeH6W1RgghhPcxuLsBQgghqkYCuBBCeCkJ4EII4aUkgAshhJeSAC6EEF5KArgQQnipcgO4UqqzUmqP1b8cpdREpVRTpVSKUuqg6WeT2miwEEIII1WZeeBKKT/gONAbeAj4TWv9glLqaaCJ1vop1zRTCCGEvcoG8JuAZ7XW0Uqpn4EbtNZZSqlQYIvWunNZxzdv3lxHRERUq8FCCFHX7Nq167TWuoX99f6VfJxRwL9Nv7fUWmeZfj8BtHR0gFJqPDAeICwsjJ07d1byKYUQom5TSmU4ur7Cg5hKqUDgFmCN/W3a2I132JXXWi/VWvfUWvds0aLUF4gQQogqqswslEHAD1rrk6bLJ02pE0w/T9V044QQQjhXmQB+FxfTJwDrgTGm38cA62qqUUIIIcpXoQCulLoEGAh8ZHX1C8BApdRBIMZ0WQghRC2p0CCm1vpPoJnddWeAAa5olBBC+AKtNUopp5erS1ZiCiGEC8xPOUBicirmqdpaaxKTU5mfcqDGnkMCuBBC1DCtNTn5hSRtS7cE8cTkVJK2pZOTX0hNbaRT2XngQgghyqGUYnpsJABJ29JJ2pYOQFx0BNNjI2ssjSI9cCGEcAHrIG5Wk8EbJIALIUS12adEtNaWtIk165x4TZAUihBCVMP8lAPk5BdaetdaaxI27GPP0XPsOXqWqwv20ifUnz873WxJpdRUT1wCuBBCVJH1YCUYA3NicirLt2fQo10j4qIiuKVdaz766CMSh12Fwc+PkKCAGkujVKoaYXX17NlTSzErIYQvsZ5hYmYerDQzB+yqzgNXSu3SWve0v15y4EIIUQ2OBivjh3RBa81TTz3FI0s2WeZ+1+QAJkgKRQghqsXRYGXsoq3EtT7JvHnzaH6LgYfvj6jxVZggPXAhhKgy6/RJXHQEh2cNIjI0mH2/5vC3CVPwb9Kaf8bdXePTB80kgAshRBUppQgJCrDkvA0GA8kT+pJ/5AcunPyFkN6389ywq1wSvEECuBBCVMukgZ1sphDO2JjGuW/fxy+4OQ2v7F/jc7+tSQ5cCCGqyRy8E5NTWfb1Qbr36M642H6caN2xxud+W5MALoQQNcCcThnXryPT5660BHWgRud+W5MALoQQNWRg6AVOnz5tuWyeYig5cCGE8HDTpk3jtttuIy8vz3Kdq4I3SAAXQoga8eOPP7J+/XomTpzIJZdcUivPKQFcCCFqwMyZMwkODmbChAm19pwV3dS4sVLqA6XUfqVUmlKqj1KqqVIqRSl10PSziasbK4QQnig1NZU1a9YwYcIEmjSpvVBY0R74K8CnWusrgO5AGvA08KXWuiPwpemyEELUOYcPH6Z9+/ZMnjy5Vp+33GqESqlGwB6gg7a6s1LqZ+AGrXWWUioU2KK17lzWY0k1QiGEr7CvbVJcXIyfn59Lnqs61QjbA9lAklJqt1LqTaXUJUBLrXWW6T4ngJZOnni8UmqnUmpndnZ2VdsvhBAew3rH+W3btlFYWMjzm/bX6I7zFVGRAO4PXAO8rrW+GvgTu3SJqWfusCuvtV6qte6pte7ZokWL6rZXCCHcxrxVmnkTh0eXfkq/fv34a9xjNb7jfEVUZCHPMeCY1vq/pssfYAzgJ5VSoVYplFOuaqQQQrib9dZp02MjQcO8aRPRBn8ONu7F/TW843xFlNsD11qfAI4qpcz57QFAKrAeGGO6bgywziUtFEIIN7PudZtrf5/JyuDPfV/RsMcg/Bo2qfXgDRVfSj8BWKWUCgQOA3EYg//7Sqn7gAxgpGuaKIQQ7mW9607StnSStqVzeuN8lJ8/jXrfDhh3nK/tIF6hAK613gOUGgHF2BsXQgifZw7iSdvS0UWFXDhxiN6DR7F90WibPTFrM4hLMSshhKgA663TlH8AoXEL6d+zFYCld+6qqoPOSAAXQohyWG+ddkdkQ+JvvZqXt2SyfHsG9eqnWgY2PTUHLoQQdZb11mm/rJnDlc9s4dChQ5braztwm0kAF0KIMphXXE4a2Im0tDSuXLWKyZMnExgY6JZetzWpRiiEEE5Yr7gESEhIwC+wHiG9RgCurfVdEdIDF0IIO+aAbZ77jYZbw4tYvXo1IX1GUhIUXKoWijtIABdCCCuOVlwmbU9n/rx/owIb8I+HH3V76sRMArgQQphYr7gE4/RAbSrz1Dj6LhpeNZAX7urjEcEbJIALIYSFoxWXAMV55/Br0Aj/kOZuWXHpjAxiCiGEFesgDlBwPI2s18ey9EbjNEJzPZTarDrojPTAhRDCivWKS601v3/9NkENQ4iKimKgabNid879tiY9cCGEMLFecRkXHcGbMfUoyPwfgT1v56XNGYAxLz5pYCc3t9RIeuBCCGFiveIyfkgX+vQZR1hYGPc++IDH9LqtSQAXQggrkwZ2QmvN3r172bVrF0uXLiVueA+PC94gAVwIIUpRStGjRw/2799PRESERwZvkBy4EKIOs55JYt7vEuDs2bNorbn88svx9/fcfq7ntkwIIVzIesXlgi8OknO+EI3mEj949aFY2vaMYcQDT3rMgKUjEsCFEHWO9YpLrTUKRdL2dAC6nNlKRkYG53uHW3aZ99QUigRwIUSdY7/i0qykII/P332demHdeHD0cI9ZcelMhXLgSql0pdT/lFJ7lFI7Tdc1VUqlKKUOmn42cW1ThRCi5tivuATI2bmOkrxzNLl+DM8O7erRwRsqN4jZX2vdQ2tt3tz4aeBLrXVH4EvTZSGE8ArWKy4BdHERf+z9nPqd+lCvdWePWS5fluqkUIYBN5h+XwFsAZ6qZnuEEMLlrFdcjo0Kt+TAQ+MWckf35jRqHuqWXeYrq6IBXAOfK6U0sERrvRRoqbXOMt1+Amjp6ECl1HhgPEBYWFg1myuEENVnveJyemwkL6zbzZjrwlAGRaP6gUyM6QjKc2qeOKMqcoqglGqjtT6ulLoUSAEmAOu11o2t7vO71rrMPHjPnj31zp07q9lkIYSoGeYZJqNGjeL48eNs2bIFPz8/m9s8gVJql1X62qJCOXCt9XHTz1PAWqAXcFIpFWp68FDgVM01VwghXE8pxY4dO1i9ejU33HCDJXibb/N05QZwpdQlSqlg8+/ATcBPwHpgjOluY4B1rmqkEEK4gtaap556iubNm/PEE0+4uzmVVpEceEtgrenbyB94V2v9qVJqB/C+Uuo+IAMY6bpmCiFEzdu0aRObN29m4cKFhISEuLs5lVZuANdaHwa6O7j+DDDAFY0SQoiaZJ/PNl9esmQJHTt25IEHHnBj66pOVmIKIXyadc0TpZRlCmFIUAAffPABmZmZBAYGuruZVSLVCIUQPsu65ol5YU5icipvbdnP6bPnCAgI4PLLL3d3M6tMeuBCCJ/lbJf5Dr9+wYpJG3g85n80btzYK2acOCI9cCGET7OveVKUe5od698mOjqaxo0bk5icyvyUA25sYdVJABdC+DT7midnv36H8wWFzJo1y7Kc3lw21ttICkUI4bPsd5kf3CqPXnO+JKT3CAa8sR/AspzeG9Mo0gMXQvgsc82Tlwp/YvrDg1ndqxctDAbuahxquY+3Bm+QAC6E8HGTTu3gtn8lojIzmQfsLCnh5S/f4JZ9mwG8omysMxLAhRC+bdo0LuTlcRxQQBjQoKiAV35cQ1x0hM0UQ28jOXAhhG/LzOQVIAHYB0SYrlZHj1pmp3h62VhnJIALIXzaiTZtmHHsGDFcDN4AhIVZphh6Y/AGSaEIIXzc1A4dKABetL6yQQOYORPwjrKxzkgAF0L4rJ07d7L8m2+YOGQIHcPDQSkID4elS2H0aHc3r9okhSKE8FlbtmyhVatWPPPuu+CF5WLLIz1wIYTXsp85Yn/58ccfJy0tzStrfVeEBHAhhFcxB+n5KQdITE6lpKTEcr25rklubi7m/XcbNWrktra6mgRwIYTXsA7a5jKxsYu28vLnP9vUNZkxYwa9e/fmyJEjlmO9cZ53eSQHLoTwCta1vQHih3Thv4fPkJqVS2pWLgBjo8K5s6MfV962gG79byE8PNxyrHkTh0kDO7nrJdQ4CeBCCK/grLa3DQ0TJ07EP7Aep7uMIHbRVmK6tCS3oMhS0Mp+ezVvJgFcCOE1zEHcYfAGXnt7NdmffcbLL79MiiHMpnfuzVUHnalwDlwp5aeU2q2USjZdbq+U+q9S6pBSarVSyiWbypU3yiyEqDvsa3sDRIYGc3jWIMZGhVP851kCQzuz4NcOlsBt5mvBGyo3iPkokGZ1eQ4wX2t9OfA7cF9NNgwuDliYg7b1KLMQom6xr+396IDLiQwNJjUrlxkb05geG8mEfz5Aq3vmofxKJxe8tWBVWSoUwJVSbYEhwJumywq4EfjAdJcVwK012TBnm5F68+4ZQoiqMeetQ4ICiIuOIH5IFyYN7EzyhL7ERUeQf/oYH3zwAWhQ6mJYM/fOvb3qoDMVzYEvAJ4Egk2XmwFntdZFpsvHgDaODlRKjQfGA4SFhVW4Yc4GLHwxjyWEcG5+ygFy8guZHhvJpIGdKCkpYcbGNMuMkvghXRg0aDLzt26nxf1LuT+mGyFB/qSknrT0zuOHdAG8t+qgM+X2wJVSscAprfWuqjyB1nqp1rqn1rpnixYtKnWs/Wak4Jt5LCGEY47OxGdsTLM5E//www/5/PPPGTx2IvfHdDMF+ou985CgAAwGg+ULwJeo8k4nlFKzgXuAIiAICAHWAn8FWmmti5RSfYDntNZ/Leuxevbsqc2royrCOm1iJj1wIeqWsuJAbm4uXbp0oWXLlnz//ff4+fnZxAZfmTKolNqlte5pf325PXCt9RStdVutdQQwCvhKaz0a2AzcbrrbGGBdDba31IDFkdmDfTaPJYRwrqwz8fj4eLKysvjXv/6Fv79/qWDtC8G7LNWZB/4U8J5S6nlgN/BWzTTJyHrAwvzH8vbdM4QQledo6mBicirTYyOJjo6mcePG9OrVy02tc69yUyg1qbIpFCh9CuQrp0RCiPLZn4lPj40sdbkuxIMqp1Dcra6dEgkhLnJ2Jt7tz53sXL/CUomwrpKl9EIIjzZpYCebM+/jx4+zZcVL9OnTB4PB4/ugLuX5r37VKoiIAIPB+HPVqlJ3keX2Qvg26zPvCRMmUFRUxOuvv17nz8g9uwe+ahWMHw95ecbLGRnGywCjR6O1ZsEXBy2T/JVSPls2UggBa9eu5eOPP2bOnDl06NDB3c1xO8/ugU+bBnl5XMC4Vl+DMZhPm4bWmoQN+/jPgWxZbi9EHXDhwgUeeeQRunfvzqRJk9zdHI/g2T3wzEwA1gBjgRIgDtCZmSRs2Mfy7RmMjQrn6naNZbm9ED4uMDCQ999/n6CgIAICAtzdHI/g2QE8LAwyMrgLWApMBgYBRcHNLcH72aFd0VqTtD3dcpgEbyF8S15eHg0aNKBPnz7ubopH8ewUysyZ0KABBuAN4DzwT2Vgbr97AVAYc96xi7baHCYrNYXwHefPn+faa69l5syZ7m6Kx/HsHvjo0caf06bRMTOTJxs0ZMafubQIqEcDIGl7uqXnHRkaTPKEvpZCNyA9cSG8jaOFe4mJiezfv5/evXu7sWWeybMDOBhnm9x9N4nJqaz4+hDddr7Jsmm3seF4PZZvz7DcLXlCXwwGQ6mykbJyUwjvYF021vzZffCVD3lj7jzGjRtHTEyMu5vocTw/gHNxNda4fpczfe56AJKP29ZGMNYH9icnv4j4IV0wGAwypVAIL2BOd1p2nNcwfWgkz368l2Wzn6RBoybMmzfPvY30UF4RwOHiaiyAaWt2smjmM9wybBgrEx60TB00b68ElKqZID1xITyPda97emwk6Iup0YLj+9Fns3hn9b9p2rSpu5vqkbwmgMPF1VhNLqlP/bOH+c+yWeQ8NtpSpTC4nj+9OzSTKYVCeAHrzRrA2OnSXJx8UK/NFWSmHyY0NNRNLfR8Hl+N0Jnvv/+ePn36EBcXx5tvvmnpYWutaT9lk+V+R2YPluAthIdytFmDLi7i/JFd1L+sF+P6tpcOGF5cjdCZXr168cQTT/DWW2/xySef2CyjtyZTCoXwXI42a7gs60uyP5xBTNPfZQOXcnhtAAd47rnn6Nq1K4899hjFxcWyg48QXsa+03Xh5GH+897r3HnnnbzxxGjLnpbmDpqw5VU5cHtBQUGsXr2a4OBg/Pz8ZAcfIbyI/WYNTw28jPArJqGCgsm5+h5mJKcRHyszysri1QEcoGvXroDxzTC8YyDh4R0twdocxCV4C+F57DdrmDZtGqcyDnJX/KvkNWxsXKSnZEZZWbx2ENPe5MmTWblyJT/99BOXXnqpS55DCFHzzAH5o48+4rvvvmPOnDkATneir4vB29kgZrkBXCkVBHwN1MPYY/9Aa/2sUqo98B7QDNgF3KO1vlDWY7kygP/0009ce+21DBo0iLVr19bJP7IQvkRmlF1UnVkoBcCNWuvuQA/gZqXUdcAcYL7W+nLgd+C+GmxvpV155ZXMnj2bdevWkZSU5M6mCCEqYeLEibz44os218mMsoopN4Broz9MFwNM/zRwI/CB6foVwK2uaGBlTJw4kf79+/Poo49y+PBhdzdHCFGOTZs28corr3DixAnLdfaDmzKjzLkKTSNUSvkppfYAp4AU4BfgrNa6yHSXY0AbJ8eOV0rtVErtzM7OroEmO2cwGFi+fDlhYWGcPHnS5jbZN1MIz5Kdnc24ceO46qqrbErFOtuJ3npKoTCq1CCmUqoxsBaIB5ab0icopdoBn2itryzreFfmwK2VlJTY7FbtqMqZTEkSwn201tx2221s2rSJHTt20K1bN4f3sS8tW1eDd42sxNRanwU2A32Axkop8zTEtsDx6jayphgMBgoLC4mPj+e7776z1FuQfTOFcB/rz9quXbtYv349s2bNchi8gVLBuq4G77KUOw9cKdUCKNRan1VK1QcGYhzA3AzcjnEmyhhgnSsbWll5eXm88847vPvuu+zevRtAilwJ4SaWs+CcPahp07g2M5PNjZqRc6rE3U3zahXpgYcCm5VSPwI7gBStdTLwFDBZKXUI41TCt1zXzMpr1KgR77zzDunp6Tz66KOl6i1I8BaidpirDp5ZmsQf4+5ja0YGSmv6nT3NTa9MR69aJWfCVeQzC3mciY+P5/nnn2fEY3PZ6X8xiEsPXIjao7XmXMs2PJ+dxcvAPqCL+bawMBIXb5IxqTL4XDXCioqPj6fdFT1Y+9rzjL72UpmSJIQbKKXYnp3FS8A/uBi8ATh6VMakqsjra6GURWtNYGAgY6a9zKnsbGaMuFaKXAnhBsePH+deZaCbLuEl+9uCm8sZcRX5bAC3njo442/9KSkpYcbGNP7MOsLc8bHyZhGiFmitKS4upu+g2zhnMPCVwZ/6hRcrbuT512Nuv3t5RT6PVeKTKRTrrZrMaZIZG9NY/MYK5j0wlE2bNsmbRQgXm59ygMTkVAwGA1f3+yuxDydwcEIiOZe2RivFsZAWPH3zw6zv2l/SmVXkkz1w6zSJ9dTBB+69kw0Zn3Hvvfeye/du2rVr58ZWCuG7zJ2oZd/8AsCHixJI2LCPx7ZnEDn136Rm5RIXFcErQyNpZlV1UM6MK8enZ6E4qmZ28OBBrr32Wrp168aWLVsICAiw3FfeOELUnKysLK76v2j8+oyh/mXGCRTG5fD+5OQXycroSqhzs1CcVTPr2LEjwycksn37dqZOnWpz3/kpB9zRVCF8TnFxMaNHjybvtxP4hTS3XD89NpJJAzvb9LTNZ8wSvCvPJwN4WdXMEjbs47LrbiLk/4ZzuKS5LK0XwgUSEhLYvHkzN42fRmCLCMv15ly3LJOvGT6bAy9rf8yJMR1BzSBpWzrtp2xC6xLG9e1gc0onbyghqmbTpk08//zz9BhwK3uCehAXFcH0oZE2O+xIrrtm+HwO3Fk1M3N+/I+fviL3h42cObCThg0bWqYbmvNxEsyFqLj5KQf44PXZ/Hl4N3fNWEG+9kejaVQ/kIkxHSXXXUV1LgcOzquZWefH/eoHcyHrAFfceAcvfbaf2EVbLemUkpISyY0LUUHmmSfHO93O4ClLeTK2GxrN8u0Z5OQXAkiuu4b5dAB3xD4/nrV6Opf/9V6O7/iUWfMXk5qVS2RoMPFDujBjY5pNMLd/HCGEkdaahIQEhocXExcdwbu7s2k/ZRPLt2fYpDLlbLZm1bkAbp8fNxgMpG54g6AO1/JbyhLyj6WSmpVLh6mfWIJ8SJA/MzamWYK2zFoRwtaSJUtISEjgww8/lMqftajOBXCASQM72QxYzvr0AM2HPoF/oxYUHLOdehg/pAs5+UWyIYQQTmzbto1HHnmEQYMGER8fL5sR1yKfnIVSEdYLCJK2pXP/gCuZlLifUUl7SM3KtdxvxsY04ocYa6fJhhCirrMf1D927BiDht5KcPNQVq5cycxPfiZpWzqRocHEdGlJbkGRzDxxoTobwME2nWLOeadm5RKan0H9X3fzl9GPWN588UO6WH4HeTOKusfR3rK3/eNJ8vL+5NLhCSzalkVIkD+RocGkZuXSu0MzS+dHKn+6Rp0O4IDNVEFzMC/6/jueX/Mmf930PkfycsltEcravz0CARf3bE5MTpUgLuoM6wJxYOzAJGzYx8kuI7lvwK20vqyrTQfHfg2GfE5co84HcLg4vdAczEvOdmKvwcDUP3O4Bhh46lcGLXyW/9z8ME3/PhaFktNC4bOcrZ+wLhC3+M23CQrvzn0x3Xh2aFe01iRtT7ccY79UXrhGnRzELItSCr/4eFaVlBAJjAQOAA2KCkj877s8O7Qr8bFdTLNTAiynkkL4AnMJWEczrsxBPO/Q95xeP49z376Pwvj+j1201eZxZOCydlRkV/p2wNtAS0ADS7XWryilmgKrgQggHRiptf7ddU2tRZmZBAPrgf8DXgVeARqdPsGCLw6Sk19I/JAuGAwGqaQmfIajNIn1momSkhIefnUdpzfMI7DVZTTudw9J29MtPe/I0GCSJ/S1rJ8wP4b0wF2nIimUIuAxrfUPSqlgYJdSKgUYC3yptX5BKfU08DTGneq9X1gYZGTQHvgOaG+6+lzzVuScL7S8Ye3f4LLsXngzZ3X0zYP8T678hqXPPEDDhsHs+/4r3tp9juXbMyzHJ0/oi8FgkC0La1G5KRStdZbW+gfT77lAGtAGGAasMN1tBXCri9pY6/TMmVyoFwTA5YAfcDiwHoOaXkaJLiEuKsJSCMscvKWnIXyBdRA3My94+3LZHAwFf7D58020bdsWhe373bzYTcrD1p5K5cCVUhHA1cB/gZZa6yzTTScwplgcHTNeKbVTKbUzOzu7Om2tNWr0aL6Y+DxnW4SilYLwcNbcNpzvft7KrvUrmD5UVpoJ3+Ssjr7Wms/eX86nm5K55pprjGee29OJi7It1+ysXKxwjQrPQlFKNQQ+BCZqrXPsRqm1UsrhiIXWeimwFIzVCKvX3Noz+IXH0LMnW96IT2rNHuC9ZS9xR5OW4H8xiMuUQuEL7OsEmVOEi99aSXHRnSTc2p0bb7wRoMxyzfI5qD0VCuBKqQCMwXuV1voj09UnlVKhWusspVQocMpVjXQX6zeiUoqkpCS+/d8hPlwwlbEz3mDZ02OkxrHwGY7q6Dc58iWn171AartA1PAelvval1qW+d7uUW4KRRn/Im8BaVrrl61uWg+MMf0+BlhX883zLEFBQfw98TWatw7j8KdJgHGFpkwpFL7Cuk7Q2rVrmThxIsOGDWP1guml7iu76rhfuRs6KKX6At8A/wPMNVWnYsyDvw+EARkYpxH+VtZj1faGDq6SmZlJo0aNWPb9SZlSKLyW/QYncDEIb9u2jZiYGHr06MGXX35JgwYNSh0jao+zDR3KTaForbcCzv5iA6rbMG8UFhaG1poz5w6xcO7znP/jCWaP6kPChn2W+sfWH4iydgYSwh2s65os+OIgOecLLTvn/LNfOENuG4l/SAv+8tBc6tevDyAdFA8kS+mrSClFbNtCXti9kQVPHODdnTMwBNRjbFS4ZUAnMTmV1F9ziGwdYlMASD4Ewp2sF+xorY2lIUxrG8ZGhfPCZ4eoP+hJOoe14v2fcrjENEgvax48jwTwarjuuutYtfIdRo26i9PrXqDF8GmWubHmN3tkaLDTlW3yIRDuYL9gx6w47xyL//UGDbvdxD9GxNjsSiVllD2TT29q7Grm3vTCRa/yW8rrNIi8nuaxj6GUcWzYukyts0ptQriLeWNvgJKCPzn53jQKT2fS+v7XOfpanOWM0XwfgCOzB8v71g3q5KbGrmQ9Z/aRCQ8xc+ZM9NG9FJ27OJvSPLhprolsJsFbuJv1gp2SwnxOfZDAhVNHaH7rFPwbtSQxOdWyqbc1KVLlWSSFUkX2c2b1kC783jaaNal/WO4Tu2grGx6OZujibTbHysIf4S7m4GvufPytZyjvzXqEguP7aT70Cf55z+2Wcsn/PXyG1Kxcm0U9subBs0gArwbzYgYw1oFYk/oHY/uEU7Lrff694xipjOCyaZ8CUqlNuJ9l5knOHiZNfpLp2Vl8FNKEWTm/c8tDz3H1wNtoVD+QiTEdQUHqrzmy2tLDSQCvJvMb2dwbf2bwFdy3NpNDn66g8XlNo+tuB6RSm3Av88yTM0uTKEx5lZCCfABGnPuN3QGBXNW7A2poV8t70nrWlKy29FwyiFmDzG/2oqIirh4wjJ++3kSTG+8n5P9utenJyOwT4Q5aa861bENwdhYPYtysJMZ8Y3g4pKe7rW2ibFVeyCMqzhycZ37yMzm9H6BLUSFpX71Jr/bNSDKlwaUHI2qDs8VjDbOzGIdxh5bLsQrgmZm130hRbRLAa5h5cHPcXy5jyvPJ3HPPPcRe35XDjSMkbSJqhaPd4xOTU2ngD9/XC+KDgnwSgSetDwoLc1NrRXVIAHcB60pt7733nuVDdOTIEXc3Tfg4Z9uiLfvmFwq/eIWsgnwS/fyJLy6yHHOhXhABM2c6rZchPJfMA3cR+x25f/jhB7p06UJCQoLNhrHWYxAyv1ZUl3mg0bzBgnnXqLHR7Wl3aWNixkzmmRXLITwcrRRnW4TyxcTnUaNHu7vpogpkELOWFBUVETV4JDtS1vL4448TOvB+cvOLLAWEJsZ0lBoposaYV1CWFBZQcj6Ho6+OsdwmhdW8jwxiupmfnx+DHkog7XQBL774Iv+37xgnr7obpQyMjQoncYNpiyqpkSKqyZzzLinI49SHiRT/cYbpMd1JHN7D4dRA4b0kgNcSpRTP3XIlSs1h/swgdnzyHs382tCwa3/Lzt5SI0VUlzl4v5myl5JPZlF4fD9XjZ7KO9//in9AoKU2j5zp+QbJgdcipRTPDu1K4xviaDFiOpdEXm9ze3WCt30qTPLpdZNSiqKzJzn/0TR+O/YLd059hbOtexMZGkxwPX/LSuCc/EJ5j/gACeC1yNw7UkrR4PJeKGWg8OwJTn30PMV556pcKGh+ygGbY83PMz/lQE2/BOEFUj9+leI/z5KSksKqxH8SFx1BalYuC786ZLNhsZzpeT8J4LXEunrh2Khw4qIiACg6c4zCjN3kfTiVpcnfOgziZV22njZmPtb8PNLLqlvMf+ulS5eydetW+vbta1PDxEyCt++QAF5LrKsXPju0KyH1A4iLiuDBe0bw4JzlkJ/L2dVP8Xt6ms2Hy753bS7xad27djRtTHpZvsv+C1xrzcqVKxkyZAgFBQU0a9aMrl27Wm6XkrC+q9xBTKXUMiAWOKW1vtJ0XVNgNRABpGPc0Ph31zXTN1gv8LGuZKjUlTw06GoGDRrE0qfu5Y7T/yN6xQp0ZiZxzVsxvffdJBJHSJA/KaknLSU+S0pKLANS02MjbTaNkODtm+z3sjyXd4Gv1yxh86rF9O/fn+c+3sOlTZtY3l/WO0BJSVjfU5Ee+HLgZrvrnga+1Fp3BL40XRYVYP2BUUpZLnfu3Jlvv/2W0b1702PePMjIQGlN4+wsXkx5lTNLk3jly0OkZuUSGRpss9PPufMXSNwgvSxfZ50uS9iwjzPn/uDFqRPYvGox3W6IJfqhl/n37tOW1Jl9zXrrRT5S1sE3VGghj1IqAki26oH/DNygtc5SSoUCW7TWnct7nLq8kKc8lrm5ERGQkUEu8BIwBagHHAtpQd8Hk0odNzYq3LIprX0vS9Iovse6V529bg55+7+h8V/uIaTPSJRSDv/mzgpbCe9R01uqtdRaZ5l+PwG0LOOJxyuldiqldmZnZ1fx6XybTZ7bVBVuE5AA3AD8CrTOOe3wWEs+XXpZdYL1oGSjPnfQ/NYpNIq6s1Qdb/tjyrosvFe1BzG1sQvvtBuvtV6qte6pte7ZokWL6j6dzyk1i6RdOwDuBNYA/wN6Al80bsrhWYOIDA22OT4xOZWJMR1tPrjmD7ks1PA9b7/9NtHD4wAIvLQDl3SOtrndfjqp8G1VXYl5UikVapVCOVXuEcIh6x5V0rZ0znS7gxd+XUyDogJuBzoDw1DE5p5l+LSFpOqORIYGE9OlJbkFRTYDUvaPK3xHYWEhjz/+OAsXLqRe2FU8/rcJBAQGkbQ9HYDubRtRWFxieT/Iisu6oaoBfD0wBnjB9HNdjbWoDjIH8aRt6azv2h+AV35cgzp6lKvCwtgxZQoT/vMfLuvanchGl1p2uzf3sCRV4tuOHj3KnXfeybfffstfht/LDfdMIuHW7iz44iBxURGU6BL2HsuxDHBbr7iU2jq+rdxBTKXUvzGmYpsDJ4FngY+B94EwIAPjNMLfynsyGcR0zHpgyszZAGRxcTFPPPEE48eP54orrqjyh1MGtrxDYWEhnTp14vTp07z11luMHDnS5m9l/fmt6HtIeJ8qD2Jqre/SWodqrQO01m211m9prc9orQdorTtqrWMqEryFY/ZzdY/MHmxZlONoKmBGRgYrV67k2muvZdmyZVV6Tll67/kKC41TAQMCAli8eDE7d+5k5MiRgOOpqLLism6SlZhuVtm5uh06dGDPnj1cd9113Hfffdx9992cO3euws/naOl9woZ9NkvvZfDLfbTW/PLLL0RHR7NkyRK01gwZMoTOnTuX+XeRFZd1k2zo4CEqm9IoLi5mzpw5TJ8+neuuu45vvvmmwr0tRymbsVHhPDvUuPxaNpZwj5c//5mvN67hq6S5FGnFLQ8nckWfAeVu+FHWiktJo/iGmp4HLmpYZefq+vn5MWXKFLZt28bs2bNRSlFQUEB+fn6FnqvUrBXTjohSCKsWrVplXLhlMHCibVv+/eBw1i2Mp2l4Zx5Y8CHf6ctZvj3DstLW2d9FVlzWXdID91KOdh6//s4H+Pm7L/l49Ur69OkDOO7ZQ+kBL2vSa6sFq1bB+PGQlwfA58AwYNQNsXzVazxKle5bWf9dHJ2hycC075IeuA9xVkL2Z9qSfz6P6OhoJk+ezJwNe0sNViZs2Mfw17bbDJqOjQq3eXwJ3jXDWRlgrTVMm8bxvDxWmW67CWNVuGWHf3QYvME4t9scvB0NOsuKy7pHAriHKqsGuLOdx/95zwgyD+3nH//4B/Pnz+eF+4fw+uqNNkHeuH2bJi4qwpJGMadPzGTwq/qczfS5c8m3PLfuf7yakUFX4EHAPIWrJcDRo04fM3bRVks5YUlzCZA9MT2So/SI/QCW9eIfM/P9X3vtNUaNGsWECROI7t2RpG3plvuZT8PBGFRmbEwzFsKKimD6UCk3WhX2qYqSkhLLGRJgM6jYpuhXPk6YxwXgRmAJ0NTqsY4HNwegS2gwaVm5AIzpE8aO9N9Jzcqlw9RPAElzCSMJ4B7GOj0ClJpRYA4WzqaNmT/U/fr1Y8+ePQC0n7KJM58uwr9RS56Y/rrl+Bkb00j9NafU4Bd47upOT8vz2n/Zmmu0B9fzt5whmf+Wd3ZrwsL7bicg6BJuuHowH/34BZcUX7A8Vp5/Peb2u5exUeE0qh/Ade2bodE0qh9I8oS+luAN8uUqjCSAexj72ij2PWfr4F2RQv2Jyano4iKKz+fwx97PaB6Wwtv/WshPgVewfHsGcdERltyq9fN7YnCoyJlJbbL/srXfcOOZwVew7OuDnP9lBw06RTF71HVEN1xN3759uWbut0xp04Unv36bNrmnOR7cnLn97mV91/4cGdrV8vrMyvqyFnWXBHAPVFZ6xHy7o2ljcLHnbB3kx/W7nPg5W7nu4QXsWfMKd9xxB/XadOHv0+aWWTv64o5BqtRtFVGTveWKnpnUJvsvW7PI0GCm3tyJnuNm8Ov6pRSdzaLVvS8TuyiYDQ8PYejibQCs79qf9V37ExkaTKopXQK2wVl21RFlkWmEHqiitVGcBUjzT3OP1Vz8qqSkhPZPJ/PHjynk7FjLsdRdtGzZkvz8fIKCgkpt15VzvtByCl/WQhJHnKUWzMdXJeBWpmaMq9l/0bWfssnye/6RHzj7zTtcOHGI4NaX8e6SBbx6KJi0E39Yjo8MDWbDw9EMXbzNUoQqeUJfmyJU5tflaWceovY5m0YoPXAPU5kel6NpY9Yf9kkDO9nkZHMLilAGP4J73EzD7jfxr+9PEz+kBTfeeCOhoaFEXdKO+9a9C7mnLXtxru/an7FR4caFJNsr1tstL7VgH8wrqrwzk9pi/X8M2GxnpwsLOJ38EiogiGaxj3Fi7Qv4+/szuKTEJoedPKEvBoOBgZHGvVBiurTEYDA4HIOw/8Lz5DSXqF0SwD1MRdIjzjhKM5h7dObTdPsvheKiQmJiYlgwbx4f5eezBeMGp9HZWbzw6WIAlmMscVvR3m5ZqQXrvTwrm/oob+C2ppSV+rH+P9Zao4tLeO3t98hL+4bJsxax62gORaOeJ6BZO5RfADM/+dnymq3N2Jhm+pLtzKMDOmIwGGf0OgvOMsdbOCIpFA9V1fyxszRDSJA/OflFTk/Df2vXjsXHjrEQOIOxVvAd2O7FeWT24ErnwM2pBXuVTX1Up95HZf4vK5Ku0Fozbc1Olry1nJwdH1P0+680DQ3jnoSlfPxLUakNN5x9ecpUQFFRkkLxMlXtcZWVZijrNLzp8eNMBx4DlgOxpmO/ysnm3Hcf0PCqmEr1dh31lq1VNnBV9cykMvnjig6UpqWl8dr4gZw7d47A0I40H/Y0WR/MYNHmw8S1Kiy14Ya3TdUU3kN64D6mqgN9OjwcZdpQ2dqogCBWF+Zj8Pcn6LLe3Hb3vSRNHYe/v+PvfvtaK3FREcTHdiF20VabmRZV7X1WpjddlV67o/+/v/VsRefzqRQUFHD//fdTVFRE9K1jyGxyNYGtr0Cpi7vBAw7b52nz14V3kR54HVCdKWdq1iwK77ufgIKL1Qwv1AvijoeeI7zLVVzYl8KSt5az8tkHKEz9ivfee6/UY1j3dkOCAizbfd32+reWmRaO9vKsbE+8rMv2t5U3p55Vq2DaNMjMhLAw1MyZTL/7bpZ98wv5GXv5M/U/LHzte3Jzc4mKimLcuHHM/ORnTl55N/9w8n/sqH2SwxauIAHch1RnAJTRowkA9NSpqKNHISyMgJkzue3uuxmhFFoPYvbs2SQnJ9OsWTO01mRlZXH99dczbNgwhg8fztk/g1nx3VHQMH1oJAkb9rHi20x6tGtk6YnX9l6eZaaUVq1CWVUEzM/IIPDvf2ftD8f4fcs35P6wERXYgO5/uYmXpk7g+uuv98rVq8J3SQrFB9X06bqzPHLeyQzWL53Dgd3bKSkqolmzZoR27c1vV9xKQJPWAOWmFlytrEHdsXffwMnTJ/gc+AzYAmwHmoW0IPHZRZzLzuK7wnCUf6CxYqOG5d/arl6t6iInISrDWQqlWtUIlVI3K6V+VkodUko9XZ3HEjWnJk/XnZWuTdqWTv1Lwxj1zGu0eXgVIx6fy5AhQzjy439R/vUA+GPfZtI/epE333yTvXv3UlhYWKpNlg6EaXMDbTAYNzlYtcq+KVVqu3VK6adnrmdU9yYkbUvnw5RtXHH6BF2AR4EDQBxQH2iTe5olk0by3qxHGdfPONC5fHsGy7/NYGxUuOWLzLqkqwRv4Q5VTqEopfyAV4GBwDFgh1Jqvdba+dQD4XXKyyMb7wRJ2y6B0Eiajb/DEsyK//iN1f/5mOXLlwMQFBTE1VdfzdatWzEYDDyxZD15RZo5Qadp+MgjkJdnLGybkWHc7ABg9OgKtdO+B1xUVMTHH3/Mtxu+puGRg6xJPkpiWhrPPPMMcdEjCShuxY/16jOg4DwDgA7Wj9Wu3cXXPjSSpO3pF/8/7HYuctcyfiGgGikUpVQf4Dmt9V9Nl6cAaK1nOztGUijey35Ot/WccPvbrEvTLtt6hFsv8+O64LPs2LGDbamZDH4ogfghXbj8mmiO7P0OgOZAK6A38KbpcV5v2pQzkyYRGBhIYGAgBoOBNm3aMGLECAASExM5fvw43/98lN/PZFO/6A9iYmJYuHAhCRv2MXtkLy4UnKd9+/Z07dqVa665hkGDBtG7d29LDvz82PtoUFRw8YU2aIBeupTEkB6W1auyc5FwN1fMQmkDWFefP4bx82f/xOOB8QBhYWHVeDrhLmWtgDT/bnN/jJ0C68G9uwZ2YtSoUTb56A+Wvcq4BR8z4p0Z/ApkAYFWj/P6b7/xv/h4m8ceMGAAI0aMYH7KAV5dthLOn6PIvz5/Gi6hSbOWHCsOsWxccf9L7zFnzI00bNjQ8WsK6cGZmx/mya/fpnXOabKbXEqLhS8yI6SHwwU4CRv2mTbEwPL6JHgLd3L5LBSt9VJgKRh74K5+PlGzypqaqLVGoSw1UqyDnDn1Yl+7xSYdA9C6N3EhLWibk13qufeGhVF48CCFhYUUFhZSXFxMUFCQJS9ff9R8y4CieZ75bmB3OascbV7T+Dhar5t7cZ76TwDploHO3h2alblzkQRx4U7VCeDHgXZWl9uarhM+pLypiWCbSnjWVMva2ZQ6R9P65va7lxc+XVwqlcGsWZb0iT1HeXn7253NEHH0mpxtmGCzMEl2LhIepjo5cH+Mg/cDMAbuHcDdWut9zo6RHLj3KqtOeElJiaUYk/19HT2O/bS+yNBgHsr6np5LX6TF76fIadGKkJfnMqPR1WVWLCyr1srYqHCeHdoVwGbpvKPXYb5PWatXpaSrcKcaz4FrrYuUUg9jnELrBywrK3gL7+YokMHFfTXt9+t0xD4dY11m9tXQXqSOf8uyWjMnv5DlVrM87B/XUV6+S6uG9O7QzDjlz5Srtk7xzE/52aagF0DChn3sOXqOPUfPlrl6VUq6Ck9UrRy41noT4LgLJHxOdXfFcZS6eHRAR0u98t4dmpG0Ld1SM8U85xpK96KdfRH07tCMsVHhNkHcnCc3l7G1bvvy7RmWlaLlrayU5fDC08hKTFEpNbErTlk7CTmajmgeGLWee77gi4NOd/yZGNPR4ZTHstoO7lkpKkRFOEuhSAAXlVbWnPDqPKZ9cLXmKKc9MaZjqaBrvt3ZF4wr2i6Eq7lkKb2oe5zNCa9OR8A+JXJk9mBj7REr9isgc/ILHT1UqceJi46wlAEoKSmp8bYL4U5SjVBUmKt2SLfPjUPpOddJ29MtS9qdpWzKmvIYXM+/1IbBMhVQeDsJ4KLCqlWuthzmwUkoPee6Misgy5otMj/lgJSBFT5FcuCi0ly9u4yjXd+tC0pVpwaJ7IwjvJHsyCNqjKun05XVG69u2kOmAgpfIgFceCRzYHVVykYIXyApFOHxJO0h6jqZRii8lqQ9hHBMArgQQngpCeBCCOGlJIALIYSXkgAuhBBeqlZnoSilsoGMcu/oWZoDp93diFomr7lukNfsPcK11i3sr6zVAO6NlFI7HU3f8WXymusGec3eT1IoQgjhpSSACyGEl5IAXr6l7m6AG8hrrhvkNXs5yYELIYSXkh64EEJ4KQngQgjhpSSAV4JS6jGllFZKNXd3W1xNKTVPKbVfKfWjUmqtUqqxu9vkKkqpm5VSPyulDimlnnZ3e1xNKdVOKbVZKZWqlNqnlHrU3W2qDUopP6XUbqVUsrvbUlMkgFeQUqodcBOQ6e621JIU4EqtdTfgADDFze1xCaWUH/AqMAiIBO5SSkW6t1UuVwQ8prWOBK4DHqoDrxngUSDN3Y2oSRLAK24+8CRQJ0Z9tdafa62LTBe/A9q6sz0u1As4pLU+rLW+ALwHDHNzm1xKa52ltf7B9HsuxqDWxr2tci2lVFtgCPCmu9tSkySAV4BSahhwXGu9191tcZNxwCfuboSLtAGOWl0+ho8HM2tKqQjgauC/bm6Kqy3A2AErcXM7apRsqWailPoCaOXgpmnAVIzpE59S1mvWWq8z3WcaxlPuVbXZNuF6SqmGwIfARK11jrvb4ypKqVjglNZ6l1LqBjc3p0ZJADfRWsc4ul4pdRXQHthr2gmmLfCDUqqX1vpELTaxxjl7zWZKqbFALDBA++6CgeNAO6vLbU3X+TSlVADG4L1Ka/2Ru9vjYtHALUqpwUAQEKKUWqm1/pub21VtspCnkpRS6UBPrbU3VjSrMKXUzcDLwPVa62x3t8dVlFL+GAdpB2AM3DuAu7XW+9zaMBdSxp7ICuA3rfVENzenVpl64I9rrWPd3JQaITlw4cxiIBhIUUrtUUr9y90NcgXTQO3DwGcYB/Pe9+XgbRIN3APcaPrb7jH1ToWXkR64EEJ4KemBCyGEl5IALoQQXkoCuBBCeCkJ4EII4aUkgAshhJeSAC6EEF5KArgQQnip/wcNSzYbPhSk5AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "## Bayesian Linear Regression\n",
    "class BayesRegression:\n",
    "\tdef __init__(self, train_input, train_output, noise=0):\n",
    "\t\tself.X = np.array(train_input)\n",
    "\t\tself.y = np.array(train_output).reshape((-1, 1))\n",
    "\t\tself.input_dim = self.X.shape[0]\n",
    "\t\tself.num_examples = self.X.shape[1]\n",
    "\t\tself.prior_mean = np.zeros((self.input_dim, 1))\n",
    "\t\tself.prior_covar = np.identity(self.input_dim)\n",
    "\t\tself.noise = noise # Defines the variance of gaussian observation noise\n",
    "\n",
    "\t# Define custom prior for weights\n",
    "\tdef set_prior(self, mean, var):\n",
    "\t\tself.prior_mean = mean\n",
    "\t\tself.prior_covar = np.identity(self.num_examples) * var\n",
    "\n",
    "\t# Computes poseterior parameters from training data and prior \n",
    "\tdef fit(self):\n",
    "\t\tA = (self.X @ self.X.T / self.noise**2\n",
    "\t   \t\t + np.linalg.inv(self.prior_covar))\n",
    "\t\tself.posterior_covar = np.linalg.inv(A)\n",
    "\t\t# print(self.posterior_covar.shape)\n",
    "\t\t# print(\"X \", self.X.shape)\n",
    "\t\t# print(\"y \", self.y.shape)\n",
    "\t\t# print(\"prior covar \", self.prior_covar.shape)\n",
    "\t\t# print(\"prior mean \", self.prior_mean.shape)\n",
    "\t\tself.posterior_mean = (self.posterior_covar\n",
    "\t\t\t\t\t\t \t   @ (self.X @ self.y / self.noise**2\n",
    "\t\t\t  \t\t\t\t\t  + np.linalg.inv(self.prior_covar)\n",
    "\t\t\t\t\t\t\t\t  @ self.prior_mean))\n",
    "\n",
    "\t\t# print(\"post mean \", self.posterior_mean.shape)\n",
    "\t\t# print(\"post covar \", self.posterior_covar.shape)\n",
    "\t\treturn self.posterior_mean, self.posterior_covar\n",
    "\t\n",
    "\t# Compute mean and covariance of predictive distribution\n",
    "\tdef predict(self, test_input):\n",
    "\t\ttest_input = np.array(test_input)\n",
    "\t\tself.pred_mean = test_input.T @ self.posterior_mean\n",
    "\t\tself.pred_covar = test_input.T @ self.posterior_covar @ test_input\n",
    "\t\treturn self.pred_mean, self.pred_covar\n",
    "\n",
    "\n",
    "### Generate example linear dataset\n",
    "\n",
    "# Parameters for the linear relationship\n",
    "a = 2  # slope\n",
    "b = 3  # intercept\n",
    "c = 1\n",
    "noise_std = 2  # standard deviation of the noise\n",
    "\n",
    "np.random.seed(0)  # for reproducibility\n",
    "X = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Generating Y values with noise\n",
    "y = a * X**2 + b * X + c\n",
    "y_noisy = y + np.random.normal(0, noise_std, y.shape)\n",
    "\n",
    "# Creating a DataFrame\n",
    "dataset = pd.DataFrame({'X': X, 'y': y})\n",
    "\n",
    "X_homo = X.reshape((1, -1))\n",
    "X_homo = np.vstack((X_homo, np.ones(X_homo.shape)))\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "print(X.shape)\n",
    "X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
    "print(X_poly.shape)\n",
    "print(X_poly[1,:])\n",
    "print(y.shape)\n",
    "model = BayesRegression(X_poly.T, y, noise_std)\n",
    "post_mean, post_covar = model.fit()\n",
    "print(post_mean)\n",
    "print(post_covar)\n",
    "\n",
    "# Test inputs\n",
    "test_input = np.arange(-5, 5, 1).reshape(-1, 1)\n",
    "test_input_poly = poly.transform(test_input)\n",
    "# print(test_input_poly.shape)\n",
    "# test_input_homo = np.vstack((test_input, np.ones(test_input.shape)))\n",
    "pred_mean, pred_covar = model.predict(test_input_poly.T)\n",
    "\n",
    "# print(X.shape, y.shape, y_noisy.shape)\n",
    "plt.scatter(X, y_noisy, marker='x')\n",
    "plt.plot(X, y, linestyle='--', color='black')\n",
    "plt.scatter(test_input, pred_mean, color='red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
