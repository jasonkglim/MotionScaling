{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from utils import stratified_sample, annotate, even_train_split\n",
    "import glob  # Importing the glob module to find all the files matching a pattern\n",
    "\n",
    "# Pattern to match the data files\n",
    "file_pattern = \"data_files/user_*/metric_df.csv\"\n",
    "\n",
    "# Initialize a dictionary to store one_user_one_user_dataframes for each dataset\n",
    "all_datasets = {}\n",
    "\n",
    "# Loop through each file that matches the file pattern\n",
    "for filepath in glob.glob(file_pattern):\n",
    "    # print(filepath)\n",
    "    # print(filepath.split('/'))\n",
    "    user_name = filepath.split('/')[1]\n",
    "    # user_name = filepath.split('\\\\')[1]\n",
    "    print(f\"Processing {filepath} dataset...\")\n",
    "\n",
    "    # Read in data file as a pandas dataframe\n",
    "    data = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    # add weighted performance metric\n",
    "    w = 1\n",
    "    data[\"total_error\"] = data['avg_osd'] + data['avg_target_error']\n",
    "    data[\"weighted_performance\"] = 10*data['throughput'] - w*data[\"total_error\"]\n",
    "\n",
    "    all_datasets[user_name] = data\n",
    "\n",
    "# Combine datasets for Lizzie\n",
    "lizzie1 = all_datasets[\"user_lizzie1\"]\n",
    "lizzie2 = all_datasets[\"user_lizzie2\"]\n",
    "combined_df = pd.concat([lizzie1, lizzie2])\n",
    "all_datasets[\"user_lizzie\"] = combined_df.groupby(['latency', 'scale']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import PolyRegression, GPRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, RationalQuadratic, WhiteKernel\n",
    "import utils\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename='warnings_log.txt', level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "# Function to redirect warnings to logging\n",
    "def warn_to_logging(message, category, filename, lineno, file=None, line=None):\n",
    "    logging.warning(f'{filename}:{lineno}: {category.__name__}: {message}')\n",
    "\n",
    "# Redirect all warnings to the warn_to_logging function\n",
    "warnings.showwarning = warn_to_logging\n",
    "\n",
    "all_results = {}\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"total_error\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t\n",
    "\tprint(output_metric)\n",
    "\tuser_results = {}\n",
    "\tfor user, data in list(all_datasets.items()): \n",
    "\t\t# if user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t# \tcontinue\n",
    "\t\tprint(f\"\\t{user}\")\n",
    "\n",
    "\t\t# Prepare data \n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tY = data[output_metric]\n",
    "\n",
    "\t\t# Initialize evaluation metrics\n",
    "\t\toptimal_match_rate = []\n",
    "\t\toptimal_scale_error = []\n",
    "\t\tmse_scores = []\n",
    "\t\tfull_mse_scores = []\n",
    "\t\tn_train_mse = []\n",
    "\t\tn_train_full_mse = []\n",
    "\t\tn_train_p = []\n",
    "\n",
    "\t\tn = len(data)\n",
    "\t\tn_train_values = range(2, n-1)\n",
    "\t\tfor n_train in n_train_values:\n",
    "\n",
    "\t\t\tn_train_p.append(n_train / n)\n",
    "\t\t\t# Split into training/test sets\n",
    "\t\t\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\t\t\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\t\t\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\t\t\tY_train, Y_test = train_set[output_metric], test_set[output_metric]\n",
    "\t\t\t\n",
    "\t\t\t# Create dense test input\n",
    "\t\t\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t\t\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\t\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\t\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\t\t\n",
    "\t\t\t# # Polynomial Regression\n",
    "\t\t\tif model_type.startswith(\"Poly\"):\n",
    "\t\t\t\tdegree = int(model_type.strip(\"Poly\"))\n",
    "\t\t\t\tY_pred, model_params = PolyRegression(X_train.values, Y_train.values, X.values, degree)\n",
    "\t\t\t\tY_pred_dense, _ = PolyRegression(X_train.values, Y_train.values, X_dense, degree)\n",
    "\n",
    "\t\t\t# Gaussian Process Regression\n",
    "\t\t\telif model_type.startswith(\"GPR\"):\n",
    "\t\t\t\t# Choose kernel\n",
    "\t\t\t\tkernel_type = model_type.removeprefix(\"GPR_\")\n",
    "\t\t\t\t# print(kernel_type)\n",
    "\t\t\t\tif kernel_type == \"RBF_Noise_default\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF() + WhiteKernel() # Default RBF with likelihood noise\n",
    "\t\t\t\telif kernel_type == \"RBF_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0])\n",
    "\t\t\t\telif kernel_type == \"RBF_Noise_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0]) + WhiteKernel() # RBF with anistropic length scale\n",
    "\t\t\t\telif kernel_type == \"RQ_Noise_default\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RationalQuadratic() + WhiteKernel() # Default Rational Quadratic with likelihood noise\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(\"Invalid kernel specification!\")\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\tY_pred, Y_pred_std, model_params = GPRegression(X_train.values, Y_train.values, X.values, kernel)\n",
    "\t\t\t\tY_pred_dense, Y_pred_std, _ = GPRegression(X_train.values, Y_train.values, X_dense, kernel)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Invalid model type specification!\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t## Evaluate metrics\n",
    "\t\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "\t\t\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t\t# Mean Square Error on whole dataset\n",
    "\t\t\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\t\t\tif True: #full_mse < 5000:\n",
    "\t\t\t\tn_train_full_mse.append(n_train)\n",
    "\t\t\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t\t\t# Mean Square Error on test set\n",
    "\t\t\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\t\t\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\t\t\tif True: #mse < 5000:\n",
    "\t\t\t\tn_train_mse.append(n_train)\n",
    "\t\t\t\tmse_scores.append(mse)\n",
    "\t\t\t\n",
    "\t\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\t\t\telse: # optimal scale at minimum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t\t# Merge the results on 'latency'\n",
    "\t\t\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\t\t\n",
    "\t\t\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t\t\t# print(optimal_scale_dense)\n",
    "\t\t\t# print(merged_ref_dense)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# Count the number of matches\n",
    "\t\t\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\t\t\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\t\t\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\t\t\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t\t\t# Visualize model prediction\n",
    "\t\t\tif n_train == n-2:\n",
    "\t\t\t\tutils.model_heatmaps(data, dense_df, X_train, user, output_metric, model_type, model_params)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Store results from this dataset\n",
    "\t\t\tuser_results[user] = {\n",
    "\t\t\t\t'n_train_mse': list(n_train_mse),\n",
    "\t\t\t\t'n_train_full_mse': list(n_train_full_mse),\n",
    "\t\t\t\t'full_mse_scores': full_mse_scores,\n",
    "\t\t\t\t'mse_scores': mse_scores,\n",
    "\t\t\t\t'n_train_all': list(n_train_values),\n",
    "\t\t\t\t'match_rate': optimal_match_rate,\n",
    "\t\t\t\t'scale_error': optimal_scale_error,\n",
    "\t\t\t\t'n_train_p': n_train_p\n",
    "\t\t\t}\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\t\n",
    "\telse:\n",
    "\t\tall_results[output_metric] = user_results\n",
    "\t\tcontinue\n",
    "\tbreak\n",
    "\n",
    "# print(all_results.keys())\n",
    "with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "\tjson.dump(all_results, file)\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "# \tog_results = json.load(file)\n",
    "\n",
    "# og_results[\"total_error\"] = all_results[\"total_error\"]\n",
    "\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(og_results, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the results for all datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "\tall_results = json.load(file)\n",
    "\n",
    "for output_metric, user_results in all_results.items():\n",
    "\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\tfig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}\")\n",
    "\tfor user, results in user_results.items():\n",
    "\t\taxes[0, 0].plot(results['n_train_p'], results['match_rate'], marker='o', label=user)\n",
    "\t\taxes[0, 1].plot(results['n_train_p'], results['scale_error'], marker='o', label=user)\n",
    "\t\taxes[1, 0].plot(results['n_train_p'], results['full_mse_scores'], marker='o', label=user)\n",
    "\t\taxes[1, 1].plot(results['n_train_p'], results['mse_scores'], marker='o', label=user)\n",
    "\n",
    "\taxes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\taxes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\taxes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\taxes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\taxes[1, 0].set_title('MSE on whole dataset')\n",
    "\taxes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\taxes[1, 1].set_title('MSE on test set')\n",
    "\taxes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\taxes[1, 0].legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/model_results/{output_metric}/{model_type}_model_eval_metrics_{output_metric}.png\", facecolor='w')\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot averaged results\n",
    "\n",
    "# Load data\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "\tall_results = json.load(file)\n",
    "\n",
    "print(all_results)\n",
    "for output_metric, user_results in all_results.items():\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\tfig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}, mean over users\")\n",
    "\n",
    "\t# Prepare lists of lists to store data\n",
    "\tmatch_rate_lists = []\n",
    "\tscale_error_lists = []\n",
    "\tfull_mse_score_lists = []\n",
    "\tmse_score_lists = []\n",
    "\tn_train_lists = []\n",
    "\n",
    "\t# Collect data for each metric\n",
    "\tfor user, results in user_results.items():\n",
    "\t\tmatch_rate_lists.append(results['match_rate'])\n",
    "\t\tscale_error_lists.append(results['scale_error'])\n",
    "\t\tfull_mse_score_lists.append(results['full_mse_scores'])\n",
    "\t\tmse_score_lists.append(results['mse_scores'])\n",
    "\t\tn_train_lists.append(results['n_train_all'])\n",
    "\n",
    "\t# Function to calculate average and standard deviation safely\n",
    "\tdef safe_mean_std(data_lists, index):\n",
    "\t\tvalid_data = [data[index] for data in data_lists if index < len(data)]\n",
    "\t\treturn np.mean(valid_data), np.std(valid_data)\n",
    "\n",
    "\t# Calculate averages and standard deviations safely\n",
    "\tavg_match_rate = []\n",
    "\tstd_match_rate = []\n",
    "\tavg_scale_error = []\n",
    "\tstd_scale_error = []\n",
    "\tavg_full_mse_scores = []\n",
    "\tstd_full_mse_scores = []\n",
    "\tavg_mse_scores = []\n",
    "\tstd_mse_scores = []\n",
    "\n",
    "\tmin_n_train_length = min([len(data_list) for data_list in n_train_lists])\n",
    "\tmin_n_train_list = n_train_lists[0][:min_n_train_length]\n",
    "\tfor n in range(min_n_train_length):\n",
    "\t\tmean, std = safe_mean_std(match_rate_lists, n)\n",
    "\t\tavg_match_rate.append(mean)\n",
    "\t\tstd_match_rate.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(scale_error_lists, n)\n",
    "\t\tavg_scale_error.append(mean)\n",
    "\t\tstd_scale_error.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(full_mse_score_lists, n)\n",
    "\t\tavg_full_mse_scores.append(mean)\n",
    "\t\tstd_full_mse_scores.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(mse_score_lists, n)\n",
    "\t\tavg_mse_scores.append(mean)\n",
    "\t\tstd_mse_scores.append(std)\n",
    "\n",
    "\t# Plotting the average values and standard deviation\n",
    "\t# Plotting the average values and standard deviation\n",
    "\taxes[0, 0].plot(min_n_train_list, avg_match_rate, marker='o', label='Mean over users')\n",
    "\taxes[0, 0].fill_between(min_n_train_list, np.subtract(avg_match_rate, std_match_rate), \n",
    "\t\t\t\t\t\t\tnp.add(avg_match_rate, std_match_rate), alpha=0.2)\n",
    "\n",
    "\taxes[0, 1].plot(min_n_train_list, avg_scale_error, marker='o', label='Mean over users')\n",
    "\taxes[0, 1].fill_between(min_n_train_list, np.subtract(avg_scale_error, std_scale_error), \n",
    "\t\t\t\t\t\t\tnp.add(avg_scale_error, std_scale_error), alpha=0.2)\n",
    "\n",
    "\taxes[1, 0].plot(min_n_train_list, avg_full_mse_scores, marker='o', label='Mean over users')\n",
    "\taxes[1, 0].fill_between(min_n_train_list, np.subtract(avg_full_mse_scores, std_full_mse_scores), \n",
    "\t\t\t\t\t\t\tnp.add(avg_full_mse_scores, std_full_mse_scores), alpha=0.2)\n",
    "\n",
    "\taxes[1, 1].plot(min_n_train_list, avg_mse_scores, marker='o', label='Mean Over Users')\n",
    "\taxes[1, 1].fill_between(min_n_train_list, np.subtract(avg_mse_scores, std_mse_scores), \n",
    "\t\t\t\t\t\t\tnp.add(avg_mse_scores, std_mse_scores), alpha=0.2)\n",
    "\t\n",
    "\taxes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\taxes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\taxes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\taxes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\taxes[1, 0].set_title('MSE on whole dataset')\n",
    "\taxes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\taxes[1, 1].set_title('MSE on test set')\n",
    "\taxes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\t# axes[1, 0].legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/model_results/{output_metric}/{model_type}_model_eval_metrics_{output_metric}_average.png\", facecolor='w')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale\n",
    "\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tfor user, data in list(all_datasets.items()):\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\t# model_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\t# kernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\t# Y_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\t# dense_df = pd.DataFrame({\n",
    "\t\t# \t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t# \t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t# \t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t# \t\t})\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t# optimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\t# optimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\t# plt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\")\n",
    "\t\tplt.ylabel(\"scaling factor\")\n",
    "\t\tplt.plot(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='x', label=user)\n",
    "\tplt.legend()\n",
    "\tplt.savefig(f\"figures/optimal_scale_per_latency/{output_metric}.png\", facecolor='w')\n",
    "\tplt.show()\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale per latency with model prediction\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tfor user, data in list(all_datasets.items())[:1]:\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\tmodel_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\tkernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\tY_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\tplt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\")\n",
    "\t\tplt.ylabel(\"scaling factor\")\n",
    "\t\tplt.scatter(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='x', label=\"measured\")\n",
    "\t\tplt.plot(optimal_scale_dense['latency'], optimal_scale_dense['scale'], label=\"predicted\")\n",
    "\tplt.legend()\n",
    "\t# plt.savefig(f\"figures/optimal_scale_per_latency/{output_metric}.png\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot key metric heatmaps for average over users\n",
    "from utils import annotate_extrema\n",
    "\n",
    "delete_keys = [\"user_lizzie\", \"user_lizzie1\", \"user_lauren\", \"user_sarah1\"]\n",
    "sub_datasets = [all_datasets[key] for key in all_datasets.keys() if key not in delete_keys]\n",
    "\n",
    "combined_df = pd.concat(sub_datasets)\n",
    "averaged_df = combined_df.groupby([\"latency\", \"scale\"]).mean().reset_index()\n",
    "\n",
    "# averaged_df = all_datasets[\"user_lizzie\"]\n",
    "# Create a 2x5 subplot for the heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "# Plot the heatmap for throughput\n",
    "heatmap_throughput = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='throughput')\n",
    "ax = sns.heatmap(heatmap_throughput, ax=axes[0], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[0].set_title('Throughput vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_throughput.values, ax)\n",
    "\n",
    "# Plot heatmap for total error (target deviation + osd)\n",
    "averaged_df['total_error'] = averaged_df['avg_osd'] + averaged_df['avg_target_error']\n",
    "heatmap_error = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='total_error')\n",
    "ax = sns.heatmap(heatmap_error, ax=axes[1], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[1].set_title('Total Error vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_error.values, ax, extrema_type='min')\n",
    "\n",
    "# Plot heatmap for combined performance (movement speed - total error)\n",
    "heatmap_combo = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='weighted_performance')\n",
    "ax = sns.heatmap(heatmap_combo, ax=axes[2], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[2].set_title('Combined Performance vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_combo.values, ax, extrema_type='max')\n",
    "\n",
    "# plt.title(\"User A\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{data_folder}/heatmap_key_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function, e.g., a sine wave function\n",
    "def smooth_2d_function(x, y):\n",
    "    return np.sin(np.sqrt(x**2 + y**2))\n",
    "\n",
    "# Generate sample points\n",
    "x = np.linspace(-5, 5, 10)\n",
    "y = np.linspace(-5, 5, 10)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Apply the function to the sample points\n",
    "z = smooth_2d_function(x, y)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'x': x.ravel(), 'y': y.ravel(), 'z': z.ravel()})\n",
    "\n",
    "X = df[['x', 'y']]\n",
    "Y = df['z']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
    "\n",
    "z_pred, _ = GPRegression(X_train, Y_train, X)\n",
    "z_pred = z_pred.reshape(x.shape)\n",
    "\n",
    "# df[\"y_pred\"] = Y_pred\n",
    "\n",
    "# Plotting the function for visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "ax = axes[0].contourf(x, y, z, cmap='viridis')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Smooth 2D Function')\n",
    "\n",
    "ax = axes[1].contourf(x, y, z_pred, cmap='viridis')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Predictions')\n",
    "\n",
    "fig.colorbar(ax, label='Function Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   latency  scale  col1  col2\n",
      "0        1      3     5     7\n",
      "1        2      4     6     8\n",
      "0        1      3     9    11\n",
      "1        2      4    10    12\n",
      "latency  scale\n",
      "1        3         [5, 9]\n",
      "2        4        [6, 10]\n",
      "Name: col1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataframes\n",
    "df1 = pd.DataFrame({'latency': [1, 2], 'scale': [3, 4], 'col1': [5, 6], 'col2': [7, 8]})\n",
    "df2 = pd.DataFrame({'latency': [1, 2], 'scale': [3, 4], 'col1': [9, 10], 'col2': [11, 12]})\n",
    "\n",
    "# Concatenating the dataframes\n",
    "combined_df = pd.concat([df1, df2])\n",
    "print(combined_df)\n",
    "# Grouping by 'latency' and 'scale' and calculating the average of other columns\n",
    "grouped_df = combined_df.groupby(['latency', 'scale'])['col1'].apply(list)\n",
    "\n",
    "print(grouped_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_samples_with_noise(x_min, x_max, num_x, num_samples, noise_std):\n",
    "    \"\"\"\n",
    "    Generates samples from a sine function with added Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    x_min (float): Minimum x value.\n",
    "    x_max (float): Maximum x value.\n",
    "    num_x (int): Number of distinct x values in the range.\n",
    "    num_samples (int): Number of samples to generate for each x value.\n",
    "    noise_std (float): Standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Generate x values\n",
    "    x_values = np.linspace(x_min, x_max, num_x)\n",
    "\n",
    "    # Plot the underlying sine function\n",
    "    plt.plot(x_values, np.sin(x_values), label='Underlying sine function', color='blue')\n",
    "\n",
    "    # Generate and plot samples with noise for each x value\n",
    "    all_samples = []\n",
    "    for x in x_values:\n",
    "        noisy_samples = np.sin(x) + np.random.normal(0, noise_std, num_samples)\n",
    "        plt.scatter([x]*num_samples, noisy_samples, color='red', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Samples from a Sine Function with Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "generate_samples_with_noise(x_min=0, x_max=2*np.pi, num_x=30, num_samples=10, noise_std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from utils import annotate_extrema\n",
    "from models import BayesRegression\n",
    "\n",
    "### Modeling approach 2. Using Bayesian Regression, and all users as prior\n",
    "\n",
    "# Separate datasets into one vs rest\n",
    "user_to_remove = \"user_jason\"\n",
    "one_user_data = all_datasets[user_to_remove]\n",
    "sub_datasets = [all_datasets[key] for key in all_datasets.keys() if key != user_to_remove]\n",
    "combined_df = pd.concat(sub_datasets)\n",
    "averaged_df = combined_df.groupby([\"latency\", \"scale\"]).mean().reset_index()\n",
    "\n",
    "# Perform Bayesian Regression on combined dataset\n",
    "metric = 'throughput'\n",
    "X = combined_df[['latency', 'scale']]\n",
    "y = combined_df[metric]\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X.values)\n",
    "# print(X_poly.shape)\n",
    "# print(y.shape)\n",
    "\n",
    "noise_std = 1\n",
    "model = BayesRegression(X_poly.T, y, noise=noise_std)\n",
    "weight_prior_mean, weight_prior_covar = model.fit()\n",
    "# print(post_mean.shape, post_covar.shape)\n",
    "\n",
    "latency_range = np.arange(0.0, combined_df['latency'].max()+0.01, 0.01)\n",
    "scale_range = np.arange(combined_df['scale'].min(), combined_df['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "latency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "X_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "X_dense = np.round(X_dense, 3)\n",
    "X_dense_poly = poly.transform(X_dense)\n",
    "\n",
    "# Y_pred, _ = model.predict(X_poly.T)\n",
    "Y_pred_dense, _ = model.predict(X_dense_poly.T)\n",
    "dense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "# one_user_data[\"Y_pred\"] = Y_pred\n",
    "\n",
    "# utils.model_heatmaps(one_user_data, dense_df, X, \n",
    "# \t\t\t\t\t user_to_remove, metric, \n",
    "# \t\t\t\t\t \"BayesRegression\", post_mean.flatten())\n",
    "\n",
    "if metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: \n",
    "        extrema_type = \"max\" # optimal scale at maximum\n",
    "else:\n",
    "\textrema_type = \"min\" # optimal scale at minimum\n",
    "\t\t\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "title = (f\"Modeling all but one user\")\n",
    "fig.suptitle(title)\n",
    "\n",
    "# Average over all but one user\n",
    "averaged_data = averaged_df.pivot(\n",
    "\tindex='latency', columns='scale', values=metric\n",
    ")\n",
    "sns.heatmap(averaged_data, cmap='YlGnBu', ax=ax[0], annot=True)\n",
    "# annotate(ax[1], averaged_data, X_train, color='green')\n",
    "ax[0].set_title('Average over all but one user')\n",
    "ax[0].set_xlabel('Scale')\n",
    "ax[0].set_ylabel('Latency')\n",
    "annotate_extrema(averaged_data.values, ax[0], extrema_type)\n",
    "\n",
    "dense_pred_data = dense_df.pivot(\n",
    "\tindex='latency', columns='scale', values='Y_pred_dense'\n",
    ")\n",
    "sns.heatmap(dense_pred_data, cmap='YlGnBu', ax=ax[1])\n",
    "# annotate(ax[1], dense_pred_data, X_train, color='green')\n",
    "ax[1].set_title(f'Predicted Data over Dense Input\\n{weight_prior_mean.flatten()}')\n",
    "ax[1].set_xlabel('Scale')\n",
    "ax[1].set_ylabel('Latency')\n",
    "annotate_extrema(dense_pred_data.values, ax[1], extrema_type)\n",
    "\n",
    "# # Plot residuals\n",
    "# data[\"residual\"] = np.abs(data[\"performance\"] - data[\"Y_pred\"])\n",
    "# residual = data.pivot(\n",
    "# \tindex='latency', columns='scale', values='residual'\n",
    "# )\n",
    "# sns.heatmap(residual, cmap='YlGnBu', ax=ax[2], annot=True)\n",
    "# annotate(ax[2], residual, X_train, color='green')\n",
    "# ax[2].set_title('Residuals')\n",
    "# ax[2].set_xlabel('Scale')\n",
    "# ax[2].set_ylabel('Latency')\n",
    "# annotate_extrema(residual.values, ax[2], 'min')\n",
    "\n",
    "plt.tight_layout()\n",
    "filepath = f\"figures/allbutone_noise_{noise_std}.png\"\n",
    "plt.savefig(filepath, facecolor='w')\n",
    "plt.show()\n",
    "# plt.close()\n",
    "\n",
    "## Use as prior for one user model\n",
    "data = one_user_data\n",
    "X = data[['latency', 'scale']]\n",
    "Y = data[metric]\n",
    "\n",
    "# Initialize evaluation metrics\n",
    "optimal_match_rate = []\n",
    "optimal_scale_error = []\n",
    "mse_scores = []\n",
    "full_mse_scores = []\n",
    "n_train_mse = []\n",
    "n_train_full_mse = []\n",
    "n_train_p = []\n",
    "\n",
    "n = len(data)\n",
    "n_train_values = range(2, n-1)\n",
    "for n_train in n_train_values:\n",
    "\n",
    "\tn_train_p.append(n_train / n)\n",
    "\t# Split into training/test sets\n",
    "\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\tY_train, Y_test = train_set[metric], test_set[metric]\n",
    "\t\n",
    "\t# Create dense test input\n",
    "\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\tpoly = PolynomialFeatures(degree=2)\n",
    "\tX_poly = poly.fit_transform(X.values)\n",
    "\tX_train_poly = poly.transform(X_train.values)\n",
    "\tX_test_poly = poly.transform(X_test.values)\n",
    "\tX_dense_poly = poly.transform(X_dense)\n",
    "\t\n",
    "\t# Train model\n",
    "\tmodel = BayesRegression(X_train_poly.T, Y_train, noise_std)\n",
    "\tmodel.set_prior(weight_prior_mean, weight_prior_covar)\n",
    "\tpost_mean, post_covar = model.fit()\n",
    "\n",
    "\t# Predict\n",
    "\tY_pred, _ = model.predict(X_poly.T)\n",
    "\tY_pred_dense, _ = model.predict(X_dense_poly.T)\n",
    "\n",
    "\t## Evaluate metrics\n",
    "\tdense_df = pd.DataFrame({\n",
    "\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t})\n",
    "\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t# Mean Square Error on whole dataset\n",
    "\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\tif True: #full_mse < 5000:\n",
    "\t\tn_train_full_mse.append(n_train)\n",
    "\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t# Mean Square Error on test set\n",
    "\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\tif True: #mse < 5000:\n",
    "\t\tn_train_mse.append(n_train)\n",
    "\t\tmse_scores.append(mse)\n",
    "\t\n",
    "\tif metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmax()][['latency', 'scale']]\n",
    "\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\telse: # optimal scale at minimum\n",
    "\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmin()][['latency', 'scale']]\n",
    "\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t# Merge the results on 'latency'\n",
    "\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\n",
    "\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t# print(optimal_scale_dense)\n",
    "\t# print(merged_ref_dense)\n",
    "\t\n",
    "\n",
    "\t# Count the number of matches\n",
    "\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t# Visualize model prediction\n",
    "\tif n_train == n-2:\n",
    "\t\tutils.model_heatmaps(data, dense_df, X_train, user_to_remove, metric, \"BayesRegressionwithPrior\", post_mean.flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from models import BayesRegression\n",
    "\n",
    "\n",
    "### Generate example linear dataset\n",
    "\n",
    "# Parameters for the linear relationship\n",
    "a = 2  # slope\n",
    "b = 3  # intercept\n",
    "c = 1\n",
    "noise_std = 30 # standard deviation of the noise\n",
    "\n",
    "np.random.seed(0)  # for reproducibility\n",
    "X = np.linspace(-5, 5, 100)\n",
    "y = []\n",
    "\n",
    "# Generating Y values with noise\n",
    "X_full = []\n",
    "y_full = []\n",
    "for x_val in X:\n",
    "\ty_val = a * x_val**2 + b * x_val + c\n",
    "\ty.append(y_val)\n",
    "\tX_full += [x_val for i in range(10)]\n",
    "\ty_noisy = y_val + np.random.normal(0, noise_std, 10)\n",
    "\ty_full += list(y_noisy)\n",
    "\n",
    "X_full = np.array(X_full)\n",
    "y_full = np.array(y_full)\n",
    "# Creating a DataFrame\n",
    "dataset = pd.DataFrame({'X': X, 'y': y})\n",
    "\n",
    "X_homo = X.reshape((1, -1))\n",
    "X_homo = np.vstack((X_homo, np.ones(X_homo.shape)))\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "# print(X.shape)\n",
    "X_poly = poly.fit_transform(X_full.reshape(-1, 1))\n",
    "# print(X_poly.shape)\n",
    "# print(X_poly[1,:])\n",
    "# print(y.shape)\n",
    "model = BayesRegression(X_poly.T, y_full, noise=1000)\n",
    "model.set_prior(np.array([1, 3, 2]), 0.0001)\n",
    "post_mean, post_covar = model.fit()\n",
    "# print(post_mean)\n",
    "# print(post_covar)\n",
    "\n",
    "# Test inputs\n",
    "test_input = np.arange(-5, 6, 1).reshape(-1, 1)\n",
    "test_input_poly = poly.transform(test_input)\n",
    "# print(test_input_poly.shape)\n",
    "# test_input_homo = np.vstack((test_input, np.ones(test_input.shape)))\n",
    "pred_mean, pred_covar = model.predict(test_input_poly.T)\n",
    "# print(pred_mean)\n",
    "# print(pred_covar.shape)\n",
    "\n",
    "# print(X.shape, y.shape, y_noisy.shape)\n",
    "plt.scatter(X_full, y_full, marker='x')\n",
    "plt.plot(X, y, linestyle='--', color='black')\n",
    "plt.scatter(test_input, pred_mean, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.0\n",
      "[4.578795057936998, 3.292105233420657, 3.262836741047193, 3.7985606049691687, 3.3981460176119507, 3.338161116543612]\n",
      "comparing scale 0.075 against baseline\n",
      "comparing scale 0.1 against baseline\n",
      "comparing scale 0.15 against baseline\n",
      "comparing scale 0.2 against baseline\n",
      "comparing scale 0.4 against baseline\n",
      "comparing scale 0.7 against baseline\n",
      "comparing scale 1.0 against baseline\n",
      "Latency: 0.25\n",
      "[1.2997063804452305, 1.53205345475427, 1.0073567399361885, 1.704686736086562, 1.2248798110639876, 1.414175542004644]\n",
      "comparing scale 0.075 against baseline\n",
      "comparing scale 0.1 against baseline\n",
      "comparing scale 0.15 against baseline\n",
      "comparing scale 0.2 against baseline\n",
      "comparing scale 0.4 against baseline\n",
      "comparing scale 0.7 against baseline\n",
      "comparing scale 1.0 against baseline\n",
      "Latency: 0.5\n",
      "[0.4969321713882856, 0.5541611777244101, 0.5533482869685347, 1.1724079354384669, 0.789520767310761, 0.997162551124956]\n",
      "comparing scale 0.075 against baseline\n",
      "comparing scale 0.1 against baseline\n",
      "comparing scale 0.15 against baseline\n",
      "comparing scale 0.2 against baseline\n",
      "comparing scale 0.4 against baseline\n",
      "comparing scale 0.7 against baseline\n",
      "comparing scale 1.0 against baseline\n",
      "Latency: 0.75\n",
      "[0.3265103596114661, 0.3439607836427477, 0.7209181136153853, 0.4407856066418474, 0.3548929567130901]\n",
      "comparing scale 0.075 against baseline\n",
      "comparing scale 0.1 against baseline\n",
      "comparing scale 0.15 against baseline\n",
      "comparing scale 0.2 against baseline\n",
      "comparing scale 0.4 against baseline\n",
      "comparing scale 0.7 against baseline\n",
      "comparing scale 1.0 against baseline\n"
     ]
    }
   ],
   "source": [
    "### Stats test\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Prepare data\n",
    "users_to_remove = [\"user_lauren\", \"user_lizzie\", \"user_lizzie1\", \"user_sarah1\"]\n",
    "all_data_list = [all_datasets[user] for user in all_datasets.keys() if user not in users_to_remove]\n",
    "# print(len(all_data_list))\n",
    "combined_df = pd.concat(all_data_list)\n",
    "grouped_df = combined_df.groupby([\"latency\", \"scale\"])\n",
    "TP_group = grouped_df[\"throughput\"].apply(list)\n",
    "\n",
    "# Group by 'latency'\n",
    "grouped_by_latency = combined_df.groupby('latency')\n",
    "\n",
    "# Iterate over each latency group\n",
    "for latency, latency_group in grouped_by_latency:\n",
    "\tprint(f\"Latency: {latency}\")\n",
    "\n",
    "\t# Within each latency group, further group by 'scale'\n",
    "\tgrouped_by_scale = latency_group.groupby('scale')\n",
    "\tbaseline = list(grouped_by_scale.get_group(1.0)[\"throughput\"])\n",
    "\tprint(baseline)\n",
    "\t# Iterate over each scale group within the current latency group\n",
    "\tfor scale, scale_group in grouped_by_scale:\n",
    "\t\tprint(f\"comparing scale {scale} against baseline\")\n",
    "\t\t# print(f\"  Scale: {scale}, Values: {list(scale_group['throughput'])}\")\n",
    "\n",
    "# print(TP_group)\n",
    "# p_vals = {}\n",
    "# baseline = TP_group[(0.0, 1.0)]\n",
    "# test = TP_group[(0.0, 0.15)]\n",
    "# print(baseline)\n",
    "# print(test)\n",
    "# t_stat, p_val = ttest_rel(baseline, test)\n",
    "# print(p_val)\n",
    "# for params, values in TP_group.items():\n",
    "# \t# print(f\"Latency: {latency}, Scale: {scale}, Values: {len(list(throughput))}\")\n",
    "# \tif len(values) == 6\n",
    "\n",
    "# # Example data: before and after treatment\n",
    "# data_before = np.array([20, 21, 19, 22, 20, 23, 21])\n",
    "# data_after = np.array([22, 22, 20, 23, 21, 24, 22])\n",
    "\n",
    "# # Perform the paired t-test\n",
    "# t_statistic, p_value = ttest_rel(data_before, data_after)\n",
    "\n",
    "# print(\"T-statistic:\", t_statistic)\n",
    "# print(\"P-value:\", p_value)\n",
    "\n",
    "# # Interpret the p-value\n",
    "# alpha = 0.05  # commonly used threshold for significance\n",
    "# if p_value < alpha:\n",
    "#     print(\"There is a significant difference.\")\n",
    "# else:\n",
    "#     print(\"There is no significant difference.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
