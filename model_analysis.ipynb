{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from utils import stratified_sample, annotate, even_train_split\n",
    "import glob  # Importing the glob module to find all the files matching a pattern\n",
    "\n",
    "# Pattern to match the data files\n",
    "file_pattern = \"data_files/user_*/metric_df.csv\"\n",
    "\n",
    "# Initialize a dictionary to store Dataframes for each dataset\n",
    "all_datasets = {}\n",
    "\n",
    "# Loop through each file that matches the file pattern\n",
    "for filepath in glob.glob(file_pattern):\n",
    "    # print(filepath)\n",
    "    # print(filepath.split('/'))\n",
    "    # user_name = filepath.split('/')[1]\n",
    "    user_name = filepath.split('\\\\')[1]\n",
    "    print(f\"Processing {filepath} dataset...\")\n",
    "\n",
    "    # Read in data file as a pandas dataframe\n",
    "    data = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    # add weighted performance metric\n",
    "    w = 1\n",
    "    data[\"weighted_performance\"] = 10*data['throughput'] - w*(data['avg_osd'] + data['avg_target_error'])\n",
    "\n",
    "    all_datasets[user_name] = data\n",
    "\n",
    "# Combine datasets for Lizzie\n",
    "lizzie1 = all_datasets[\"user_lizzie1\"]\n",
    "lizzie2 = all_datasets[\"user_lizzie2\"]\n",
    "combined_df = pd.concat([lizzie1, lizzie2])\n",
    "all_datasets[\"user_lizzie\"] = combined_df.groupby(['latency', 'scale']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "throughput\n",
      "\tuser_jason\n",
      "\tuser_lauren\n",
      "\tuser_lizzie1\n",
      "\tuser_lizzie2\n",
      "\tuser_sarah1\n",
      "\tuser_shreya\n",
      "\tuser_sujaan\n",
      "\tuser_xiao\n",
      "\tuser_yutong\n",
      "\tuser_lizzie\n",
      "avg_target_error\n",
      "\tuser_jason\n",
      "\tuser_lauren\n",
      "\tuser_lizzie1\n",
      "\tuser_lizzie2\n",
      "\tuser_sarah1\n",
      "\tuser_shreya\n",
      "\tuser_sujaan\n",
      "\tuser_xiao\n",
      "\tuser_yutong\n",
      "\tuser_lizzie\n",
      "avg_osd\n",
      "\tuser_jason\n",
      "\tuser_lauren\n",
      "\tuser_lizzie1\n",
      "\tuser_lizzie2\n",
      "\tuser_sarah1\n",
      "\tuser_shreya\n",
      "\tuser_sujaan\n",
      "\tuser_xiao\n",
      "\tuser_yutong\n",
      "\tuser_lizzie\n",
      "avg_movement_speed\n",
      "\tuser_jason\n",
      "\tuser_lauren\n",
      "\tuser_lizzie1\n",
      "\tuser_lizzie2\n",
      "\tuser_sarah1\n",
      "\tuser_shreya\n",
      "\tuser_sujaan\n",
      "\tuser_xiao\n",
      "\tuser_yutong\n",
      "\tuser_lizzie\n",
      "weighted_performance\n",
      "\tuser_jason\n",
      "\tuser_lauren\n",
      "\tuser_lizzie1\n",
      "\tuser_lizzie2\n",
      "\tuser_sarah1\n",
      "\tuser_shreya\n",
      "\tuser_sujaan\n",
      "\tuser_xiao\n",
      "\tuser_yutong\n",
      "\tuser_lizzie\n"
     ]
    }
   ],
   "source": [
    "from models import PolyRegression, GPRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, RationalQuadratic, WhiteKernel\n",
    "import utils\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename='warnings_log.txt', level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "# Function to redirect warnings to logging\n",
    "def warn_to_logging(message, category, filename, lineno, file=None, line=None):\n",
    "    logging.warning(f'{filename}:{lineno}: {category.__name__}: {message}')\n",
    "\n",
    "# Redirect all warnings to the warn_to_logging function\n",
    "warnings.showwarning = warn_to_logging\n",
    "\n",
    "all_results = {}\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t\n",
    "\tprint(output_metric)\n",
    "\tuser_results = {}\n",
    "\tfor user, data in list(all_datasets.items()): \n",
    "\t\t# if user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t# \tcontinue\n",
    "\t\tprint(f\"\\t{user}\")\n",
    "\n",
    "\t\t# Prepare data \n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tY = data[output_metric]\n",
    "\n",
    "\t\t# Initialize evaluation metrics\n",
    "\t\toptimal_match_rate = []\n",
    "\t\toptimal_scale_error = []\n",
    "\t\tmse_scores = []\n",
    "\t\tfull_mse_scores = []\n",
    "\t\tn_train_mse = []\n",
    "\t\tn_train_full_mse = []\n",
    "\t\tn_train_p = []\n",
    "\n",
    "\t\tn = len(data)\n",
    "\t\tn_train_values = range(2, n-1)\n",
    "\t\tfor n_train in n_train_values:\n",
    "\n",
    "\t\t\tn_train_p.append(n_train / n)\n",
    "\t\t\t# Split into training/test sets\n",
    "\t\t\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\t\t\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\t\t\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\t\t\tY_train, Y_test = train_set[output_metric], test_set[output_metric]\n",
    "\t\t\t\n",
    "\t\t\t# Create dense test input\n",
    "\t\t\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t\t\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\t\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\t\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\t\t\n",
    "\t\t\t# # Polynomial Regression\n",
    "\t\t\tif model_type.startswith(\"Poly\"):\n",
    "\t\t\t\tdegree = int(model_type.strip(\"Poly\"))\n",
    "\t\t\t\tY_pred, model_params = PolyRegression(X_train.values, Y_train.values, X.values, degree)\n",
    "\t\t\t\tY_pred_dense, _ = PolyRegression(X_train.values, Y_train.values, X_dense, degree)\n",
    "\n",
    "\t\t\t# Gaussian Process Regression\n",
    "\t\t\telif model_type.startswith(\"GPR\"):\n",
    "\t\t\t\t# Choose kernel\n",
    "\t\t\t\tkernel_type = model_type.removeprefix(\"GPR_\")\n",
    "\t\t\t\t# print(kernel_type)\n",
    "\t\t\t\tif kernel_type == \"RBF_Noise_default\":\n",
    "\t\t\t\t\tConstantKernel() * RBF() + WhiteKernel() # Default RBF with likelihood noise\n",
    "\t\t\t\telif kernel_type == \"RBF_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0])\n",
    "\t\t\t\telif kernel_type == \"RBF_Noise_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0]) + WhiteKernel() # RBF with anistropic length scale\n",
    "\t\t\t\telif kernel_type == \"RQ_Noise_default\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RationalQuadratic() + WhiteKernel() # Default Rational Quadratic with likelihood noise\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(\"Invalid kernel specification!\")\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\tY_pred, Y_pred_std, model_params = GPRegression(X_train.values, Y_train.values, X.values, kernel)\n",
    "\t\t\t\tY_pred_dense, Y_pred_std, _ = GPRegression(X_train.values, Y_train.values, X_dense, kernel)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Invalid model type specification!\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t## Evaluate metrics\n",
    "\t\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "\t\t\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t\t# Mean Square Error on whole dataset\n",
    "\t\t\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\t\t\tif True: #full_mse < 5000:\n",
    "\t\t\t\tn_train_full_mse.append(n_train)\n",
    "\t\t\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t\t\t# Mean Square Error on test set\n",
    "\t\t\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\t\t\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\t\t\tif True: #mse < 5000:\n",
    "\t\t\t\tn_train_mse.append(n_train)\n",
    "\t\t\t\tmse_scores.append(mse)\n",
    "\t\t\t\n",
    "\t\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\t\t\telse: # optimal scale at minimum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t\t# Merge the results on 'latency'\n",
    "\t\t\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\t\t\n",
    "\t\t\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t\t\t# print(optimal_scale_dense)\n",
    "\t\t\t# print(merged_ref_dense)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# Count the number of matches\n",
    "\t\t\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\t\t\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\t\t\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\t\t\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t\t\t# Visualize model prediction\n",
    "\t\t\tif n_train == n-2:\n",
    "\t\t\t\tutils.model_heatmaps(data, dense_df, X_train, user, output_metric, model_type, model_params)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Store results from this dataset\n",
    "\t\t\tuser_results[user] = {\n",
    "\t\t\t\t'n_train_mse': list(n_train_mse),\n",
    "\t\t\t\t'n_train_full_mse': list(n_train_full_mse),\n",
    "\t\t\t\t'full_mse_scores': full_mse_scores,\n",
    "\t\t\t\t'mse_scores': mse_scores,\n",
    "\t\t\t\t'n_train_all': list(n_train_values),\n",
    "\t\t\t\t'match_rate': optimal_match_rate,\n",
    "\t\t\t\t'scale_error': optimal_scale_error,\n",
    "\t\t\t\t'n_train_p': n_train_p\n",
    "\t\t\t}\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\t\n",
    "\telse:\n",
    "\t\tall_results[output_metric] = user_results\n",
    "\t\tcontinue\n",
    "\tbreak\n",
    "\n",
    "with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "\tjson.dump(all_results, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the results for all datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_defualt\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "\tall_results = json.load(file)\n",
    "\n",
    "for output_metric, user_results in all_results.items():\n",
    "\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\tfig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}\")\n",
    "\tfor user, results in user_results.items():\n",
    "\t\taxes[0, 0].plot(results['n_train_p'], results['match_rate'], marker='o', label=user)\n",
    "\t\taxes[0, 1].plot(results['n_train_p'], results['scale_error'], marker='o', label=user)\n",
    "\t\taxes[1, 0].plot(results['n_train_p'], results['full_mse_scores'], marker='o', label=user)\n",
    "\t\taxes[1, 1].plot(results['n_train_p'], results['mse_scores'], marker='o', label=user)\n",
    "\n",
    "\taxes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\taxes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\taxes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\taxes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\taxes[1, 0].set_title('MSE on whole dataset')\n",
    "\taxes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\taxes[1, 1].set_title('MSE on test set')\n",
    "\taxes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\taxes[1, 0].legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/model_results/{output_metric}/{model_type}_model_eval_metrics_{output_metric}.png\", facecolor='w')\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot averaged results\n",
    "\n",
    "for output_metric, user_results in all_results.items():\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\tfig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}, mean over users\")\n",
    "\n",
    "\t# Prepare lists of lists to store data\n",
    "\tmatch_rate_lists = []\n",
    "\tscale_error_lists = []\n",
    "\tfull_mse_score_lists = []\n",
    "\tmse_score_lists = []\n",
    "\tn_train_lists = []\n",
    "\n",
    "\t# Collect data for each metric\n",
    "\tfor user, results in user_results.items():\n",
    "\t\tmatch_rate_lists.append(results['match_rate'])\n",
    "\t\tscale_error_lists.append(results['scale_error'])\n",
    "\t\tfull_mse_score_lists.append(results['full_mse_scores'])\n",
    "\t\tmse_score_lists.append(results['mse_scores'])\n",
    "\t\tn_train_lists.append(results['n_train_all'])\n",
    "\n",
    "\t# Function to calculate average and standard deviation safely\n",
    "\tdef safe_mean_std(data_lists, index):\n",
    "\t\tvalid_data = [data[index] for data in data_lists if index < len(data)]\n",
    "\t\treturn np.mean(valid_data), np.std(valid_data)\n",
    "\n",
    "\t# Calculate averages and standard deviations safely\n",
    "\tavg_match_rate = []\n",
    "\tstd_match_rate = []\n",
    "\tavg_scale_error = []\n",
    "\tstd_scale_error = []\n",
    "\tavg_full_mse_scores = []\n",
    "\tstd_full_mse_scores = []\n",
    "\tavg_mse_scores = []\n",
    "\tstd_mse_scores = []\n",
    "\n",
    "\tmin_n_train_length = min([len(data_list) for data_list in n_train_lists])\n",
    "\tmin_n_train_list = n_train_lists[0][:min_n_train_length]\n",
    "\tfor n in range(min_n_train_length):\n",
    "\t\tmean, std = safe_mean_std(match_rate_lists, n)\n",
    "\t\tavg_match_rate.append(mean)\n",
    "\t\tstd_match_rate.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(scale_error_lists, n)\n",
    "\t\tavg_scale_error.append(mean)\n",
    "\t\tstd_scale_error.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(full_mse_score_lists, n)\n",
    "\t\tavg_full_mse_scores.append(mean)\n",
    "\t\tstd_full_mse_scores.append(std)\n",
    "\n",
    "\t\tmean, std = safe_mean_std(mse_score_lists, n)\n",
    "\t\tavg_mse_scores.append(mean)\n",
    "\t\tstd_mse_scores.append(std)\n",
    "\n",
    "\t# Plotting the average values and standard deviation\n",
    "\t# Plotting the average values and standard deviation\n",
    "\taxes[0, 0].plot(min_n_train_list, avg_match_rate, marker='o', label='Mean over users')\n",
    "\taxes[0, 0].fill_between(min_n_train_list, np.subtract(avg_match_rate, std_match_rate), \n",
    "\t\t\t\t\t\t\tnp.add(avg_match_rate, std_match_rate), alpha=0.2)\n",
    "\n",
    "\taxes[0, 1].plot(min_n_train_list, avg_scale_error, marker='o', label='Mean over users')\n",
    "\taxes[0, 1].fill_between(min_n_train_list, np.subtract(avg_scale_error, std_scale_error), \n",
    "\t\t\t\t\t\t\tnp.add(avg_scale_error, std_scale_error), alpha=0.2)\n",
    "\n",
    "\taxes[1, 0].plot(min_n_train_list, avg_full_mse_scores, marker='o', label='Mean over users')\n",
    "\taxes[1, 0].fill_between(min_n_train_list, np.subtract(avg_full_mse_scores, std_full_mse_scores), \n",
    "\t\t\t\t\t\t\tnp.add(avg_full_mse_scores, std_full_mse_scores), alpha=0.2)\n",
    "\n",
    "\taxes[1, 1].plot(min_n_train_list, avg_mse_scores, marker='o', label='Mean Over Users')\n",
    "\taxes[1, 1].fill_between(min_n_train_list, np.subtract(avg_mse_scores, std_mse_scores), \n",
    "\t\t\t\t\t\t\tnp.add(avg_mse_scores, std_mse_scores), alpha=0.2)\n",
    "\t\n",
    "\taxes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\taxes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\taxes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\taxes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\taxes[1, 0].set_title('MSE on whole dataset')\n",
    "\taxes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\taxes[1, 1].set_title('MSE on test set')\n",
    "\taxes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\t# axes[1, 0].legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f\"figures/model_results/{output_metric}/{model_type}_model_eval_metrics_{output_metric}_average.png\", facecolor='w')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tfor user, data in list(all_datasets.items()):\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\tmodel_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\tkernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\tY_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\tplt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\")\n",
    "\t\tplt.ylabel(\"scaling factor\")\n",
    "\t\tplt.plot(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='o', label=user)\n",
    "\tplt.legend()\n",
    "\tplt.savefig(f\"figures/optimal_scale_per_latency/{output_metric}.png\")\n",
    "\tplt.show()\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale per latency with model prediction\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tfor user, data in list(all_datasets.items())[:1]:\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\tmodel_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\tkernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\tY_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\tplt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\")\n",
    "\t\tplt.ylabel(\"scaling factor\")\n",
    "\t\tplt.scatter(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='x', label=\"measured\")\n",
    "\t\tplt.plot(optimal_scale_dense['latency'], optimal_scale_dense['scale'], label=\"predicted\")\n",
    "\tplt.legend()\n",
    "\t# plt.savefig(f\"figures/optimal_scale_per_latency/{output_metric}.png\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot key metric heatmaps\n",
    "from utils import annotate_extrema\n",
    "\n",
    "metric_df = all_datasets[\"user_lizzie\"]\n",
    "# Create a 2x5 subplot for the heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "# Define a function to highlight the maximum value in each row\n",
    "def highlight_max(data):\n",
    "    max_index = np.argmax(data, axis=1)\n",
    "    for i, max_col in enumerate(max_index):\n",
    "        data[i, max_col] = data[i, max_col]\n",
    "    return data\n",
    "\n",
    "# Plot the heatmap for throughput\n",
    "heatmap_throughput = metric_df.pivot(\n",
    "    index='latency', columns='scale', values='throughput')\n",
    "ax = sns.heatmap(heatmap_throughput, ax=axes[0], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[0].set_title('Throughput vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_throughput.values, ax)\n",
    "\n",
    "# Plot heatmap for total error (target deviation + osd)\n",
    "metric_df['total_error'] = metric_df['avg_osd'] + metric_df['avg_target_error']\n",
    "heatmap_error = metric_df.pivot(\n",
    "    index='latency', columns='scale', values='total_error')\n",
    "ax = sns.heatmap(heatmap_error, ax=axes[1], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[1].set_title('Total Error vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_error.values, ax, extrema_type='min')\n",
    "\n",
    "# Plot heatmap for combined performance (movement speed - total error)\n",
    "heatmap_combo = metric_df.pivot(\n",
    "    index='latency', columns='scale', values='weighted_performance')\n",
    "ax = sns.heatmap(heatmap_combo, ax=axes[2], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[2].set_title('Combined Performance vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_combo.values, ax, extrema_type='max')\n",
    "\n",
    "# plt.title(\"User A\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{data_folder}/heatmap_key_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function, e.g., a sine wave function\n",
    "def smooth_2d_function(x, y):\n",
    "    return np.sin(np.sqrt(x**2 + y**2))\n",
    "\n",
    "# Generate sample points\n",
    "x = np.linspace(-5, 5, 10)\n",
    "y = np.linspace(-5, 5, 10)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Apply the function to the sample points\n",
    "z = smooth_2d_function(x, y)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'x': x.ravel(), 'y': y.ravel(), 'z': z.ravel()})\n",
    "\n",
    "X = df[['x', 'y']]\n",
    "Y = df['z']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
    "\n",
    "z_pred, _ = GPRegression(X_train, Y_train, X)\n",
    "z_pred = z_pred.reshape(x.shape)\n",
    "\n",
    "# df[\"y_pred\"] = Y_pred\n",
    "\n",
    "# Plotting the function for visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "ax = axes[0].contourf(x, y, z, cmap='viridis')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Smooth 2D Function')\n",
    "\n",
    "ax = axes[1].contourf(x, y, z_pred, cmap='viridis')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Predictions')\n",
    "\n",
    "fig.colorbar(ax, label='Function Value')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
