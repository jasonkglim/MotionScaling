{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from utils import stratified_sample, annotate, even_train_split\n",
    "import glob  # Importing the glob module to find all the files matching a pattern\n",
    "from scipy import stats\n",
    "\n",
    "# Pattern to match the data files\n",
    "file_pattern = \"../data_files/user_*/metric_df.csv\"\n",
    "\n",
    "# Initialize a dictionary to store one_user_one_user_dataframes for each dataset\n",
    "all_datasets = {}\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"total_error\", \"weighted_performance\"]\n",
    "\n",
    "# Loop through each file that matches the file pattern\n",
    "for filepath in glob.glob(file_pattern):\n",
    "    # print(filepath)\n",
    "    # print(filepath.split('/'))\n",
    "    # user_name = filepath.split('/')[1]\n",
    "    user_name = filepath.split('\\\\')[1]\n",
    "    if user_name == \"user_test\": continue\n",
    "    print(f\"Processing {user_name} dataset...\")\n",
    "\n",
    "    # Read in data file as a pandas dataframe\n",
    "    data = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "    # add weighted performance metric\n",
    "    w = 1\n",
    "    data[\"total_error\"] = data['avg_osd'] + data['avg_target_error']\n",
    "    data[\"weighted_performance\"] = 10*data['throughput'] - w*data[\"total_error\"]\n",
    "\n",
    "    # Standardize metrics\n",
    "    for m in output_metrics:\n",
    "        data[f\"{m}_standard\"] = stats.zscore(data[m])\n",
    "\n",
    "    all_datasets[user_name] = data\n",
    "\n",
    "# Combine datasets for Lizzie\n",
    "lizzie1 = all_datasets[\"user_lizzie1\"]\n",
    "lizzie2 = all_datasets[\"user_lizzie2\"]\n",
    "combined_df = pd.concat([lizzie1, lizzie2])\n",
    "all_datasets[\"user_lizzie\"] = combined_df.groupby(['latency', 'scale']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import PolyRegression, GPRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, RationalQuadratic, WhiteKernel\n",
    "import utils\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_type = \"blah\"\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename='warnings_log.txt', level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "# Function to redirect warnings to logging\n",
    "def warn_to_logging(message, category, filename, lineno, file=None, line=None):\n",
    "    logging.warning(f'{filename}:{lineno}: {category.__name__}: {message}')\n",
    "\n",
    "# Redirect all warnings to the warn_to_logging function\n",
    "warnings.showwarning = warn_to_logging\n",
    "\n",
    "all_results = {}\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"total_error\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t\n",
    "\tprint(output_metric)\n",
    "\tuser_results = {}\n",
    "\tfor user, data in list(all_datasets.items()): \n",
    "\t\t# if user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t# \tcontinue\n",
    "\t\tprint(f\"\\t{user}\")\n",
    "\n",
    "\t\t# Prepare data \n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tY = data[output_metric]\n",
    "\n",
    "\t\t# Initialize evaluation metrics\n",
    "\t\toptimal_match_rate = []\n",
    "\t\toptimal_scale_error = []\n",
    "\t\tmse_scores = []\n",
    "\t\tfull_mse_scores = []\n",
    "\t\tn_train_mse = []\n",
    "\t\tn_train_full_mse = []\n",
    "\t\tn_train_p = []\n",
    "\n",
    "\t\tn = len(data)\n",
    "\t\tn_train_values = range(2, n-1)\n",
    "\t\tfor n_train in n_train_values:\n",
    "\n",
    "\t\t\tn_train_p.append(n_train / n)\n",
    "\t\t\t# Split into training/test sets\n",
    "\t\t\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\t\t\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\t\t\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\t\t\tY_train, Y_test = train_set[output_metric], test_set[output_metric]\n",
    "\t\t\t\n",
    "\t\t\t# Create dense test input\n",
    "\t\t\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t\t\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\t\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\t\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\t\t\n",
    "\t\t\t# # Polynomial Regression\n",
    "\t\t\tif model_type.startswith(\"Poly\"):\n",
    "\t\t\t\tdegree = int(model_type.strip(\"Poly\"))\n",
    "\t\t\t\tY_pred, model_params = PolyRegression(X_train.values, Y_train.values, X.values, degree)\n",
    "\t\t\t\tY_pred_dense, _ = PolyRegression(X_train.values, Y_train.values, X_dense, degree)\n",
    "\n",
    "\t\t\t# Gaussian Process Regression\n",
    "\t\t\telif model_type.startswith(\"GPR\"):\n",
    "\t\t\t\t# Choose kernel\n",
    "\t\t\t\tkernel_type = model_type.removeprefix(\"GPR_\")\n",
    "\t\t\t\t# print(kernel_type)\n",
    "\t\t\t\tif kernel_type == \"RBF_Noise_default\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF() + WhiteKernel() # Default RBF with likelihood noise\n",
    "\t\t\t\telif kernel_type == \"RBF_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0])\n",
    "\t\t\t\telif kernel_type == \"RBF_Noise_anisotropic\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RBF([1.0, 1.0]) + WhiteKernel() # RBF with anistropic length scale\n",
    "\t\t\t\telif kernel_type == \"RQ_Noise_default\":\n",
    "\t\t\t\t\tkernel = ConstantKernel() * RationalQuadratic() + WhiteKernel() # Default Rational Quadratic with likelihood noise\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(\"Invalid kernel specification!\")\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\tY_pred, Y_pred_std, model_params = GPRegression(X_train.values, Y_train.values, X.values, kernel)\n",
    "\t\t\t\tY_pred_dense, Y_pred_std, _ = GPRegression(X_train.values, Y_train.values, X_dense, kernel)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Invalid model type specification!\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t## Evaluate metrics\n",
    "\t\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "\t\t\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t\t# Mean Square Error on whole dataset\n",
    "\t\t\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\t\t\tif True: #full_mse < 5000:\n",
    "\t\t\t\tn_train_full_mse.append(n_train)\n",
    "\t\t\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t\t\t# Mean Square Error on test set\n",
    "\t\t\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\t\t\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\t\t\tif True: #mse < 5000:\n",
    "\t\t\t\tn_train_mse.append(n_train)\n",
    "\t\t\t\tmse_scores.append(mse)\n",
    "\t\t\t\n",
    "\t\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\t\t\telse: # optimal scale at minimum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t\t# Merge the results on 'latency'\n",
    "\t\t\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\t\t\n",
    "\t\t\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t\t\t# print(optimal_scale_dense)\n",
    "\t\t\t# print(merged_ref_dense)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# Count the number of matches\n",
    "\t\t\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\t\t\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\t\t\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\t\t\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t\t\t# Visualize model prediction\n",
    "\t\t\tif n_train == n-2:\n",
    "\t\t\t\tutils.model_heatmaps(data, dense_df, X_train, user, output_metric, model_type, model_params)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Store results from this dataset\n",
    "\t\t\tuser_results[user] = {\n",
    "\t\t\t\t'n_train_mse': list(n_train_mse),\n",
    "\t\t\t\t'n_train_full_mse': list(n_train_full_mse),\n",
    "\t\t\t\t'full_mse_scores': full_mse_scores,\n",
    "\t\t\t\t'mse_scores': mse_scores,\n",
    "\t\t\t\t'n_train_all': list(n_train_values),\n",
    "\t\t\t\t'match_rate': optimal_match_rate,\n",
    "\t\t\t\t'scale_error': optimal_scale_error,\n",
    "\t\t\t\t'n_train_p': n_train_p\n",
    "\t\t\t}\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\t\n",
    "\telse:\n",
    "\t\tall_results[output_metric] = user_results\n",
    "\t\tcontinue\n",
    "\tbreak\n",
    "\n",
    "# print(all_results.keys())\n",
    "with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "\tjson.dump(all_results, file)\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "# \tog_results = json.load(file)\n",
    "\n",
    "# og_results[\"total_error\"] = all_results[\"total_error\"]\n",
    "\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(og_results, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the results for all datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load data\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "model_type = \"BRNIG_InformPrior_standardized\"\n",
    "\n",
    "with open(f\"../model_result_data/{model_type}.pkl\", \"rb\") as file:\n",
    "\tall_results = pickle.load(file)\n",
    "\n",
    "for output_metric, user_results in all_results.items():\n",
    "\n",
    "\tfig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\tfig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}\")\n",
    "\tfor user, results in user_results.items():\n",
    "\t\tif user in [\"user_lizzie1\", \"user_lizzie2\"]:\n",
    "\t\t\tcontinue\n",
    "\t\taxes[0, 0].plot(results['n_train_p'], results['match_rate'], marker='o', label=user)\n",
    "\t\taxes[0, 1].plot(results['n_train_p'], results['scale_error'], marker='o', label=user)\n",
    "\t\taxes[1, 0].plot(results['n_train_p'], results['full_mse_scores'], marker='o', label=user)\n",
    "\t\taxes[1, 1].plot(results['n_train_p'], results['mse_scores'], marker='o', label=user)\n",
    "\n",
    "\taxes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\taxes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\taxes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\taxes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\taxes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\taxes[1, 0].set_title('MSE on whole dataset')\n",
    "\taxes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\taxes[1, 1].set_title('MSE on test set')\n",
    "\taxes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\taxes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\taxes[1, 0].legend()\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tos.makedirs(f\"../figures/model_results/{output_metric}\", exist_ok=True)\n",
    "\tplt.savefig(f\"../figures/model_results/{output_metric}/{model_type}.png\", facecolor='w')\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot averaged results\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load data\n",
    "## Choose models to apply, final plot compares across models\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# model_types = [\"BRNIG_InformPrior_standardized\", \"BRNIG_InformPrior_gammaEst_standardized\", \"BR_CustomPrior_ObsNoise_1_PriorVar_1_standardized\"]\n",
    "model_types = [\"BRNIG_Poly1_NoninformPrior_standardized\",\n",
    "\t\t\t   \"BRNIG_NoninformPrior_standardized\",\n",
    "\t\t\t   \"BRNIG_Poly3_NoninformPrior_standardized\"] #,\n",
    "\t\t\t#    \"BRNIG_Poly4_NoninformPrior_standardized\",]\n",
    "model_names = [\"Noninformed Prior Degree 1\", \"Noninformed Prior Degree 2\", \"Noninformed Prior Degree 3\"] #, \"Noninformed Prior Degree 4\"] # For legend\n",
    "\n",
    "# all_results = {}\n",
    "wanted_metrics = [\"throughput_standard\", \"total_error_standard\", \"weighted_performance_standard\", \"avg_movement_speed_standard\"]\n",
    "# wanted_metrics = [\"throughput\", \"total_error\", \"weighted_performance\", \"avg_movement_speed\"]\n",
    "for output_metric in wanted_metrics:\n",
    "\n",
    "\t# Loop needs to save metric avgs (across users) and std for each model\n",
    "\tall_models_avg_match_rate = []\n",
    "\tall_models_std_match_rate = []\n",
    "\tall_models_avg_scale_error = []\n",
    "\tall_models_std_scale_error = []\n",
    "\tall_models_avg_full_mse_scores = []\n",
    "\tall_models_std_full_mse_scores = []\n",
    "\tall_models_avg_mse_scores = {}\n",
    "\tall_models_std_mse_scores = {}\n",
    "\tall_models_min_n_train_list = {} # Accounts for different n_train values for different models/users\n",
    "\n",
    "\t# Set up figure (do this here to generate plot comparing models, different plot for each metric)\n",
    "\tplt.figure(figsize=(6, 4), facecolor=\"white\")\n",
    "\n",
    "\tfor model_type, model_name in zip(model_types, model_names):\n",
    "\t\t# if model_name.startswith(\"NIG\"):\n",
    "\t\twith open(f\"../model_result_data/{model_type}.pkl\", \"rb\") as file:\n",
    "\t\t\tall_results = pickle.load(file)\n",
    "\t\t# else:\n",
    "\t\t# \twith open(f\"../model_result_data/{model_type}.json\", \"rb\") as file:\n",
    "\t\t# \t\tall_results = json.load(file)\n",
    "\t\t\t\t\n",
    "\t\tuser_results = all_results[output_metric]\n",
    "\t# for output_metric, user_results in all_results.items():\n",
    "\t# \tif output_metric not in wanted_metrics: break\n",
    "\t\t\n",
    "\t\t\n",
    "\n",
    "\t\t# print(all_results)\n",
    "\t\n",
    "\n",
    "\t\t# fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "\t\t# fig.suptitle(f\"Model Evaluation Metrics for predicting {output_metric}, mean over users\")\n",
    "\t\t# fig.suptitle(f\"Model Evaluation Metrics for {model_type} predicting {output_metric}, mean over users\")\n",
    "\n",
    "\t\t# Prepare lists of lists to store data\n",
    "\t\tmatch_rate_lists = []\n",
    "\t\tscale_error_lists = []\n",
    "\t\tfull_mse_score_lists = []\n",
    "\t\tmse_score_lists = []\n",
    "\t\tn_train_lists = []\n",
    "\n",
    "\t\t# Collect data for each metric\n",
    "\t\tfor user, results in user_results.items():\n",
    "\t\t\tif user in [\"user_test\", \"user_lizzie1\", \"user_lizzie2\"]: continue\n",
    "\t\t\tprint(user)\n",
    "\t\t\tmatch_rate_lists.append(results['match_rate'])\n",
    "\t\t\tscale_error_lists.append(results['scale_error'])\n",
    "\t\t\tfull_mse_score_lists.append(results['full_mse_scores'])\n",
    "\t\t\tmse_score_lists.append(results['mse_scores'])\n",
    "\t\t\tn_train_lists.append(results['n_train_all'])\n",
    "\t\t\t# if len(results[\"n_train_all\"]) == 0:\n",
    "\t\t\t# \tprint(user, \" Something is wrong\")\n",
    "\n",
    "\t\t# Function to calculate average and standard deviation safely\n",
    "\t\tdef safe_mean_std(data_lists, index):\n",
    "\t\t\tvalid_data = [data[index] for data in data_lists if index < len(data)]\n",
    "\t\t\treturn np.mean(valid_data), np.std(valid_data)\n",
    "\n",
    "\t\tavg_match_rate = []\n",
    "\t\tstd_match_rate = []\n",
    "\t\tavg_scale_error = []\n",
    "\t\tstd_scale_error = []\n",
    "\t\tavg_full_mse_scores = []\n",
    "\t\tstd_full_mse_scores = []\n",
    "\t\tavg_mse_scores = []\n",
    "\t\tstd_mse_scores = []\n",
    "\n",
    "\t\tmin_n_train_length = min([len(data_list) for data_list in n_train_lists])\n",
    "\t\tmin_n_train_list = n_train_lists[0][:min_n_train_length]\n",
    "\t\tall_models_min_n_train_list[model_type] = min_n_train_list\n",
    "\t\t# Calculate avg and std for metrics for each n_train in min_n_train_length\n",
    "\t\tfor n in range(min_n_train_length):\n",
    "\t\t\tmean, std = safe_mean_std(match_rate_lists, n)\n",
    "\t\t\tavg_match_rate.append(mean)\n",
    "\t\t\tstd_match_rate.append(std)\n",
    "\n",
    "\t\t\tmean, std = safe_mean_std(scale_error_lists, n)\n",
    "\t\t\tavg_scale_error.append(mean)\n",
    "\t\t\tstd_scale_error.append(std)\n",
    "\n",
    "\t\t\tmean, std = safe_mean_std(full_mse_score_lists, n)\n",
    "\t\t\tavg_full_mse_scores.append(mean)\n",
    "\t\t\tstd_full_mse_scores.append(std)\n",
    "\n",
    "\t\t\tmean, std = safe_mean_std(mse_score_lists, n)\n",
    "\t\t\tavg_mse_scores.append(mean)\n",
    "\t\t\tstd_mse_scores.append(std)\n",
    "\n",
    "\n",
    "\t\t# append vectors for current model results \n",
    "\t\tall_models_avg_mse_scores[model_type] = avg_mse_scores\n",
    "\t\tall_models_std_mse_scores[model_type] = std_mse_scores\n",
    "\n",
    "\n",
    "\n",
    "\t# # Plotting the average values and standard deviation\n",
    "\t# # Plotting the average values and standard deviation\n",
    "\t# axes[0, 0].plot(min_n_train_list, avg_match_rate, marker='o', label='Mean over users')\n",
    "\t# axes[0, 0].fill_between(min_n_train_list, np.subtract(avg_match_rate, std_match_rate), \n",
    "\t# \t\t\t\t\t\tnp.add(avg_match_rate, std_match_rate), alpha=0.2)\n",
    "\n",
    "\t# axes[0, 1].plot(min_n_train_list, avg_scale_error, marker='o', label='Mean over users')\n",
    "\t# axes[0, 1].fill_between(min_n_train_list, np.subtract(avg_scale_error, std_scale_error), \n",
    "\t# \t\t\t\t\t\tnp.add(avg_scale_error, std_scale_error), alpha=0.2)\n",
    "\n",
    "\t# axes[1, 0].plot(min_n_train_list, avg_full_mse_scores, marker='o', label='Mean over users')\n",
    "\t# axes[1, 0].fill_between(min_n_train_list, np.subtract(avg_full_mse_scores, std_full_mse_scores), \n",
    "\t# \t\t\t\t\t\tnp.add(avg_full_mse_scores, std_full_mse_scores), alpha=0.2)\n",
    "\n",
    "\t# axes[1, 1].plot(min_n_train_list, avg_mse_scores, marker='o', label='Mean Over Users')\n",
    "\t# axes[1, 1].fill_between(min_n_train_list, np.subtract(avg_mse_scores, std_mse_scores), \n",
    "\t# \t\t\t\t\t\tnp.add(avg_mse_scores, std_mse_scores), alpha=0.2)\n",
    "\t\n",
    "\t# axes[0, 0].set_title(\"Optimal Scale Prediction Rate\")\n",
    "\t# axes[0, 0].set_xlabel(\"Training Set Proportion\")\n",
    "\t# axes[0, 0].set_ylabel(\"Percentage of Correct Predictions\")\n",
    "\n",
    "\t# axes[0, 1].set_title(\"Optimal Scale Prediction Error Using Dense Prediction\")\n",
    "\t# axes[0, 1].set_xlabel(\"Training Set Proportion\")\n",
    "\t# axes[0, 1].set_ylabel(\"Avg Error\")\n",
    "\n",
    "\n",
    "\t# axes[1, 0].set_title('MSE on whole dataset')\n",
    "\t# axes[1, 0].set_xlabel('Training Set Proportion')\n",
    "\t# axes[1, 0].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\n",
    "\t# axes[1, 1].set_title('MSE on test set')\n",
    "\t# axes[1, 1].set_xlabel('Training Set Proportion')\n",
    "\t# axes[1, 1].set_ylabel('Model Accuracy (MSE Score)')\n",
    "\t# # axes[1, 0].legend()\n",
    "\n",
    "\n",
    "\t\tplt.plot(min_n_train_list, avg_mse_scores, marker='o', label=model_name)\n",
    "\t\tplt.fill_between(min_n_train_list, np.subtract(avg_mse_scores, std_mse_scores), \n",
    "\t\t\t\t\t\t\t\tnp.add(avg_mse_scores, std_mse_scores), alpha=0.2)\n",
    "\t\t\n",
    "# plt.plot(min_n_train_list2, avg_mse_scores2, marker='o', label='Uninformed Prior')\n",
    "# plt.fill_between(min_n_train_list1, np.subtract(avg_mse_scores2, std_mse_scores2), \n",
    "# \t\t\t\t\t\tnp.add(avg_mse_scores2, std_mse_scores2), alpha=0.2)\n",
    "\tplt.legend()\n",
    "\tplt.xlabel(\"Training Points\")\n",
    "\tplt.ylabel(\"Mean Squared Error\")\n",
    "\tplt.tight_layout()\n",
    "\tos.makedirs(f\"../figures/model_results/{output_metric}\", exist_ok=True)\n",
    "\t# plt.savefig(f\"../figures/model_results/{output_metric}/comparing_models_CustomPrior_priorVar_100_vs_1.png\", facecolor='w')\n",
    "\tplt.savefig(f\"../figures/model_results/{output_metric}/Poly_degree_comparison_noninformed.png\", facecolor='w')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Scale Per Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale\n",
    "\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6), facecolor=\"white\")\n",
    "\tfor i, (user, data) in enumerate(list(all_datasets.items())):\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\t# model_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\t# kernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\t# Y_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\t# dense_df = pd.DataFrame({\n",
    "\t\t# \t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t# \t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t# \t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t# \t\t})\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\t# optimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\t# optimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\t# plt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\", fontsize=16)\n",
    "\t\tplt.ylabel(\"scaling factor\", fontsize=16)\n",
    "\t\tplt.xticks(fontsize=14)\n",
    "\t\tplt.yticks(fontsize=14)\n",
    "\t\tplt.plot(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='*', markersize=10, label=f\"User {i+1}\")\n",
    "\tplt.legend(fontsize=12)\n",
    "\tplt.savefig(f\"../figures/optimal_scale_per_latency/{output_metric}.png\", facecolor='w')\n",
    "\tplt.show()\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot optimal scale per latency with model prediction\n",
    "\n",
    "output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"weighted_performance\"]\n",
    "for output_metric in output_metrics:\n",
    "\t# print(output_metric)\n",
    "\tuser_results = {}\n",
    "\tplt.figure(figsize=(12, 6))\n",
    "\tfor user, data in list(all_datasets.items())[:1]:\n",
    "\n",
    "\t\tif user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "\t\t## Choose model to apply, training on whole dataset, returning predictions over dense input\n",
    "\t\t# model_type = \"GPR_RBF_default\"\n",
    "\t\tmodel_type = \"GPR_RQ_default\"\n",
    "\t\t# model_type = \"Poly2\"\n",
    "\t\t\n",
    "\t\t# # Polynomial Regression\n",
    "\t\t# degree = 2\n",
    "\t\t# Y_pred_dense = PolyRegression(X.values, Y.values, X_dense, degree)\n",
    "\n",
    "\t\t# Gaussian Process Regression\n",
    "\t\t# kernel = ConstantKernel() * RBF() # Default RBF\n",
    "\t\tkernel = ConstantKernel() * RationalQuadratic() # Default Rational Quadratic\n",
    "\t\tY_pred_dense, Y_pred_std = GPRegression(X.values, Y.values, X_dense, kernel)\n",
    "\n",
    "\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\t\t\n",
    "\t\t\n",
    "\n",
    "\t\tif output_metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmax()][['latency', 'scale']]\n",
    "\t\telse: # optimal scale at minimum\n",
    "\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[output_metric].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t# print(optimal_scale_ref)\n",
    "\t\tplt.title(f\"Optimal Scale by {output_metric}\")\n",
    "\t\tplt.xlabel(\"latency\")\n",
    "\t\tplt.ylabel(\"scaling factor\")\n",
    "\t\tplt.scatter(optimal_scale_ref['latency'], optimal_scale_ref['scale'], marker='x', label=\"measured\")\n",
    "\t\tplt.plot(optimal_scale_dense['latency'], optimal_scale_dense['scale'], label=\"predicted\")\n",
    "\tplt.legend()\n",
    "\t# plt.savefig(f\"figures/optimal_scale_per_latency/{output_metric}.png\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot key metric heatmaps for average over users\n",
    "from utils import annotate_extrema\n",
    "\n",
    "delete_keys = [\"user_lizzie\", \"user_lizzie1\", \"user_lauren\", \"user_sarah1\"]\n",
    "sub_datasets = [all_datasets[key] for key in all_datasets.keys() if key not in delete_keys]\n",
    "\n",
    "combined_df = pd.concat(sub_datasets)\n",
    "averaged_df = combined_df.groupby([\"latency\", \"scale\"]).mean().reset_index()\n",
    "\n",
    "# averaged_df = all_datasets[\"user_lizzie\"]\n",
    "# Create a 2x5 subplot for the heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 6))\n",
    "\n",
    "# Plot the heatmap for throughput\n",
    "heatmap_throughput = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='throughput')\n",
    "ax = sns.heatmap(heatmap_throughput, ax=axes[0], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[0].set_title('Throughput vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_throughput.values, ax)\n",
    "\n",
    "# Plot heatmap for total error (target deviation + osd)\n",
    "averaged_df['total_error'] = averaged_df['avg_osd'] + averaged_df['avg_target_error']\n",
    "heatmap_error = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='total_error')\n",
    "ax = sns.heatmap(heatmap_error, ax=axes[1], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[1].set_title('Total Error vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_error.values, ax, extrema_type='min')\n",
    "\n",
    "# Plot heatmap for combined performance (movement speed - total error)\n",
    "heatmap_combo = averaged_df.pivot(\n",
    "    index='latency', columns='scale', values='weighted_performance')\n",
    "ax = sns.heatmap(heatmap_combo, ax=axes[2], cmap=\"YlGnBu\", annot=True, fmt='.3g')\n",
    "axes[2].set_title('Combined Performance vs. Latency and Scale')\n",
    "annotate_extrema(heatmap_combo.values, ax, extrema_type='max')\n",
    "\n",
    "# plt.title(\"User A\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f\"{data_folder}/heatmap_key_metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function, e.g., a sine wave function\n",
    "def smooth_2d_function(x, y):\n",
    "    return np.sin(np.sqrt(x**2 + y**2))\n",
    "\n",
    "# Generate sample points\n",
    "x = np.linspace(-5, 5, 10)\n",
    "y = np.linspace(-5, 5, 10)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Apply the function to the sample points\n",
    "z = smooth_2d_function(x, y)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'x': x.ravel(), 'y': y.ravel(), 'z': z.ravel()})\n",
    "\n",
    "X = df[['x', 'y']]\n",
    "Y = df['z']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8)\n",
    "\n",
    "z_pred, _ = GPRegression(X_train, Y_train, X)\n",
    "z_pred = z_pred.reshape(x.shape)\n",
    "\n",
    "# df[\"y_pred\"] = Y_pred\n",
    "\n",
    "# Plotting the function for visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "ax = axes[0].contourf(x, y, z, cmap='viridis')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].set_title('Smooth 2D Function')\n",
    "\n",
    "ax = axes[1].contourf(x, y, z_pred, cmap='viridis')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].set_title('Predictions')\n",
    "\n",
    "fig.colorbar(ax, label='Function Value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataframes\n",
    "df1 = pd.DataFrame({'latency': [1, 2], 'scale': [3, 4], 'col1': [5, 6], 'col2': [7, 8]})\n",
    "df2 = pd.DataFrame({'latency': [1, 2], 'scale': [3, 4], 'col1': [9, 10], 'col2': [11, 12]})\n",
    "\n",
    "# Concatenating the dataframes\n",
    "combined_df = pd.concat([df1, df2])\n",
    "print(combined_df)\n",
    "# Grouping by 'latency' and 'scale' and calculating the average of other columns\n",
    "grouped_df = combined_df.groupby(['latency', 'scale'])['col1'].apply(list)\n",
    "\n",
    "print(grouped_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_samples_with_noise(x_min, x_max, num_x, num_samples, noise_std):\n",
    "    \"\"\"\n",
    "    Generates samples from a sine function with added Gaussian noise.\n",
    "\n",
    "    Parameters:\n",
    "    x_min (float): Minimum x value.\n",
    "    x_max (float): Maximum x value.\n",
    "    num_x (int): Number of distinct x values in the range.\n",
    "    num_samples (int): Number of samples to generate for each x value.\n",
    "    noise_std (float): Standard deviation of the Gaussian noise.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Generate x values\n",
    "    x_values = np.linspace(x_min, x_max, num_x)\n",
    "\n",
    "    # Plot the underlying sine function\n",
    "    plt.plot(x_values, np.sin(x_values), label='Underlying sine function', color='blue')\n",
    "\n",
    "    # Generate and plot samples with noise for each x value\n",
    "    all_samples = []\n",
    "    for x in x_values:\n",
    "        noisy_samples = np.sin(x) + np.random.normal(0, noise_std, num_samples)\n",
    "        plt.scatter([x]*num_samples, noisy_samples, color='red', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Samples from a Sine Function with Noise')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "generate_samples_with_noise(x_min=0, x_max=2*np.pi, num_x=30, num_samples=10, noise_std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from utils import annotate_extrema\n",
    "# from models import BayesRegression\n",
    "\n",
    "# ### Modeling approach 2. Using Bayesian Regression, and all users as prior\n",
    "\n",
    "# # Separate datasets into one vs rest\n",
    "# user_to_remove = \"user_jason\"\n",
    "# one_user_data = all_datasets[user_to_remove]\n",
    "# sub_datasets = [all_datasets[key] for key in all_datasets.keys() if key != user_to_remove]\n",
    "# combined_df = pd.concat(sub_datasets)\n",
    "# averaged_df = combined_df.groupby([\"latency\", \"scale\"]).mean().reset_index()\n",
    "\n",
    "# # Perform Bayesian Regression on combined dataset\n",
    "# metric = 'throughput'\n",
    "# X = combined_df[['latency', 'scale']]\n",
    "# y = combined_df[metric]\n",
    "\n",
    "# poly = PolynomialFeatures(degree=2)\n",
    "# X_poly = poly.fit_transform(X.values)\n",
    "# # print(X_poly.shape)\n",
    "# # print(y.shape)\n",
    "\n",
    "# noise_std = 1\n",
    "# model = BayesRegression(X_poly.T, y, noise=noise_std)\n",
    "# weight_prior_mean, weight_prior_covar = model.fit()\n",
    "# # print(post_mean.shape, post_covar.shape)\n",
    "\n",
    "# latency_range = np.arange(0.0, combined_df['latency'].max()+0.01, 0.01)\n",
    "# scale_range = np.arange(combined_df['scale'].min(), combined_df['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "# latency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "# X_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "# X_dense = np.round(X_dense, 3)\n",
    "# X_dense_poly = poly.transform(X_dense)\n",
    "\n",
    "# # Y_pred, _ = model.predict(X_poly.T)\n",
    "# Y_pred_dense, _ = model.predict(X_dense_poly.T)\n",
    "# dense_df = pd.DataFrame({\n",
    "# \t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "# \t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "# \t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "# \t\t\t\t})\n",
    "# # one_user_data[\"Y_pred\"] = Y_pred\n",
    "\n",
    "# # utils.model_heatmaps(one_user_data, dense_df, X, \n",
    "# # \t\t\t\t\t user_to_remove, metric, \n",
    "# # \t\t\t\t\t \"BayesRegression\", post_mean.flatten())\n",
    "\n",
    "# if metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: \n",
    "#         extrema_type = \"max\" # optimal scale at maximum\n",
    "# else:\n",
    "# \textrema_type = \"min\" # optimal scale at minimum\n",
    "\t\t\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "# title = (f\"Modeling all but one user\")\n",
    "# fig.suptitle(title)\n",
    "\n",
    "# # Average over all but one user\n",
    "# averaged_data = averaged_df.pivot(\n",
    "# \tindex='latency', columns='scale', values=metric\n",
    "# )\n",
    "# sns.heatmap(averaged_data, cmap='YlGnBu', ax=ax[0], annot=True)\n",
    "# # annotate(ax[1], averaged_data, X_train, color='green')\n",
    "# ax[0].set_title('Average over all but one user')\n",
    "# ax[0].set_xlabel('Scale')\n",
    "# ax[0].set_ylabel('Latency')\n",
    "# annotate_extrema(averaged_data.values, ax[0], extrema_type)\n",
    "\n",
    "# dense_pred_data = dense_df.pivot(\n",
    "# \tindex='latency', columns='scale', values='Y_pred_dense'\n",
    "# )\n",
    "# sns.heatmap(dense_pred_data, cmap='YlGnBu', ax=ax[1])\n",
    "# # annotate(ax[1], dense_pred_data, X_train, color='green')\n",
    "# ax[1].set_title(f'Predicted Data over Dense Input\\n{weight_prior_mean.flatten()}')\n",
    "# ax[1].set_xlabel('Scale')\n",
    "# ax[1].set_ylabel('Latency')\n",
    "# annotate_extrema(dense_pred_data.values, ax[1], extrema_type)\n",
    "\n",
    "# # # Plot residuals\n",
    "# # data[\"residual\"] = np.abs(data[\"performance\"] - data[\"Y_pred\"])\n",
    "# # residual = data.pivot(\n",
    "# # \tindex='latency', columns='scale', values='residual'\n",
    "# # )\n",
    "# # sns.heatmap(residual, cmap='YlGnBu', ax=ax[2], annot=True)\n",
    "# # annotate(ax[2], residual, X_train, color='green')\n",
    "# # ax[2].set_title('Residuals')\n",
    "# # ax[2].set_xlabel('Scale')\n",
    "# # ax[2].set_ylabel('Latency')\n",
    "# # annotate_extrema(residual.values, ax[2], 'min')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# filepath = f\"figures/allbutone_noise_{noise_std}.png\"\n",
    "# plt.savefig(filepath, facecolor='w')\n",
    "# plt.show()\n",
    "# # plt.close()\n",
    "\n",
    "# ## Use as prior for one user model\n",
    "# data = one_user_data\n",
    "# X = data[['latency', 'scale']]\n",
    "# Y = data[metric]\n",
    "\n",
    "# # Initialize evaluation metrics\n",
    "# optimal_match_rate = []\n",
    "# optimal_scale_error = []\n",
    "# mse_scores = []\n",
    "# full_mse_scores = []\n",
    "# n_train_mse = []\n",
    "# n_train_full_mse = []\n",
    "# n_train_p = []\n",
    "\n",
    "# n = len(data)\n",
    "# n_train_values = range(2, n-1)\n",
    "# for n_train in n_train_values:\n",
    "\n",
    "# \tn_train_p.append(n_train / n)\n",
    "# \t# Split into training/test sets\n",
    "# \t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "# \ttrain_set, test_set = even_train_split(data, n_train)\n",
    "# \tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "# \tY_train, Y_test = train_set[metric], test_set[metric]\n",
    "\t\n",
    "# \t# Create dense test input\n",
    "# \t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "# \t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "# \tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "# \tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "# \tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "# \tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "# \tX_dense = np.round(X_dense, 3)\n",
    "\n",
    "# \tpoly = PolynomialFeatures(degree=2)\n",
    "# \tX_poly = poly.fit_transform(X.values)\n",
    "# \tX_train_poly = poly.transform(X_train.values)\n",
    "# \tX_test_poly = poly.transform(X_test.values)\n",
    "# \tX_dense_poly = poly.transform(X_dense)\n",
    "\t\n",
    "# \t# Train model\n",
    "# \tmodel = BayesRegression(X_train_poly.T, Y_train, noise_std)\n",
    "# \tmodel.set_prior(weight_prior_mean, weight_prior_covar)\n",
    "# \tpost_mean, post_covar = model.fit()\n",
    "\n",
    "# \t# Predict\n",
    "# \tY_pred, _ = model.predict(X_poly.T)\n",
    "# \tY_pred_dense, _ = model.predict(X_dense_poly.T)\n",
    "\n",
    "# \t## Evaluate metrics\n",
    "# \tdense_df = pd.DataFrame({\n",
    "# \t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "# \t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "# \t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "# \t\t})\n",
    "# \tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "# \t# Mean Square Error on whole dataset\n",
    "# \tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "# \tif True: #full_mse < 5000:\n",
    "# \t\tn_train_full_mse.append(n_train)\n",
    "# \t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "# \t# Mean Square Error on test set\n",
    "# \tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "# \tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "# \tif True: #mse < 5000:\n",
    "# \t\tn_train_mse.append(n_train)\n",
    "# \t\tmse_scores.append(mse)\n",
    "\t\n",
    "# \tif metric in [\"throughput\", \"avg_movement_speed\", \"weighted_performance\"]: # optimal scale at maximum\n",
    "# \t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "# \t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmax()][['latency', 'scale']]\n",
    "# \t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "# \telse: # optimal scale at minimum\n",
    "# \t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "# \t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmin()][['latency', 'scale']]\n",
    "# \t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "# \t# Merge the results on 'latency'\n",
    "# \tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "# \t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\n",
    "# \tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "# \t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "# \t# print(optimal_scale_dense)\n",
    "# \t# print(merged_ref_dense)\n",
    "\t\n",
    "\n",
    "# \t# Count the number of matches\n",
    "# \tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "# \tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "# \toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "# \toptimal_scale_error.append(scale_error)\n",
    "\n",
    "# \t# Visualize model prediction\n",
    "# \tif n_train == n-2:\n",
    "# \t\tutils.model_heatmaps(data, dense_df, X_train, user_to_remove, metric, \"BayesRegressionwithPrior\", post_mean.flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from models import BayesRegression\n",
    "\n",
    "\n",
    "### Generate example linear dataset\n",
    "\n",
    "# Parameters for the linear relationship\n",
    "a = 2  # slope\n",
    "b = 3  # intercept\n",
    "c = 1\n",
    "noise_std = 30 # standard deviation of the noise\n",
    "\n",
    "np.random.seed(0)  # for reproducibility\n",
    "X = np.linspace(-5, 5, 100)\n",
    "y = []\n",
    "\n",
    "# Generating Y values with noise\n",
    "X_full = []\n",
    "y_full = []\n",
    "for x_val in X:\n",
    "\ty_val = a * x_val**2 + b * x_val + c\n",
    "\ty.append(y_val)\n",
    "\tX_full += [x_val for i in range(10)]\n",
    "\ty_noisy = y_val + np.random.normal(0, noise_std, 10)\n",
    "\ty_full += list(y_noisy)\n",
    "\n",
    "X_full = np.array(X_full)\n",
    "y_full = np.array(y_full)\n",
    "# Creating a DataFrame\n",
    "dataset = pd.DataFrame({'X': X, 'y': y})\n",
    "\n",
    "X_homo = X.reshape((1, -1))\n",
    "X_homo = np.vstack((X_homo, np.ones(X_homo.shape)))\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "# print(X.shape)\n",
    "X_poly = poly.fit_transform(X_full.reshape(-1, 1))\n",
    "# print(X_poly.shape)\n",
    "# print(X_poly[1,:])\n",
    "# print(y.shape)\n",
    "model = BayesRegression(X_poly.T, y_full, noise=1000)\n",
    "model.set_prior(np.array([1, 3, 2]), 0.0001)\n",
    "post_mean, post_covar = model.fit()\n",
    "# print(post_mean)\n",
    "# print(post_covar)\n",
    "\n",
    "# Test inputs\n",
    "test_input = np.arange(-5, 6, 1).reshape(-1, 1)\n",
    "test_input_poly = poly.transform(test_input)\n",
    "# print(test_input_poly.shape)\n",
    "# test_input_homo = np.vstack((test_input, np.ones(test_input.shape)))\n",
    "pred_mean, pred_covar = model.predict(test_input_poly.T)\n",
    "# print(pred_mean)\n",
    "# print(pred_covar.shape)\n",
    "\n",
    "# print(X.shape, y.shape, y_noisy.shape)\n",
    "plt.scatter(X_full, y_full, marker='x')\n",
    "plt.plot(X, y, linestyle='--', color='black')\n",
    "plt.scatter(test_input, pred_mean, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stat test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stats test\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Prepare data\n",
    "scale_values = [0.1, 0.15, 0.2, 0.4, 0.7, 1.0]\n",
    "users_to_keep = [\"user_jason\", \"user_shreya\", \"user_sujaan\", \"user_xiao\", \"user_yutong\", \"user_colin\"]\n",
    "users_to_remove = [\"user_lauren\", \"user_lizzie\", \"user_lizzie1\", \"user_sarah1\", \"user_lizzie2\"]\n",
    "all_data_list = [all_datasets[user] for user in all_datasets.keys() if user in users_to_keep]\n",
    "# all_data_list = [all_datasets[user] for user in all_datasets.keys() if user not in users_to_remove]\n",
    "# print(len(all_data_list))\n",
    "combined_df = pd.concat(all_data_list)\n",
    "grouped_df = combined_df.groupby([\"latency\", \"scale\"])\n",
    "TP_group = grouped_df[\"throughput\"].apply(list)\n",
    "\n",
    "# Group by 'latency'\n",
    "grouped_by_latency = combined_df.groupby('latency')\n",
    "pvals = {}\n",
    "scales = {}\n",
    "# Iterate over each latency group\n",
    "metric = \"total_error\"\n",
    "alternative_hyp = \"less\"\n",
    "for l, (latency, latency_group) in enumerate(grouped_by_latency):\n",
    "\tprint(f\"Latency: {latency}\")\n",
    "\n",
    "\t# Within each latency group, further group by 'scale'\n",
    "\tgrouped_by_scale = latency_group.groupby('scale')\n",
    "\tbaseline = list(grouped_by_scale.get_group(1.0)[metric])\n",
    "\tprint(\"Baseline: \", baseline)\n",
    "\tpvals[latency] = []\n",
    "\tscales[latency] = []\n",
    "\t# Iterate over each scale group within the current latency group\n",
    "\tfor s, (scale, scale_group) in enumerate(grouped_by_scale):\n",
    "\t\tprint(\"Scale: \", scale)\n",
    "\t\tvalues = list(scale_group[metric])\n",
    "\t\tprint(\"Values: \", values)\n",
    "\t\tif len(values) == len(users_to_keep) and scale != 1.0:\n",
    "\t\t\tprint(f\"comparing scale {scale} against baseline\")\n",
    "\t\t\tprint(f\"  Scale: {scale}, Values: {list(scale_group[metric])}\")\n",
    "\t\t\tt_statistic, p_value = ttest_rel(values, baseline, alternative=alternative_hyp)\n",
    "\t\t\tpvals[latency].append(p_value)\n",
    "\t\t\tscales[latency].append(scale)\n",
    "\t\t\tprint(p_value)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "for latency, p_values in pvals.items():\n",
    "\tplt.plot(scales[latency], p_values, marker='o', label=f\"{latency} s\")\n",
    "\n",
    "# plt.title(f\"Paired-Sample T Test Comparing {metric}, Alternative Hypothesis: {alternative_hyp}\")\n",
    "plt.axhline(0.05, color=\"black\", linestyle='--', label=\"5% Significance\")\n",
    "plt.xlabel(\"scaling factor\")\n",
    "plt.ylabel(\"p value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../figures/stat_tests/{metric}_{alternative_hyp}.png\", facecolor='w')\n",
    "plt.show()\n",
    "\n",
    "# print(TP_group)\n",
    "# p_vals = {}\n",
    "# baseline = TP_group[(0.0, 1.0)]\n",
    "# test = TP_group[(0.0, 0.15)]\n",
    "# print(baseline)\n",
    "# print(test)\n",
    "# t_stat, p_val = ttest_rel(baseline, test)\n",
    "# print(p_val)\n",
    "# for params, values in TP_group.items():\n",
    "# \t# print(f\"Latency: {latency}, Scale: {scale}, Values: {len(list(throughput))}\")\n",
    "# \tif len(values) == 6\n",
    "\n",
    "# # Example data: before and after treatment\n",
    "# data_before = np.array([20, 21, 19, 22, 20, 23, 21])\n",
    "# data_after = np.array([22, 22, 20, 23, 21, 24, 22])\n",
    "\n",
    "# # Perform the paired t-test\n",
    "# t_statistic, p_value = ttest_rel(data_before, data_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Regression modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All vs. one Bayesian Regression\n",
    "\n",
    "from models import PolyRegression, GPRegression, BayesRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, RationalQuadratic, WhiteKernel\n",
    "from utils import annotate_extrema, model_heatmaps\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "obs_noise_std = 1\n",
    "prior_base_var = 100\n",
    "prior_type = \"Custom\"\n",
    "model_type = f\"BR_{prior_type}Prior_ObsNoise_{obs_noise_std}_PriorVar_{prior_base_var}_standardized\"\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename='warnings_log.txt', level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "# Function to redirect warnings to logging\n",
    "def warn_to_logging(message, category, filename, lineno, file=None, line=None):\n",
    "    logging.warning(f'{filename}:{lineno}: {category.__name__}: {message}')\n",
    "\n",
    "# Redirect all warnings to the warn_to_logging function\n",
    "warnings.showwarning = warn_to_logging\n",
    "\n",
    "all_results = {}\n",
    "# output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"total_error\", \"weighted_performance\"]\n",
    "output_metrics = [\"throughput_standard\", \"avg_target_error_standard\", \"avg_osd_standard\", \"avg_movement_speed_standard\", \"total_error_standard\", \"weighted_performance_standard\"]\n",
    "for metric in output_metrics:\n",
    "\t\n",
    "\tprint(metric)\n",
    "\tuser_results = {}\n",
    "\tfor user, data in list(all_datasets.items()): \n",
    "\t\tif user == \"user_test\": continue\n",
    "\t\t# if user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t# \tcontinue\n",
    "\t\tprint(f\"\\t{user}\")\n",
    "\n",
    "\t\tsub_datasets = [all_datasets[key] for key in all_datasets.keys() if key != user]\n",
    "\t\tcombined_df = pd.concat(sub_datasets)\n",
    "\t\taveraged_df = combined_df.groupby([\"latency\", \"scale\"]).mean().reset_index()\n",
    "\n",
    "\t\t# Perform Bayesian Regression on combined dataset\n",
    "\t\tX = combined_df[['latency', 'scale']]\n",
    "\t\ty = combined_df[[metric]].to_dict('list') # convert to dict bc i did this for some reason..\n",
    "\n",
    "\t\tpoly = PolynomialFeatures(degree=2)\n",
    "\t\tX_poly = poly.fit_transform(X.values)\n",
    "\t\t# print(X_poly.shape)\n",
    "\t\t# print(y.shape)\n",
    "\n",
    "\t\t### Train a model on all but one user\n",
    "\t\tmodel = BayesRegression(X_poly, y, obs_noise_std=obs_noise_std, prior_base_var=prior_base_var)\n",
    "\t\tposterior_dict = model.train()\n",
    "\t\tweight_prior_mean, weight_prior_covar = posterior_dict[metric][0], posterior_dict[metric][1] \n",
    "\t\t# print(post_mean.shape, post_covar.shape)\n",
    "\n",
    "\t\t# Predict and visualize for all vs. one model\n",
    "\t\tlatency_range = np.arange(0.0, combined_df['latency'].max()+0.01, 0.01)\n",
    "\t\tscale_range = np.arange(combined_df['scale'].min(), combined_df['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\tX_dense_poly = poly.transform(X_dense)\n",
    "\n",
    "\t\tY_pred = model.predict(X_poly)[metric][0]\n",
    "\t\tY_pred_dense = model.predict(X_dense_poly)[metric][0]\n",
    "\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t\t\t})\n",
    "\t\t# one_user_data[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t# utils.model_heatmaps(one_user_data, dense_df, X, \n",
    "\t\t# \t\t\t\t\t user_to_remove, metric, \n",
    "\t\t# \t\t\t\t\t \"BayesRegression\", post_mean.flatten())\n",
    "\n",
    "\t\tif metric in [\"throughput_standard\", \"avg_movement_speed_standard\", \"weighted_performance_standard\"]: \n",
    "\t\t\t\textrema_type = \"max\" # optimal scale at maximum\n",
    "\t\telse:\n",
    "\t\t\textrema_type = \"min\" # optimal scale at minimum\n",
    "\t\t\t\t\n",
    "\t\tfig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\t\ttitle = (f\"Modeling all but {user}\")\n",
    "\t\tfig.suptitle(title)\n",
    "\n",
    "\t\t# Average over all but one user\n",
    "\t\taveraged_data = averaged_df.pivot(\n",
    "\t\t\tindex='latency', columns='scale', values=metric\n",
    "\t\t)\n",
    "\t\tsns.heatmap(averaged_data, cmap='YlGnBu', ax=ax[0], annot=True)\n",
    "\t\t# annotate(ax[1], averaged_data, X_train, color='green')\n",
    "\t\tax[0].set_title('Average')\n",
    "\t\tax[0].set_xlabel('Scale')\n",
    "\t\tax[0].set_ylabel('Latency')\n",
    "\t\tannotate_extrema(averaged_data.values, ax[0], extrema_type)\n",
    "\n",
    "\t\tdense_pred_data = dense_df.pivot(\n",
    "\t\t\tindex='latency', columns='scale', values='Y_pred_dense'\n",
    "\t\t)\n",
    "\t\tsns.heatmap(dense_pred_data, cmap='YlGnBu', ax=ax[1])\n",
    "\t\t# annotate(ax[1], dense_pred_data, X_train, color='green')\n",
    "\t\tax[1].set_title(f'Predicted Data over Dense Input\\n{weight_prior_mean.flatten()}')\n",
    "\t\tax[1].set_xlabel('Scale')\n",
    "\t\tax[1].set_ylabel('Latency')\n",
    "\t\tannotate_extrema(dense_pred_data.values, ax[1], extrema_type)\n",
    "\n",
    "\t\t# # Plot residuals\n",
    "\t\t# data[\"residual\"] = np.abs(data[\"performance\"] - data[\"Y_pred\"])\n",
    "\t\t# residual = data.pivot(\n",
    "\t\t# \tindex='latency', columns='scale', values='residual'\n",
    "\t\t# )\n",
    "\t\t# sns.heatmap(residual, cmap='YlGnBu', ax=ax[2], annot=True)\n",
    "\t\t# annotate(ax[2], residual, X_train, color='green')\n",
    "\t\t# ax[2].set_title('Residuals')\n",
    "\t\t# ax[2].set_xlabel('Scale')\n",
    "\t\t# ax[2].set_ylabel('Latency')\n",
    "\t\t# annotate_extrema(residual.values, ax[2], 'min')\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tfolder = f\"../figures/allbutone/{metric}/{user}\"\n",
    "\t\tos.makedirs(folder, exist_ok=True)\n",
    "\t\tfilepath = f\"{folder}/obs_noise_std_{obs_noise_std}_prior_base_var_{prior_base_var}.png\"\n",
    "\t\tplt.savefig(filepath, facecolor='w')\n",
    "\t\t# plt.show()\n",
    "\n",
    "\t\t### Now perform Bayesian Regression for one user, using prior from other users\n",
    "\n",
    "\t\t# Prepare data \n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tY = data[metric]\n",
    "\n",
    "\t\t# Whether to perform individual modeling, i.e. uninformed prior\n",
    "\t\tif prior_type == \"Default\":\n",
    "\t\t\tweight_prior_mean = np.zeros(np.shape(weight_prior_mean))\n",
    "\t\t\tweight_prior_covar = np.eye(np.shape(weight_prior_covar)[0]) * prior_base_var * obs_noise_std**2\n",
    "\n",
    "\t\t# Initialize evaluation metrics\n",
    "\t\toptimal_match_rate = []\n",
    "\t\toptimal_scale_error = []\n",
    "\t\tmse_scores = []\n",
    "\t\tfull_mse_scores = []\n",
    "\t\tn_train_mse = []\n",
    "\t\tn_train_full_mse = []\n",
    "\t\tn_train_p = []\n",
    "\t\t\n",
    "\t\t# print('hi')\n",
    "\t\tn = len(data)\n",
    "\t\tn_train_values = range(2, n-1)\n",
    "\t\tfor n_train in n_train_values:\n",
    "\n",
    "\t\t\tn_train_p.append(n_train / n)\n",
    "\t\t\t# Split into training/test sets\n",
    "\t\t\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\t\t\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\t\t\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\t\t\tY_train, Y_test = train_set[metric], test_set[metric]\n",
    "\t\t\tY_train_dict = train_set[[metric]].to_dict('list')\n",
    "\t\t\tY_test_dict = test_set[[metric]].to_dict('list')\n",
    "\t\t\t\n",
    "\t\t\t# Create dense test input\n",
    "\t\t\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t\t\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\t\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\t\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\t\t\t\n",
    "\t\t\tif model_type.startswith(\"BR\"):\n",
    "\t\t\t\tpoly = PolynomialFeatures(degree=2)\n",
    "\t\t\t\tX_poly = poly.fit_transform(X.values)\n",
    "\t\t\t\tX_train_poly = poly.transform(X_train.values)\n",
    "\t\t\t\tX_test_poly = poly.transform(X_test.values)\n",
    "\t\t\t\tX_dense_poly = poly.transform(X_dense)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Train model\n",
    "\t\t\t\t# noise_std = 0.001\n",
    "\t\t\t\tmodel = BayesRegression(X_train_poly, Y_train_dict, obs_noise_std=obs_noise_std)\n",
    "\t\t\t\tmodel.set_informed_prior(weight_prior_mean, weight_prior_covar)\n",
    "\t\t\t\tposterior_dict = model.train()\n",
    "\t\t\t\tpost_mean, post_covar = posterior_dict[metric][0], posterior_dict[metric][1]\n",
    "\t\t\t\tmodel_params = f\"noise: {obs_noise_std}, coef: {post_mean.flatten()}\"\n",
    "\t\t\t\t# Predict\n",
    "\t\t\t\tY_pred = model.predict(X_poly)[metric][0]\n",
    "\t\t\t\tY_pred_dense = model.predict(X_dense_poly)[metric][0]\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(\"Invalid kernel specification!\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t## Evaluate metrics\n",
    "\t\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "\t\t\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t\t# Mean Square Error on whole dataset\n",
    "\t\t\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\t\t\tif True: #full_mse < 5000:\n",
    "\t\t\t\tn_train_full_mse.append(n_train)\n",
    "\t\t\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t\t\t# Mean Square Error on test set\n",
    "\t\t\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\t\t\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\t\t\tif True: #mse < 5000:\n",
    "\t\t\t\tn_train_mse.append(n_train)\n",
    "\t\t\t\tmse_scores.append(mse)\n",
    "\t\t\t\n",
    "\t\t\tif metric in [\"throughput_standard\", \"avg_movement_speed_standard\", \"weighted_performance_standard\"]: # optimal scale at maximum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\t\t\telse: # optimal scale at minimum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t\t# Merge the results on 'latency'\n",
    "\t\t\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\t\t\n",
    "\t\t\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t\t\t# print(optimal_scale_dense)\n",
    "\t\t\t# print(merged_ref_dense)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# Count the number of matches\n",
    "\t\t\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\t\t\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\t\t\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\t\t\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t\t\t# Visualize model prediction\n",
    "\t\t\tif n_train == n-2:\n",
    "\t\t\t\tmodel_heatmaps(data, dense_df, X_train, user, metric, model_type, model_params)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Store results from this dataset\n",
    "\t\t\tuser_results[user] = {\n",
    "\t\t\t\t'n_train_mse': list(n_train_mse),\n",
    "\t\t\t\t'n_train_full_mse': list(n_train_full_mse),\n",
    "\t\t\t\t'full_mse_scores': full_mse_scores,\n",
    "\t\t\t\t'mse_scores': mse_scores,\n",
    "\t\t\t\t'n_train_all': list(n_train_values),\n",
    "\t\t\t\t'match_rate': optimal_match_rate,\n",
    "\t\t\t\t'scale_error': optimal_scale_error,\n",
    "\t\t\t\t'n_train_p': n_train_p\n",
    "\t\t\t}\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\t\n",
    "\telse:\n",
    "\t\t# print(f\"saving {metric} to all_results\")\n",
    "\t\tall_results[metric] = user_results\n",
    "\t\tcontinue\n",
    "\tbreak\n",
    "\n",
    "# print(all_results.keys())\n",
    "with open(f\"../model_result_data/{model_type}.json\", \"w\") as file:\n",
    "\tjson.dump(all_results, file)\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "# \tog_results = json.load(file)\n",
    "\n",
    "# og_results[\"total_error\"] = all_results[\"total_error\"]\n",
    "\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(og_results, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal-Inverse Gamma Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New approach to modeling 8/19/24\n",
    "\n",
    "from models import PolyRegression, GPRegression, BayesRegression, BayesRegressionNIG\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, RationalQuadratic, WhiteKernel\n",
    "from utils import annotate_extrema, model_heatmaps\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "## Choose model to apply, returning predictions over original dataset and dense inputs\n",
    "# model_type = \"Poly2\"\n",
    "# model_type = \"GPR_RBF_default\"\n",
    "# model_type = \"GPR_RBF_anisotropic\"\n",
    "# model_type = \"GPR_RBF_Noise_default\"\n",
    "# model_type = \"GPR_RBF_Noise_anisotropic\"\n",
    "# model_type = \"GPR_RQ_default\"\n",
    "# model_type = \"GPR_RQ_Noise_default\"\n",
    "# obs_noise_std = 1\n",
    "# prior_base_var = 100\n",
    "prior_type = \"Noninform\"\n",
    "model_type = f\"BRNIG_Poly1_{prior_type}Prior_standardized\"\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename='warnings_log.txt', level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "# Function to redirect warnings to logging\n",
    "def warn_to_logging(message, category, filename, lineno, file=None, line=None):\n",
    "    logging.warning(f'{filename}:{lineno}: {category.__name__}: {message}')\n",
    "\n",
    "# Redirect all warnings to the warn_to_logging function\n",
    "warnings.showwarning = warn_to_logging\n",
    "\n",
    "all_results = {}\n",
    "# output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"total_error\", \"weighted_performance\"]\n",
    "output_metrics = [\"throughput_standard\", \"avg_target_error_standard\", \"avg_osd_standard\", \"avg_movement_speed_standard\", \"total_error_standard\", \"weighted_performance_standard\"]\n",
    "for metric in output_metrics:\n",
    "\t\n",
    "\tprint(metric)\n",
    "\tuser_results = {}\n",
    "\tfor user, data in list(all_datasets.items()): \n",
    "\t\tif user in [\"user_test\", \"user_jason_new\", \"user_jaysuhn\"]: continue\n",
    "\t\t# if user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t# \tcontinue\n",
    "\t\tprint(f\"\\t{user}\")\n",
    "\n",
    "\t\t# sub_datasets = [all_datasets[key] for key in all_datasets.keys() if key != user]\n",
    "\t\t# combined_df = pd.concat(sub_datasets)\n",
    "\t\t# averaged_df = combined_df.groupby([\"latency\", \"scale\"]).mean().reset_index()\n",
    "\n",
    "\t\t# # Perform Bayesian Regression on combined dataset\n",
    "\t\t# X = combined_df[['latency', 'scale']]\n",
    "\t\t# y = combined_df[[metric]].to_dict('list') # convert to dict bc i did this for some reason..\n",
    "\n",
    "\t\t# poly = PolynomialFeatures(degree=2)\n",
    "\t\t# X_poly = poly.fit_transform(X.values)\n",
    "\t\t# # print(X_poly.shape)\n",
    "\t\t# # print(y.shape)\n",
    "\n",
    "\t\t# ### Train a model on all but one user\n",
    "\t\t# model = BayesRegression(X_poly, y, obs_noise_std=obs_noise_std, prior_base_var=prior_base_var)\n",
    "\t\t# posterior_dict = model.train()\n",
    "\t\t# weight_prior_mean, weight_prior_covar = posterior_dict[metric][0], posterior_dict[metric][1] \n",
    "\t\t# # print(post_mean.shape, post_covar.shape)\n",
    "\n",
    "\t\t# # Predict and visualize for all vs. one model\n",
    "\t\t# latency_range = np.arange(0.0, combined_df['latency'].max()+0.01, 0.01)\n",
    "\t\t# scale_range = np.arange(combined_df['scale'].min(), combined_df['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t# latency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t# X_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t# X_dense = np.round(X_dense, 3)\n",
    "\t\t# X_dense_poly = poly.transform(X_dense)\n",
    "\n",
    "\t\t# Y_pred = model.predict(X_poly)[metric][0]\n",
    "\t\t# Y_pred_dense = model.predict(X_dense_poly)[metric][0]\n",
    "\t\t# dense_df = pd.DataFrame({\n",
    "\t\t# \t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t# \t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t# \t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t# \t\t\t\t})\n",
    "\t\t# # one_user_data[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t# # utils.model_heatmaps(one_user_data, dense_df, X, \n",
    "\t\t# # \t\t\t\t\t user_to_remove, metric, \n",
    "\t\t# # \t\t\t\t\t \"BayesRegression\", post_mean.flatten())\n",
    "\n",
    "\t\t# if metric in [\"throughput_standard\", \"avg_movement_speed_standard\", \"weighted_performance_standard\"]: \n",
    "\t\t# \t\textrema_type = \"max\" # optimal scale at maximum\n",
    "\t\t# else:\n",
    "\t\t# \textrema_type = \"min\" # optimal scale at minimum\n",
    "\t\t\t\t\n",
    "\t\t# fig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\t\t# title = (f\"Modeling all but {user}\")\n",
    "\t\t# fig.suptitle(title)\n",
    "\n",
    "\t\t# # Average over all but one user\n",
    "\t\t# averaged_data = averaged_df.pivot(\n",
    "\t\t# \tindex='latency', columns='scale', values=metric\n",
    "\t\t# )\n",
    "\t\t# sns.heatmap(averaged_data, cmap='YlGnBu', ax=ax[0], annot=True)\n",
    "\t\t# # annotate(ax[1], averaged_data, X_train, color='green')\n",
    "\t\t# ax[0].set_title('Average')\n",
    "\t\t# ax[0].set_xlabel('Scale')\n",
    "\t\t# ax[0].set_ylabel('Latency')\n",
    "\t\t# annotate_extrema(averaged_data.values, ax[0], extrema_type)\n",
    "\n",
    "\t\t# dense_pred_data = dense_df.pivot(\n",
    "\t\t# \tindex='latency', columns='scale', values='Y_pred_dense'\n",
    "\t\t# )\n",
    "\t\t# sns.heatmap(dense_pred_data, cmap='YlGnBu', ax=ax[1])\n",
    "\t\t# # annotate(ax[1], dense_pred_data, X_train, color='green')\n",
    "\t\t# ax[1].set_title(f'Predicted Data over Dense Input\\n{weight_prior_mean.flatten()}')\n",
    "\t\t# ax[1].set_xlabel('Scale')\n",
    "\t\t# ax[1].set_ylabel('Latency')\n",
    "\t\t# annotate_extrema(dense_pred_data.values, ax[1], extrema_type)\n",
    "\n",
    "\t\t# # # Plot residuals\n",
    "\t\t# # data[\"residual\"] = np.abs(data[\"performance\"] - data[\"Y_pred\"])\n",
    "\t\t# # residual = data.pivot(\n",
    "\t\t# # \tindex='latency', columns='scale', values='residual'\n",
    "\t\t# # )\n",
    "\t\t# # sns.heatmap(residual, cmap='YlGnBu', ax=ax[2], annot=True)\n",
    "\t\t# # annotate(ax[2], residual, X_train, color='green')\n",
    "\t\t# # ax[2].set_title('Residuals')\n",
    "\t\t# # ax[2].set_xlabel('Scale')\n",
    "\t\t# # ax[2].set_ylabel('Latency')\n",
    "\t\t# # annotate_extrema(residual.values, ax[2], 'min')\n",
    "\n",
    "\t\t# plt.tight_layout()\n",
    "\t\t# folder = f\"../figures/allbutone/{metric}/{user}\"\n",
    "\t\t# os.makedirs(folder, exist_ok=True)\n",
    "\t\t# filepath = f\"{folder}/obs_noise_std_{obs_noise_std}_prior_base_var_{prior_base_var}.png\"\n",
    "\t\t# plt.savefig(filepath, facecolor='w')\n",
    "\t\t# # plt.show()\n",
    "\n",
    "\t\t### Now perform Bayesian Regression for one user, using prior from other users\n",
    "\n",
    "\t\t# Prepare data \n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tY = data[metric]\n",
    "\t\tY_dict = data[[metric]].to_dict('list')\n",
    "\t\tpoly = PolynomialFeatures(degree=1)\n",
    "\t\tX_poly = poly.fit_transform(X.values)\n",
    "\t\tprint(\"X_Poly shape: \", X_poly.shape)\n",
    "\t\t# save posterior information from full dataset for later use\n",
    "\t\tmodel_full = BayesRegressionNIG(X_poly, Y_dict)\n",
    "\t\tposterior_hyperparams = model_full.train()[metric]\n",
    "\n",
    "\t\t# Initialize evaluation metrics\n",
    "\t\toptimal_match_rate = []\n",
    "\t\toptimal_scale_error = []\n",
    "\t\tmse_scores = []\n",
    "\t\tfull_mse_scores = []\n",
    "\t\tn_train_mse = []\n",
    "\t\tn_train_full_mse = []\n",
    "\t\tn_train_p = []\n",
    "\t\t\n",
    "\t\t# print('hi')\n",
    "\t\tn = len(data)\n",
    "\t\tn_train_values = range(2, n-1)\n",
    "\t\tfor n_train in n_train_values:\n",
    "\n",
    "\t\t\tn_train_p.append(n_train / n)\n",
    "\t\t\t# Split into training/test sets\n",
    "\t\t\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\t\t\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\t\t\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\t\t\tY_train, Y_test = train_set[metric], test_set[metric]\n",
    "\t\t\tY_train_dict = train_set[[metric]].to_dict('list')\n",
    "\t\t\tY_test_dict = test_set[[metric]].to_dict('list')\n",
    "\t\t\t\n",
    "\t\t\t# Create dense test input\n",
    "\t\t\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t\t\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\t\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\t\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tX_train_poly = poly.transform(X_train.values)\n",
    "\t\t\tX_test_poly = poly.transform(X_test.values)\n",
    "\t\t\tX_dense_poly = poly.transform(X_dense)\n",
    "\t\t\t\n",
    "\t\t\t# Train model\n",
    "\t\t\tmodel = BayesRegressionNIG(X_train_poly, Y_train_dict)\n",
    "\t\t\tposterior_dict = model.train()\n",
    "\t\t\tpost_mean, post_covar = posterior_dict[metric][0], posterior_dict[metric][1]\n",
    "\t\t\tmodel_params = f\"coef: {post_mean.flatten()}\"\n",
    "\t\t\t# Predict\n",
    "\t\t\tY_pred = model.predict(X_poly)[metric][1]\n",
    "\t\t\tY_pred_dense = model.predict(X_dense_poly)[metric][1]\n",
    "\n",
    "\t\t\t## Evaluate metrics\n",
    "\t\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "\t\t\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t\t# Mean Square Error on whole dataset\n",
    "\t\t\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\t\t\tif True: #full_mse < 5000:\n",
    "\t\t\t\tn_train_full_mse.append(n_train)\n",
    "\t\t\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t\t\t# Mean Square Error on test set\n",
    "\t\t\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\t\t\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\t\t\tif True: #mse < 5000:\n",
    "\t\t\t\tn_train_mse.append(n_train)\n",
    "\t\t\t\tmse_scores.append(mse)\n",
    "\t\t\t\n",
    "\t\t\tif metric in [\"throughput_standard\", \"avg_movement_speed_standard\", \"weighted_performance_standard\"]: # optimal scale at maximum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\t\t\telse: # optimal scale at minimum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t\t# Merge the results on 'latency'\n",
    "\t\t\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\t\t\n",
    "\t\t\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t\t\t# print(optimal_scale_dense)\n",
    "\t\t\t# print(merged_ref_dense)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# Count the number of matches\n",
    "\t\t\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\t\t\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\t\t\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\t\t\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t\t\t# Visualize model prediction\n",
    "\t\t\tif n_train == n-2:\n",
    "\t\t\t\tmodel_heatmaps(data, dense_df, X_train, user, metric, model_type, model_params)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Store results from this dataset\n",
    "\t\t\tuser_results[user] = {\n",
    "\t\t\t\t'n_train_mse': list(n_train_mse),\n",
    "\t\t\t\t'n_train_full_mse': list(n_train_full_mse),\n",
    "\t\t\t\t'full_mse_scores': full_mse_scores,\n",
    "\t\t\t\t'mse_scores': mse_scores,\n",
    "\t\t\t\t'n_train_all': list(n_train_values),\n",
    "\t\t\t\t'match_rate': optimal_match_rate,\n",
    "\t\t\t\t'scale_error': optimal_scale_error,\n",
    "\t\t\t\t'n_train_p': n_train_p,\n",
    "\t\t\t\t'posterior_hyperparams': posterior_hyperparams\n",
    "\t\t\t}\n",
    "\t\t\t# print(user_results)\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\t\n",
    "\telse:\n",
    "\t\t# print(f\"saving {metric} to all_results\")\n",
    "\t\tall_results[metric] = user_results\n",
    "\t\tcontinue\n",
    "\tbreak\n",
    "\n",
    "# print(all_results.keys())\n",
    "# with open(f\"../model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(all_results, file)\n",
    "with open(f\"../model_result_data/{model_type}.pkl\", 'wb') as f:\n",
    "        pickle.dump(all_results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "# \tog_results = json.load(file)\n",
    "\n",
    "# og_results[\"total_error\"] = all_results[\"total_error\"]\n",
    "\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(og_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Demographic Analysis\n",
    "\n",
    "# M vs F t test\n",
    "male_users = [\"user_jason\", \"user_sujaan\", \"user_xiao\", \"user_yutong\"]\n",
    "female_users = [\"user_shreya\", \"user_lauren\", \"user_lizzie2\", \"user_sarah1\"]\n",
    "male_combined = pd.concat([all_datasets[user] for user in male_users])\n",
    "female_combined = pd.concat([all_datasets[user] for user in female_users])\n",
    "male_users\n",
    "\n",
    "p_vals = {}\n",
    "\n",
    "for latency, latency_group in male_users.groupby(\"latency\"):\n",
    "\tprint(f\"Latency: {latency}\")\n",
    "\n",
    "\t# Within each latency group, further group by 'scale'\n",
    "\tgrouped_by_scale = latency_group.groupby('scale')\n",
    "\tbaseline = list(grouped_by_scale.get_group(1.0)[metric])\n",
    "\tprint(baseline)\n",
    "\tpvals[latency] = []\n",
    "\tscales[latency] = []\n",
    "\t# Iterate over each scale group within the current latency group\n",
    "\tfor s, (scale, scale_group) in enumerate(grouped_by_scale):\n",
    "\t\tvalues = list(scale_group[metric])\n",
    "\t\t\n",
    "\t\tprint(f\"comparing scale {scale} against baseline\")\n",
    "\t\tprint(f\"  Scale: {scale}, Values: {list(scale_group[metric])}\")\n",
    "\t\tt_statistic, p_value = ttest_rel(values, baseline, alternative=alternative_hyp)\n",
    "\t\tpvals[latency].append(p_value)\n",
    "\t\tscales[latency].append(scale)\n",
    "\t\tprint(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRNIG Informed Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import PolyRegression, GPRegression, BayesRegression, BayesRegressionNIG\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, RationalQuadratic, WhiteKernel\n",
    "from utils import annotate_extrema, model_heatmaps\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from scipy.stats import gamma\n",
    "\n",
    "# Load invidual noninformed model data\n",
    "with open(\"../model_result_data/BRNIG_NoninformPrior_standardized.pkl\", 'rb') as f:\n",
    "\tnoninform_model_data = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "prior_type = \"Inform\"\n",
    "model_type = f\"BRNIG_{prior_type}Prior_gammaEst_standardized\"\n",
    "\n",
    "# Configure logging to write to a file\n",
    "logging.basicConfig(filename='warnings_log.txt', level=logging.WARNING, format='%(message)s')\n",
    "\n",
    "# Function to redirect warnings to logging\n",
    "def warn_to_logging(message, category, filename, lineno, file=None, line=None):\n",
    "    logging.warning(f'{filename}:{lineno}: {category.__name__}: {message}')\n",
    "\n",
    "# Redirect all warnings to the warn_to_logging function\n",
    "warnings.showwarning = warn_to_logging\n",
    "\n",
    "all_results = {}\n",
    "# output_metrics = [\"throughput\", \"avg_target_error\", \"avg_osd\", \"avg_movement_speed\", \"total_error\", \"weighted_performance\"]\n",
    "output_metrics = [\"throughput_standard\", \"avg_target_error_standard\", \"avg_osd_standard\", \"avg_movement_speed_standard\", \"total_error_standard\", \"weighted_performance_standard\"]\n",
    "for metric in output_metrics:\n",
    "\t\n",
    "\tprint(metric)\n",
    "\tuser_results = {}\n",
    "\tfor user, data in list(all_datasets.items()): \n",
    "\t\tif user in [\"user_test\", \"user_jason_new\", \"user_jaysuhn\"]: continue\n",
    "\t\t# if user == \"user_lizzie\" or user == \"user_lizzie1\":\n",
    "\t\t# \tcontinue\n",
    "\t\tprint(f\"\\t{user}\")\n",
    "\n",
    "\t\t# Calculate informed prior\n",
    "\t\t# Collect a list of posterior hyperparams from all but one users individual model \n",
    "\t\tall_prior_hyperparams = [noninform_model_data[metric][u][\"posterior_hyperparams\"] for u in noninform_model_data[metric].keys() if u != user]\n",
    "\t\t# all_n = [noninform_model_data[metric][u][]]\n",
    "\t\t# m_mean = np.mean(all_prior_hyperparams[:,0])\n",
    "\t\tall_m = []\n",
    "\t\tall_sigma = []\n",
    "\t\tfor hyperparams in all_prior_hyperparams:\n",
    "\t\t\tall_m.append(hyperparams[0]) # posterior mean for weights\n",
    "\t\t\tall_sigma.append(float(hyperparams[3] / hyperparams[2])) # divided by n - p degrees of freedom\n",
    "\n",
    "\t\tall_m = np.array(all_m).reshape(-1, 6) # reshaping to n x 6 \n",
    "\t\tprior_noise_var = np.mean(all_sigma) # noise variance is estimated as the average of sample variances from other users\n",
    "\t\t# fitting to gamma distribution\n",
    "\t\tshape, loc, scale = gamma.fit(all_sigma, floc=0)\n",
    "\t\trate = 1 / scale\n",
    "\t\tprior_mean = np.mean(all_m, 0)\n",
    "\t\tprior_covar = np.cov(all_m.T)\n",
    "\t\tV_est = prior_covar / prior_noise_var\n",
    "\t\tm_est = prior_mean\n",
    "\t\td_est = 2 * shape\n",
    "\t\ta_est = 2 * rate\n",
    "\t\t\n",
    "\t\t# if prior_mean.shape != (6, 1):\n",
    "\t\t# \tprint(\"somethings wrong! prior mean is shape: \", prior_mean.shape)\n",
    "\t\t\n",
    "\t\tprint(\"Prior mean: \", prior_mean)\n",
    "\t\tprint(\"Prior covar: \", prior_covar)\n",
    "\t\tprint(\"Prior noise var: \", prior_noise_var)\n",
    "\t\t\t\n",
    "\t\t### Now perform Bayesian Regression for one user, using prior from other users\n",
    "\n",
    "\t\t# Prepare data \n",
    "\t\tX = data[['latency', 'scale']]\n",
    "\t\tY = data[metric]\n",
    "\t\tY_dict = data[[metric]].to_dict('list')\n",
    "\t\tpoly = PolynomialFeatures(degree=2)\n",
    "\t\tX_poly = poly.fit_transform(X.values)\n",
    "\n",
    "\t\t# Initialize evaluation metrics\n",
    "\t\toptimal_match_rate = []\n",
    "\t\toptimal_scale_error = []\n",
    "\t\tmse_scores = []\n",
    "\t\tfull_mse_scores = []\n",
    "\t\tn_train_mse = []\n",
    "\t\tn_train_full_mse = []\n",
    "\t\tn_train_p = []\n",
    "\t\t\n",
    "\t\t# print('hi')\n",
    "\t\tn = len(data)\n",
    "\t\tn_train_values = range(2, n-1)\n",
    "\t\tfor n_train in n_train_values:\n",
    "\n",
    "\t\t\tn_train_p.append(n_train / n)\n",
    "\t\t\t# Split into training/test sets\n",
    "\t\t\t# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=n_train/n)\n",
    "\t\t\ttrain_set, test_set = even_train_split(data, n_train)\n",
    "\t\t\tX_train, X_test = train_set[['latency', 'scale']], test_set[['latency', 'scale']]\n",
    "\t\t\tY_train, Y_test = train_set[metric], test_set[metric]\n",
    "\t\t\tY_train_dict = train_set[[metric]].to_dict('list')\n",
    "\t\t\tY_test_dict = test_set[[metric]].to_dict('list')\n",
    "\t\t\t\n",
    "\t\t\t# Create dense test input\n",
    "\t\t\t# latency_set = data['latency'].unique()# np.arange(0.0, 0.76, 0.01)\n",
    "\t\t\t# latency_range = np.array(data['latency'].unique()) #np.linspace(latency_set.min(), latency_set.max(), 50)\n",
    "\t\t\tlatency_range = np.arange(0.0, data['latency'].max()+0.01, 0.01)\n",
    "\t\t\tscale_range = np.arange(data['scale'].min(), data['scale'].max()+0.025, 0.025) #np.linspace(data['scale'].min(), data['scale'].max(), 50)\n",
    "\t\t\tlatency_grid, scale_grid = np.meshgrid(latency_range, scale_range)\n",
    "\t\t\tX_dense = np.c_[latency_grid.ravel(), scale_grid.ravel()]\n",
    "\t\t\tX_dense = np.round(X_dense, 3)\n",
    "\t\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tX_train_poly = poly.transform(X_train.values)\n",
    "\t\t\tX_test_poly = poly.transform(X_test.values)\n",
    "\t\t\tX_dense_poly = poly.transform(X_dense)\n",
    "\t\t\t\n",
    "\t\t\t# # Train model\n",
    "\t\t\t# model = BayesRegression(X_train_poly, Y_train_dict, obs_noise_std=prior_noise_var)\n",
    "\t\t\t# model.set_informed_prior(prior_mean, prior_covar)\n",
    "\t\t\t# posterior_dict = model.train()\n",
    "\t\t\t# post_mean, post_covar = posterior_dict[metric][0], posterior_dict[metric][1]\n",
    "\t\t\t# model_params = f\"coef: {post_mean.flatten()}\"\n",
    "\t\t\t# # Predict\n",
    "\t\t\t# Y_pred = model.predict(X_poly)[metric][0]\n",
    "\t\t\t# Y_pred_dense = model.predict(X_dense_poly)[metric][0]\n",
    "\n",
    "\t\t\t# Train model\n",
    "\t\t\tmodel = BayesRegressionNIG(X_train_poly, Y_train_dict, hyperparams=(m_est, V_est, d_est, a_est))\n",
    "\t\t\tposterior_dict = model.train()\n",
    "\t\t\tpost_mean, post_covar = posterior_dict[metric][0], posterior_dict[metric][1]\n",
    "\t\t\tmodel_params = f\"coef: {post_mean.flatten()}\"\n",
    "\t\t\t# Predict\n",
    "\t\t\tY_pred = model.predict(X_poly)[metric][1]\n",
    "\t\t\tY_pred_dense = model.predict(X_dense_poly)[metric][1]\n",
    "\n",
    "\t\t\t## Evaluate metrics\n",
    "\t\t\tdense_df = pd.DataFrame({\n",
    "\t\t\t\t\t'latency': X_dense[:, 0].flatten(),\n",
    "\t\t\t\t\t'scale': X_dense[:, 1].flatten(),\n",
    "\t\t\t\t\t'Y_pred_dense': Y_pred_dense.flatten()\n",
    "\t\t\t\t})\n",
    "\t\t\tdata[\"Y_pred\"] = Y_pred\n",
    "\n",
    "\t\t\t# Mean Square Error on whole dataset\n",
    "\t\t\tfull_mse = mean_squared_error(Y, Y_pred)\n",
    "\t\t\tif True: #full_mse < 5000:\n",
    "\t\t\t\tn_train_full_mse.append(n_train)\n",
    "\t\t\t\tfull_mse_scores.append(full_mse)\n",
    "\n",
    "\t\t\t# Mean Square Error on test set\n",
    "\t\t\tY_test_pred = data.loc[Y_test.index][\"Y_pred\"]\n",
    "\t\t\tmse = mean_squared_error(Y_test, Y_test_pred)\n",
    "\t\t\tif True: #mse < 5000:\n",
    "\t\t\t\tn_train_mse.append(n_train)\n",
    "\t\t\t\tmse_scores.append(mse)\n",
    "\t\t\t\n",
    "\t\t\tif metric in [\"throughput_standard\", \"avg_movement_speed_standard\", \"weighted_performance_standard\"]: # optimal scale at maximum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmax()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmax()][['latency', 'scale']]\n",
    "\t\t\telse: # optimal scale at minimum\n",
    "\t\t\t\toptimal_scale_dense = dense_df.loc[dense_df.groupby('latency')['Y_pred_dense'].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_ref = data.loc[data.groupby('latency')[metric].idxmin()][['latency', 'scale']]\n",
    "\t\t\t\toptimal_scale_pred = data.loc[data.groupby('latency')['Y_pred'].idxmin()][['latency', 'scale']]\n",
    "\n",
    "\t\t\t# Merge the results on 'latency'\n",
    "\t\t\tmerged_ref_pred = pd.merge(optimal_scale_ref, optimal_scale_pred, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_pred'))\n",
    "\t\t\t\n",
    "\t\t\tmerged_ref_dense = pd.merge(optimal_scale_ref, optimal_scale_dense, \n",
    "\t\t\t\t\t\t\t\ton='latency', suffixes=('_ref', '_dense'))\n",
    "\t\t\t# print(optimal_scale_dense)\n",
    "\t\t\t# print(merged_ref_dense)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\t# Count the number of matches\n",
    "\t\t\tmatches = (merged_ref_pred['scale_ref'] == merged_ref_pred['scale_pred']).sum()\n",
    "\t\t\tscale_error = np.abs(merged_ref_dense['scale_ref'] - merged_ref_dense['scale_dense']).mean()\n",
    "\n",
    "\t\t\toptimal_match_rate.append(matches / len(optimal_scale_ref))\n",
    "\t\t\toptimal_scale_error.append(scale_error)\n",
    "\n",
    "\t\t\t# Visualize model prediction\n",
    "\t\t\tif n_train == n-2:\n",
    "\t\t\t\tmodel_heatmaps(data, dense_df, X_train, user, metric, model_type, model_params)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t# Store results from this dataset\n",
    "\t\t\tuser_results[user] = {\n",
    "\t\t\t\t'n_train_mse': list(n_train_mse),\n",
    "\t\t\t\t'n_train_full_mse': list(n_train_full_mse),\n",
    "\t\t\t\t'full_mse_scores': full_mse_scores,\n",
    "\t\t\t\t'mse_scores': mse_scores,\n",
    "\t\t\t\t'n_train_all': list(n_train_values),\n",
    "\t\t\t\t'match_rate': optimal_match_rate,\n",
    "\t\t\t\t'scale_error': optimal_scale_error,\n",
    "\t\t\t\t'n_train_p': n_train_p,\n",
    "\t\t\t\t'posterior_hyperparams': posterior_dict[metric]\n",
    "\t\t\t}\n",
    "\t\t\t# print(user_results)\n",
    "\t\t\tcontinue\n",
    "\t\tbreak\t\n",
    "\telse:\n",
    "\t\t# print(f\"saving {metric} to all_results\")\n",
    "\t\tall_results[metric] = user_results\n",
    "\t\tcontinue\n",
    "\tbreak\n",
    "\n",
    "# print(all_results.keys())\n",
    "# with open(f\"../model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(all_results, file)\n",
    "with open(f\"../model_result_data/{model_type}.pkl\", 'wb') as f:\n",
    "        pickle.dump(all_results, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"r\") as file:\n",
    "# \tog_results = json.load(file)\n",
    "\n",
    "# og_results[\"total_error\"] = all_results[\"total_error\"]\n",
    "\n",
    "# with open(f\"model_result_data/{model_type}.json\", \"w\") as file:\n",
    "# \tjson.dump(og_results, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dvrk trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# X = (scale, delay)\n",
    "# Y = (start, end, errors, drops, num_complete, notes)\n",
    "trial_data = {(0.2, 0.0): (25, 2*60+48, 0, 0, 6, \"\"), \n",
    "\t#   (0.4, 0.5): (20, 2*60+29, 0, 0, 3, \"large wrist motion caused robot error halfway through trial\"),\n",
    "\t (0.2, 0.25): (45, 3*60+6, 0, 0, 6, \"\"),\n",
    "\t (0.3, 0.0): (40, 2*60+43, 0, 0, 6, \"\"),\n",
    "\t (0.4, 0.0): (24, 2*60+37, 0, 2, 6, \"two drops recovered quickly\"),\n",
    "\t (0.4, 0.25): (16, 3*60+17, 0, 1, 6, \"one drop recovered quickly\"),\n",
    "\t (0.3, 0.25): (18, 3*60+13-15, 0, 0, 6, \"froze for 15 seconds, end time adjusted accordingly\"), # freeze start 1:39 continue 1:54\n",
    "\t (0.2, 0.5): (22, 3*60+40, 0, 0, 6, \"\"),\n",
    "\t (0.3, 0.5): (27, 3*60+41, 1, 5, 3, \"Dropped first object many times and eventually gave up, stopped at halfway mark due to time.\"), # 5 drop, 1 unrecoverd\n",
    "\t (0.4, 0.5): (35, 3*60+23-6, 0, 1, 6, \"Dropped one and recovered, 6 second freeze subtracted from end time.\") # trial 2, 1 drop, 6 second freeze\n",
    "\t }\n",
    "\n",
    "data = pandas.DataFrame(columns=[\"scale\", \"latency\", \"time_score\", \"penalty\", \"drops\", \"overall_score\", \"transfer_speed\"])\n",
    "for x, y in trial_data.items():\n",
    "\tduration = y[1] - y[0]\n",
    "\tnum_transfers = y[4]\n",
    "\ttransfer_speed = num_transfers / duration * 1000\n",
    "\tcutoff_time = 300 * (y[4] / 6) # accounting for percentage completion\n",
    "\ttime_score = max(0, cutoff_time - duration)\n",
    "\tdrops = y[3]\n",
    "\tpenalty = (y[2] / 3) * 100 + drops * 10\n",
    "\toverall_score = max(0, time_score - penalty)\n",
    "\tdata.loc[len(data)] = [x[0], x[1], time_score, penalty, drops, overall_score, transfer_speed]\n",
    "\n",
    "         \n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "user = \"Jason\"\n",
    "title = (f\"Results of dvrk trial for {user}\")\n",
    "fig.suptitle(title)\n",
    "# Original data heatmap with all points highlighted (now all are training points)\n",
    "time_score_heatmap = data.pivot(\n",
    "\tindex='latency', columns='scale', values='time_score'\n",
    ")\n",
    "sns.heatmap(time_score_heatmap, cmap='YlGnBu', ax=ax[0], annot=True, fmt='.0f')\n",
    "ax[0].set_title('Time Score')\n",
    "ax[0].set_xlabel('Scale')\n",
    "ax[0].set_ylabel('Latency')\n",
    "# annotate_extrema(time_score_heatmap.values, ax[0], extrema_type='max')\n",
    "\n",
    "# Full predicted data heatmap (prediction on the entire dataset)\n",
    "penalty_heatmap = data.pivot(\n",
    "\tindex='latency', columns='scale', values='drops'\n",
    ")\n",
    "sns.heatmap(penalty_heatmap, cmap='YlGnBu', ax=ax[1], annot=True)\n",
    "ax[1].set_title('Number of Drops')\n",
    "ax[1].set_xlabel('Scale')\n",
    "ax[1].set_ylabel('Latency')\n",
    "# annotate_extrema(penalty_heatmap.values, ax[1], extrema_type='min')\n",
    "\n",
    "overall_score_heatmap = data.pivot(\n",
    "\tindex='latency', columns='scale', values='overall_score'\n",
    ")\n",
    "sns.heatmap(overall_score_heatmap, cmap='YlGnBu', ax=ax[2], annot=True, fmt='.0f')\n",
    "# annotate(ax[1], dense_pred_data, X_train, color='green')\n",
    "ax[2].set_title('Overall Score')\n",
    "ax[2].set_xlabel('Scale')\n",
    "ax[2].set_ylabel('Latency')\n",
    "# annotate_extrema(overall_score_heatmap, ax[2], extrema_type='max')\n",
    "\n",
    "# # Plot residuals\n",
    "# data[\"residual\"] = np.abs(data[\"performance\"] - data[\"Y_pred\"])\n",
    "# residual = data.pivot(\n",
    "# \tindex='latency', columns='scale', values='residual'\n",
    "# )\n",
    "# sns.heatmap(residual, cmap='YlGnBu', ax=ax[2], annot=True)\n",
    "# annotate(ax[2], residual, X_train, color='green')\n",
    "# ax[2].set_title('Residuals')\n",
    "# ax[2].set_xlabel('Scale')\n",
    "# ax[2].set_ylabel('Latency')\n",
    "# annotate_extrema(residual.values, ax[2], 'min')\n",
    "\n",
    "plt.tight_layout()\n",
    "folder = \"../figures/dvrk\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "filepath = f\"{folder}/{user}\"\n",
    "plt.savefig(filepath, facecolor='w')\n",
    "plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dvrk trials 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/jlimk/Documents/dvrk_trial_data/user_sarah/sarah_trial_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m metric_data_save_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../dvrk/trial_data/user_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/metric_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Process trial video data (time score and drop penalties)\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m video_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/jlimk/Documents/dvrk_trial_data/user_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muser\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muser\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_trial_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(video_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m-\u001b[39mvideo_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrosbag_start_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     34\u001b[0m end_time_str \u001b[38;5;241m=\u001b[39m video_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jlimk\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:610\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    605\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    606\u001b[0m     dialect, delimiter, delim_whitespace, engine, sep, defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    607\u001b[0m )\n\u001b[0;32m    608\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jlimk\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:462\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    459\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 462\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\jlimk\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:819\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jlimk\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:1050\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown engine: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (valid options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1048\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;66;03m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jlimk\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:1867\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1864\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# open handles\u001b[39;00m\n\u001b[1;32m-> 1867\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jlimk\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:1362\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_handles\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: FilePathOrBuffer, kwds: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1359\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;124;03m    Let the readers open IOHanldes after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jlimk\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:642\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m         errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/jlimk/Documents/dvrk_trial_data/user_sarah/sarah_trial_data.csv'"
     ]
    }
   ],
   "source": [
    "### Code for analyzing dvrk trial data, generating performance metric datasets and heatmaps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.signal import filtfilt, butter\n",
    "\n",
    "# Define a Butterworth bandpass filter\n",
    "def butter_bandpass(data, lowcut, highcut, fs, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "user_list = [\"sarah\", \"soofiyan\", \"nikhil\", \"neelay\"]\n",
    "for user in user_list:\n",
    "\t# scale, latency paramter combinations\n",
    "\tscale = [1, 2, 3, 4]\n",
    "\tdelay = [2, 5]\n",
    "\t# metrics = [\"board_force\", \"psm_force\", \"time_score\", \"drop_penalty\", \"overall_score\"]\n",
    "\tmetrics = [\"board_force\", \"psm_force\"]\n",
    "\tmetric_data = pd.DataFrame(columns=[\"scale\", \"latency\"] + metrics)\n",
    "\tmetric_data_save_filepath = f\"../dvrk/trial_data/user_{user}/metric_data\"\n",
    "\n",
    "\t# Process trial video data (time score and drop penalties)\n",
    "\tvideo_data = pd.read_csv(f\"C:/Users/jlimk/Documents/dvrk_trial_data/user_{user}/{user}_trial_data.csv\")\n",
    "\tstart_time = int(video_data[\"start_time\"]-video_data[\"rosbag_start_time\"])\n",
    "\tend_time_str = video_data[\"end_time\"]\n",
    "\tif len(end_time_str.split(':')) == 1:\n",
    "\t\tend_time = int(end_time_str.split(':')[0])\n",
    "\telif len(end_time_str.split(':')) == 2:\n",
    "\t\tend_time = int(end_time_str.split(':')[0])*60 + int(end_time_str.split(':')[1])\n",
    "\tend_time = end_time - int(video_data[\"rosbag_start_time\"])\n",
    "\t\n",
    "\n",
    "\tfts_data_filepath = \"\"\n",
    "\tpsm_data_filepath = \"\"\n",
    "\n",
    "\t# Loop over scale, delay parameters\n",
    "\tfor s, d in itertools.product(scale, delay):\n",
    "\t\tif user == \"sarah\" and s == 1 and d == 2: continue\n",
    "\t\tif user in [\"sarah\", \"soofiyan\"]:\n",
    "\t\t\ttest_name = f\"scale_{s}e-1_delay_{d}e-1\"\n",
    "\t\telse:\n",
    "\t\t\ttest_name = f\"scale{s}_delay{d}\"\n",
    "\n",
    "\t\t# Load psm force data\n",
    "\t\tpsm_data_filepath = f\"../dvrk/trial_data/user_{user}/{test_name}_psm_data.pkl\"\n",
    "\t\twith open(psm_data_filepath, 'rb') as f:\n",
    "\t\t\tpsm_force_data = pickle.load(f)\n",
    "\n",
    "\t\t# Load FTS data\n",
    "\t\tfts_data_filepath = f\"C:/Users/jlimk/Documents/dvrk_trial_data/user_{user}/fts_files\" + f\"/{test_name}.csv\"\n",
    "\t\tfts_data = pd.read_csv(fts_data_filepath, skiprows=6)\n",
    "\t\t\n",
    "\t\t# Clip fts data to trial start and end times\n",
    "\t\t# find closest index to start time\n",
    "\t\tfts_data[' Time '].apply(lambda x: int(x.split(':')[0])*60*60 + int(x.split(':')[1])*60 + int(x.split(':')[2]))\n",
    "\t\tfts_data[' Time '] = fts_data[' Time '] - fts_data[' Time '][0]\n",
    "\t\tfts_start_idx = fts_data[' Time '].sub(start_time).abs().idxmin()\n",
    "\t\tfts_end_idx = fts_data[' Time '].sub(end_time).abs().idxmin()\n",
    "\t\tfts_data_clip = fts_data.loc[fts_start_idx:fts_end_idx]\n",
    "\t\tfts_data_clip[' Time '] = fts_data_clip[' Time '] - fts_data_clip[' Time '][0] # rezero time data\n",
    "\n",
    "\t\t# # Load video metric data (time score and drop penalties)\n",
    "\t\t# video_metric_filepath = \"\" + f\"/scale_{s}e-1_delay_{d}e-1.csv\"\n",
    "\t\t# video_metric_data = pd.read_csv(video_metric_filepath)\n",
    "\n",
    "\t\t# Obtain avg filtered psm forces\n",
    "\t\tpsm_total_force = 0\n",
    "\t\tfiltered_mean = []\n",
    "\t\tfor psm_id, psm_data in psm_force_data.items():\n",
    "\t\t\tif psm_id == \"stereo_image\": continue\n",
    "\n",
    "\t\t\t# Clip psm data\n",
    "\t\t\tpsm_data_full = psm_data.copy()\n",
    "\t\t\tpsm_start_idx = psm_data['time'].sub(start_time).abs().idxmin()\n",
    "\t\t\tpsm_end_idx = psm_data['time'].sub(end_time).abs().idxmin()\n",
    "\t\t\tpsm_data = psm_data.loc[psm_start_idx:psm_end_idx]\n",
    "\t\t\tpsm_data['time'] = psm_data['time'] - psm_data['time'][0] # rezero time data\n",
    "\n",
    "\t\t\t# Filter parameters\n",
    "\t\t\torder = 3\n",
    "\t\t\tlcf = 0.1\n",
    "\t\t\thcf = 5\n",
    "\t\t\tfs = len(psm_data_full) / psm_data_full[\"time\"].iloc[-1]\n",
    "\t\t\tforce_filtered = butter_bandpass(psm_data[\"force_mag\"], lowcut=lcf, highcut=hcf, fs=fs, order=order)\n",
    "\t\t\tpsm_data[\"force_filtered\"] = force_filtered # Add filtered force to dict for plotting later\n",
    "\t\t\tforce_filtered_positive = np.maximum(force_filtered, 0)\n",
    "\t\t\tpsm_data[\"force_filtered_positive\"] = force_filtered_positive\n",
    "\t\t\tfiltered_mean.append(np.mean(force_filtered_positive))\n",
    "\t\t\tprint(psm_id, \" force_filtered_positive mean = \", np.mean(force_filtered_positive))\n",
    "\t\t\t# psm_total_force += filtered_mean # sum of average filtered psm forces\n",
    "\t\t\n",
    "\t\tpsm_total_force = np.mean(filtered_mean)\n",
    "\t\tprint(\"PSM total force = \", psm_total_force)\n",
    "\t\t# Calculate Peg Board forces\n",
    "\t\t# Remove Bias\n",
    "\t\tfts_data[' Fx '] = fts_data[' Fx '] - fts_data[' Fx '][0]\n",
    "\t\tfts_data[' Fy '] = fts_data[' Fy '] - fts_data[' Fy '][0]\n",
    "\t\tfts_data[' Fz '] = fts_data[' Fz '] - fts_data[' Fz '][0]\n",
    "\t\t# Calculate z axis drift\n",
    "\t\tdrift_slope = (fts_data[' Fz '].iloc[-1] - fts_data[' Fz '][0]) / (len(fts_data)-1)\n",
    "\t\tdrift = [drift_slope * i + fts_data[' Fz '][0] for i in range(len(fts_data))]\n",
    "\t\tFz_adjusted = fts_data[' Fz '] - drift\n",
    "\t\ttotal_force = np.sqrt(fts_data[' Fx ']**2 + fts_data[' Fy ']**2 + fts_data[' Fz ']**2)\n",
    "\t\ttotal_force_adjusted = np.sqrt(fts_data[' Fx ']**2 + fts_data[' Fy ']**2 + Fz_adjusted**2)\n",
    "\t\tavg_total_force = np.mean(total_force)\n",
    "\t\tboard_total_force = np.mean(total_force_adjusted) # Average force magnitude after adjusting for drift\n",
    "\t\tprint(\"Peg board force = \", board_total_force)\n",
    "\n",
    "\t\t# plt.figure()\n",
    "\t\t# plt.plot(fts_data[' Fz '], label='Fz')\n",
    "\t\t# plt.plot(fts_data[' Fy '], label='Fy')\n",
    "\t\t# plt.plot(fts_data[' Fx '], label='Fx')\n",
    "\t\t# # plt.plot(Fz_adjusted, label='Fz adjusted')\n",
    "\t\t# plt.legend()\n",
    "\t\t# plt.show()\n",
    "\n",
    "\t\t# Append metrics to data\n",
    "\t\tmetric_data.loc[len(metric_data)] = [s, d, board_total_force, psm_total_force]\n",
    "\t\t\n",
    "\t\t# # Plot Peg Board and PSM force magnitude over time\n",
    "\t\t# fig_save_filepath = \"dvrk/figures/\"\n",
    "\t\t# fig, axs = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\t\t# fig.suptitle(f\"Trial Parameters: Scale = {s}e-1, Delay = {d}e-1\")\n",
    "\n",
    "\t\t# axs[0].set_title(\"Peg Board Force\")\n",
    "\t\t# axs[0].plot(total_force_adjusted, label=\"Board Force\")\n",
    "\t\t# axs[0].axhline(board_total_force)\n",
    "\n",
    "\t\t# axs[1].set_title(\"PSM Arm Forces\")\n",
    "\t\t# for psm_id, psm_data in psm_force_data.items():\n",
    "\t\t# \taxs[1].plot(psm_data[\"time\"], psm_data[\"force_filtered_positive\"], label=psm_id)\n",
    "\t\t# axs[1].axhline(filtered_mean[0])\n",
    "\t\t# axs[1].axhline(filtered_mean[1])\n",
    "\t\t# axs[1].legend()\n",
    "\t\t# # plt.savefig(fig_save_filepath, facecolor='w')\n",
    "\t\t# plt.show()\n",
    "\n",
    "\t# Save metric data?\n",
    "\t# metric_data.to_csv(metric_data_save_filepath + '.csv')\n",
    "\n",
    "\t# Plot metric heatmaps\n",
    "\tfig, ax = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\t# plt.figure()\n",
    "\ttitle = (f\"{user} force metrics\")\n",
    "\tfig.suptitle(title)\n",
    "\tboard_force_heatmap = metric_data.pivot(\n",
    "\t\tindex='latency', columns='scale', values='board_force'\n",
    "\t)\n",
    "\tsns.heatmap(board_force_heatmap, cmap='YlGnBu', ax=ax[0], annot=True, fmt='.3f')\n",
    "\tax[0].set_title('Peg Board Force')\n",
    "\tax[0].set_xlabel('Scale')\n",
    "\tax[0].set_ylabel('Latency')\n",
    "\t# annotate_extrema(board_force_heatmap.values, ax[0], extrema_type='max')\n",
    "\n",
    "\tpsm_force_heatmap = metric_data.pivot(\n",
    "\t\tindex='latency', columns='scale', values='psm_force'\n",
    "\t)\n",
    "\tsns.heatmap(psm_force_heatmap, cmap='YlGnBu', ax=ax[1], annot=True, fmt='.3f')\n",
    "\tax[1].set_title('PSM Force')\n",
    "\tax[1].set_xlabel('Scale')\n",
    "\tax[1].set_ylabel('Latency')\n",
    "\t# annotate_extrema(psm_force_heatmap.values, ax[1], extrema_type='max')\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tfolder = \"../figures/dvrk\"\n",
    "\tos.makedirs(folder, exist_ok=True)\n",
    "\tfilepath = f\"{folder}/{user}_force\"\n",
    "\tplt.savefig(filepath, facecolor='w')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random shuffle combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2), (4, 5), (2, 5), (3, 5), (1, 2), (1, 5), (4, 2), (3, 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "scales = [1, 2, 3, 4]\n",
    "delays = [2, 5]\n",
    "combos = [(s, d) for s in scales for d in delays]\n",
    "random.shuffle(combos)\n",
    "combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
